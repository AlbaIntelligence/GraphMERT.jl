% \usepackage[round,authoryear]{natbib}
% \bibliographystyle{plainnat} % or abbrvnat, unsrtnat

% % (Optional) make \cite behave like \citep
\AtBeginDocument{\let\cite\citep}


\documentclass[10pt]{article}

% === TMLR style (choose ONE of the options below) ===
% \usepackage{tmlr}            % anonymized submission
% \usepackage[accepted]{tmlr}  % camera-ready
\usepackage[preprint]{tmlr}    % de-anonymized preprint


% ---- Core math & symbols
\usepackage{amsmath,amssymb}

% ---- Language & encoding (kept for compatibility)
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

% ---- Colors, boxes, lists
\usepackage[dvipsnames,table]{xcolor}
\usepackage[most]{tcolorbox}
\usepackage{mdframed}
\usepackage{enumitem}
\usepackage{xspace}
\usepackage{soul}
\sethlcolor{antiquewhite}

% Custom colors
\definecolor{amber}{rgb}{1.0, 0.75, 0.0}
\definecolor{antiquewhite}{rgb}{0.98, 0.92, 0.84}
\definecolor{amber(sae/ece)}{rgb}{1.0, 0.4, 0.0}
\definecolor{burntorange}{rgb}{0.8, 0.33, 0.0}
\definecolor{aurometalsaurus}{rgb}{0.42, 0.5, 0.5}

% ---- Figures & floats
\usepackage{graphicx}
\usepackage{float}
\usepackage{subcaption} % (loads caption internally; avoid custom caption styling)


% ---- Tables
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage[table]{xcolor}
\usepackage{multicol}

% ---- Listings / algorithms
\usepackage{listings}
\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  captionpos=b
}
\usepackage[ruled,vlined]{algorithm2e}
\SetKw{In}{in}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}


\usepackage{url}
\usepackage[colorlinks=true,allcolors=blue]{hyperref}

% ---- (Optional) Appendices
\usepackage[toc,page]{appendix}

% ---- Math operators & shortcuts
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\red}{\color{red}}
\newcommand{\ours}{\textsc{GraphMERT}\xspace}


\title{\ours: Efficient and Scalable Distillation of Reliable Knowledge Graphs from Unstructured Data}

\author{\name Margarita Belova \email margarita.bel@princeton.edu \\
       \addr Department of Electrical and Computer Engineering, Princeton University
\AND
       \name Jiaxin Xiao \email jx0800@princeton.edu \\
       \addr Department of Electrical and Computer Engineering, Princeton University
\AND
       \name Shikhar Tuli \email stuli@alumni.princeton.edu \\
       \addr Department of Electrical and Computer Engineering, Princeton University
\AND
       \name Niraj K. Jha \email jha@princeton.edu \\
       \addr Department of Electrical and Computer Engineering, Princeton University
}

\date{}



\begin{document}
\maketitle

\begin{abstract}
Researchers have pursued neurosymbolic artificial intelligence (AI) applications for nearly three decades because symbolic components provide abstraction while neural components provide generalization. Thus, a marriage of the two components can lead to rapid advancements in AI. Yet, the field has not realized this promise since most neurosymbolic AI frameworks fail to scale. In addition, the implicit representations and approximate reasoning of purely neural approaches limit interpretability and trust. Knowledge graphs (KGs), a gold-standard representation of explicit semantic knowledge, can address the symbolic side. However, automatically deriving reliable KGs from text corpora has remained an open problem. We address the above challenges by introducing \ours, a tiny graphical encoder-only model that distills high-quality KGs from unstructured text corpora and its own internal representations. Together, \ours and its equivalent KG form a modular neurosymbolic stack: neural learning of abstractions; symbolic KGs for verifiable reasoning. \ours + KG is the first efficient and scalable neurosymbolic model to achieve state-of-the-art benchmark accuracy along with superior symbolic representations relative to baselines.

More concretely, we target \emph{reliable} domain-specific KGs that are both (1) factual (with provenance) and (2) valid (ontology-consistent relations with domain-appropriate semantics). When an off-the-shelf large language model (LLM), e.g., Qwen3-32B, generates domain-specific KGs, it falls short on the reliability front due to prompt sensitivity, shallow domain expertise, and hallucinated relations. Thus, practitioners should avoid employing LLM-generated KGs in high-stakes domains, e.g., medicine, law, business, education, etc. On text obtained from PubMed papers related to diabetes, our KG extraction pipeline with a small 80M-parameter \ours yields a KG with a 69.8\% FActScore; a 32B-parameter baseline LLM yields a KG that achieves only a 40.2\% FActScore. The \ours-extracted KG also achieves a significantly higher ValidityScore of 68.8\%, compared to an LLM-generated baseline (43.0\%), demonstrating its ability to preserve ontology alignment. Finally, human experts can edit and audit the extracted KGs, further increasing their reliability. This is nearly impossible with purely-neural representations. Hence, \ours enables efficient, scalable, transparent (interpretable and explainable), attributable (with provenance), accountable (with governance), editable, auditable, and continually improvable state-of-the-art neurosymbolic AI.
%\ours can then treat the improved KG as a seed and refine it again, enabling scalable, auditable, editable, and improvable KG extraction while consistently improving downstream performance.

\vspace*{2mm}
\noindent
\textit{Index Terms}: Hallucinations, interpretability, knowledge graphs, language models, neurosymbolic methods, retrieval-augmented generation.

\end{abstract}

\section{Introduction}
\label{intro}

Artificial intelligence (AI) has long oscillated between two dominant paradigms: symbolic reasoning and neural learning~\citep{shavlik1991symbolic, AvilaGarcez2002}. Symbolic systems excel at explicit (rule-based) inference, providing interpretability and strong exact reasoning. This assumes precise and consistent symbolic abstractions. However, such systems struggle with noisy or ambiguous data. Neural approaches, by contrast, thrive on large-scale and general-purpose pattern recognition, often outperforming hand-coded explicit representations~\cite{sutton2019bitter}. Nevertheless, neural networks operate as black boxes, offering little transparency in their decision-making~\cite{sharkey2025open}. Their representations are approximate: ambiguous, difficult to control, and not grounded in explicit rules. Thus, each paradigm, taken alone, has critical gaps. Neurosymbolic AI is a synthesis of the two paradigms, aiming to combine the flexibility of neural models with the rigor and interpretability of symbolic systems. By uniting these complementary strengths, it opens up a path toward systems capable of both scalable learning and sound reasoning --- a longstanding ambition of the field~\cite{neurosymboic_3rd, towell1994using, garcez2019neuralsymbolic_methodology}.

% How AI and other fields benefit from a reliable KG. 

Large language models (LLMs) have generated enormous excitement, but their reasoning is ultimately probabilistic and often unable to perform causal inference, opaque to humans, and prone to hallucinations, especially during multi-step reasoning~\cite{marcus2018_dl_critical, 10.1145/3571730}. Their opaqueness raises serious concerns about whether such models can be trusted~\cite{vonEschenbach2021transparency, 10.1155/2023/4459198}. LLMs trained on general text corpora may also fail to adapt to specialized domains or incorporate new knowledge without undergoing expensive retraining~\cite{zhao2025survey-llms}. These limitations highlight the need for external, explicit sources of factual grounding in high-stakes use cases~\cite{fan_rag_survey2024}.

Unifying knowledge graphs (KGs) with LLMs could help overcome some of these limitations, as KGs offer a natural complement~\cite{ibrahim2024survey-kg-llm}. By encoding knowledge in a structured, symbolic form of head-relation-tail triples with explicit and verifiable relations, KGs offer interpretability, auditability, and domain-specific depth that LLMs lack~\cite{pan_et_al:TGDK}. Thus, KGs can guide LLM inference and enable robust evolution of background knowledge, while LLMs contribute flexible reasoning, efficient handling of ambiguity through approximate inference, and serve as a natural language interface~\cite{Pan2023UnifyingLL}. This synergy can enable KG-guided exploration and learning, support agentic workflows driven by interaction with an editable knowledge base, and enhance the trustworthiness of LLMs in high-stakes application domains by reinforcing factuality and enabling immediate knowledge updates. 

Yet, constructing a KG from scratch in a new domain is a notoriously arduous task~\cite{survey_atuomatic_kgc, 9416312}. The process typically involves cleaning and pre-processing heterogeneous data, multi-step knowledge acquisition, and post-processing. Ensuring that the resulting KG is reliable and factual is even more challenging, often requiring extensive manual inspection and crowdsourcing~\cite{WANG2021607}. In fields where no ground-truth KG or domain-specific benchmarks exist, this task becomes infeasible without expert intervention. This makes existing approaches unscalable.

Given these obstacles, we propose characteristics that an effective KG construction method should provide:
\begin{enumerate}
    \item \textbf{Factuality and provenance:} Triples grounded in source text with verifiable citations.
    \item \textbf{Validity:} Adherence to ontological schema constraints, appropriate term granularity, and domain-correct relations.
    \item \textbf{Automation:} End-to-end extraction without expert oversight; usable by non-specialists.  
    \item \textbf{Scalability:} Robust performance as data volume and compute grow.
    \item \textbf{Domain generality:} Principles that transfer across subject areas.  
    \item \textbf{Global integration:} Cross-document linking of concepts, not just within local spans.  
\end{enumerate}

We refer to KGs that are both factual and valid as \emph{reliable}; hence, we classify methods that produce factual and valid triples as reliable.

Recently, KG generation with decoder-based LLMs has become the most widely used and studied method~\cite{xu2024llm-ie-survey}, especially for commonsense knowledge extraction~\cite{symbolic_kg_distillation}. Nonetheless, we demonstrate that LLMs are unable to construct reliable domain-specific KGs. The factuality challenge faced by LLMs intersects with several other pressing issues, such as hallucinations, outdated knowledge, and lack of depth in given domains (e.g., in health, law, and finance applications)~\cite{10.1145/3742420}.

Despite their ease of use, versatility, broad knowledge coverage, and impressive general-purpose language skills~\cite{2025largelanguagemodelssurvey}, LLMs as KG constructors are known to underperform on domain-specific datasets~\cite{LLM_for_kg_construction} and are susceptible to domain-irrelevant noise in the context~\cite{chen-etal-2024-sac}. For example,~\citet{Yang2025_llm_sepsis} highlight limitations of LLM-based KG creation in handling domain-specific medical terms and encoding context-dependent relations beyond generic predicates. Hallucinations further restrict text-generative approaches from producing fully accurate and trustworthy domain-specific KGs without human validation~\cite{Ghanem2025_ft_vs_prompt}, often resulting in inclusion of fabricated facts in the KG~\cite{huang2025llmsgoodgraphjudge}. Since LLMs are oblivious to their training sources~\cite{khalifa2024sourceaware}, prompt-based KG distillation from an LLM's weights does not provide source attribution and obfuscates knowledge provenance, thus casting doubts on its reliability~\cite {pan_et_al:TGDK}.

Fine-tuned models demonstrate significantly higher accuracy compared to zero-shot and few-shot prompting, exhibit enhanced extraction fidelity in the domains they are adapted to, and often produce much fewer hallucinated facts~\cite{Ghanem2025_ft_vs_prompt}. Without fine-tuning, LLMs are inclined to leak general knowledge obtained through pretraining, potentially overlooking vital domain-specific information. This calls into question their usefulness in specialized domains~\cite{yang-etal-2023-empower}. In addition, because LLMs are highly prompt-dependent, their behavior can only be steered through prompting; without fine-tuning, prompted outputs may not align with task requirements. However, fine-tuning of LLMs requires labeled training data, negatively impacts generalization in heterogeneous domains, and ultimately reduces adaptability to knowledge domains that differ from the fine-tuning set~\cite{Ghanem2025_ft_vs_prompt}.  

Finally, KG extraction from text corpora with off-the-shelf LLMs is not global in the sense that the extracted triples are confined to a single text chunk presented in the context window. Such triples are often local, i.e, they may only reflect a spurious correlation based on co-occurrence of various terms, as opposed to global triples obtained from the larger text corpora that can generally be deemed as universal facts. Extending context length is not a solution to countering this problem because it is known to degrade output quality. In particular, hallucinations become more frequent in this situation~\cite{an2024doeseffectivecontextlength}, thus affecting the model's capability to harness information from lengthy input contexts.

The current approaches fail to meet all six requirements for KGs we listed above~\cite{info15080509}. A method that satisfies all six could unlock numerous downstream applications.  
However, a reliable KG can be extracted only if the data are of high quality~\cite{rejeleene2024trustablelms, geiger2020garbage, wang2023dataquality}. Yet, such data are scarce.  
This leads us to the central question:
\begin{center}
    \textsl{How can we build a reliable domain-specific KG from limited high-quality sources?}
\end{center}

To address this challenge, we propose a novel framework, \ours (\underline{Graph}ical \underline{M}ultidirectional \underline{E}ncoder \underline{R}epresentations from \underline{T}ransformers), for reliable KG extraction from small high-quality domain-specific data. \ours relies on an encoder-only transformer that distills a symbolic representation from its weights. It jointly learns cross-modal representations: semantic --- from a small expert-curated initial seed KG, and syntactic --- from unstructured sentence-level text by minimizing the standard masked language modeling (MLM) and the proposed masked node modeling (MNM) losses. We automatically extract a KG that captures factual knowledge by training \ours on high-quality texts with a small seed KG (e.g., 100+ triples per relation). The framework is domain-agnostic and only requires a seed KG along with a small, high-quality, domain-specific dataset (e.g., $\sim$ 100M tokens). To the best of our knowledge, {\em \ours-powered KG extraction is the first framework that possesses all six characteristics of an effective KG mentioned earlier:}
\begin{enumerate}
    \item Factuality and provenance: We implement triple extraction at the sentence level. One can trace back each triple to its source sequence, thus supporting knowledge provenance. FActScore, which quantifies the factuality of a KG (more details in Sec.~\ref{subsec:factsocre_validity}), for the \ours-generated KG (69.8\%) is much higher than that of an LLM-generated one (40.2\%).
    \item Validity: The resulting KG preserves the relation usage patterns imposed by the ontological structure of the seed KG, which enhances the validity of the relations in the extracted KG relative to the LLM-extracted baseline KG. ValidityScore, which quantifies the ontological alignment of a KG (more details in Sec.~\ref{subsec:factsocre_validity}), for our KG (68.8\%) is much higher than that of the baseline (43.0\%).
    \item Automation: It does not need manual feature selection, rule handcrafting, or human experts in the loop. It leverages a neural-to-symobolic converter, i.e., \ours, to automatically and reliably generate a KG.
    \item Scalability: We obtain training data only from credible sources and the compact \ours (with just 80M parameters) eliminates the need for pretraining on large unverified text, making the approach much more practical than employing expensive LLMs (with billions or trillions of parameters). It can be scaled when provided with more data and given extra compute resources.
    \item Domain generality: It relies on domain-agnostic principles. We do not hard-code any domain-specific parameters to the proposed \ours pipeline.
    \item Global integration: It can connect global concepts across the whole dataset throughout training, in contrast to extracting disconnected information from isolated text.
\end{enumerate}
  
The rest of the article is organized as follows. In Sec.~\ref{sec:background}, we review different KG extraction techniques and motivate the need for a reliable KG. In Sec.~\ref{sec:motivation}, we provide a brief motivational example. In Sec.~\ref{sec:methodology}, we give a detailed overview of our proposed \ours framework and its architecture. In Sec.~\ref{sec:experimental_setup}, we describe the experimental setup. In Sec.~\ref{sec:experimental_results}, we provide experimental results. In Sec.~\ref{sec:discussion}, we discuss the limitations of our methodology and discuss future work. Finally, we conclude in Sec.~\ref{sec:conclusion}.

\section{Background and Related Work}
\label{sec:background}

In this section, we review prior research that is relevant to our work, including applications of neurosymbolic AI and KGs. We then examine existing KG extraction methods, their limitations, and how our framework addresses these gaps. Finally, we provide the technical background on graph transformer architectures that is necessary to understand the remainder of this work.

\subsection{Knowledge Graphs}
\label{subsec:kg}

The term ``Knowledge Graph'' was coined by Google in a blog~\cite{Singhal2012KG} in 2012. Google anticipated a great potential of graph representation in web search for discerning semantic connections in vast web data to respond to user queries. Since then, KGs have sparked a great deal of research on knowledge-aware applications~\cite{zhang_10884362}. 
%However, there is no universal agreement on how to define a KG. 
A KG $\mathcal{G}=(V,E)$ can be viewed as a directed graph where nodes \(V\) represent real-world entities and directed edges $E \subseteq V \times V$ represent relationships between them (see Fig.~\ref{fig:toy_kg} for a toy example). Each directed edge \(e = (u, v) \in E\) connects two nodes \(u, v \in V\) and encodes a relationship $r$ between the corresponding entities. Semantically, a KG can be thought of as a set of triples $\mathcal{G} = {\langle h, r, t\rangle = \langle head, relation, tail\rangle}$, where head and tail denote two KG entities connected by a directed relation. For example, in Fig.~\ref{fig:toy_kg}, $\langle \text{Metformin}, \text{TREATS}, \text{Type\ 2\ Diabetes}\rangle$ (note the implied directionality in the representation) is one of the triples in the toy KG.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\linewidth]{pictures/toy_kg.pdf}
    \caption{A toy KG example from the medical domain.}
    \label{fig:toy_kg}
\end{figure}

\subsection{Importance of KGs in Neurosymbolic AI}
\label{kg_for_ns_ai}
Reasoning is the defining challenge in neurosymbolic AI. Researchers have long struggled to combine the efficiency of neural learning with the rigor and interpretability of symbolic inference~\cite{ZHANG202114}.

\subsubsection{The Representation Dilemma: Neural or Symbolic?}
\label{subsubsec:neu_or_symb}

Traditional AI research overwhelmingly associates reasoning with purely symbolic systems, such as expert systems~\cite{MYCIN1978_313, DENDRAL-1993209} or logic-based AI~\cite{MCCARTHY198027, prolog_1996}. For decades, this paradigm shaped AI practice under the premise that human intelligence could be reduced to formal logic operating on symbols~\cite{newell_simon, haugeland1985artificial}. Symbolic methods offer clarity and structure by explicitly encoding rules over discrete concepts~\cite{newell_simon} and are reliable, given suitable abstractions. The symbolic approach governed the AI field till the 90s, when its drawbacks became evident: Symbolic systems struggle with ambiguity, contextualization, and the fluidity of real-world knowledge~\cite{HARNAD_1990}. In addition, computational complexity limits scalability of systems that are already prone to brittleness: Complete symbolic grounding of a knowledge base leads to a worst-case combinatorial explosion~\cite{CHEN1994467}.

In contrast, neural approaches rely on multidimensional embeddings as their representations and approximate knowledge grounding through gradient-based learning over a continuous parameter space~\cite{LeCun2015}. They are robust against outliers and inaccuracies in data, and scale learning and inference well~\cite{10.1093/nsr/nwac035}. Modern deep learning (DL) excels in domains such as image classification~\cite{NIPS2012_Krizhevsky} and machine translation~\cite{bahdanau2016neuralmachinetranslation}. Neural systems are efficient learners but forfeit transparency: Their decision pathways remain opaque and lack the verifiable interpretability of symbolic inference~\cite{tran2025reasoningneurosymbolicai}. They tend to memorize, thus undermining reliable generalization beyond observed facts, especially in out-of-distribution domains. Furthermore, while probabilistic and approximate inference accommodates ambiguities, it yields imprecise logical inference. 

A fundamental trade-off is apparent: Symbolic reasoning offers precision and interpretability but lacks scalability, while neural reasoning is scalable and flexible but lacks transparency. The central question remains: Which representation best fits a given task? Tasks that involve complex, high-volume, and noisy data, particularly those centered on pattern recognition, primarily favor neural DL systems. In contrast, symbolic forms are best suited to problems that require human interpretability or necessitate verifiable reasoning, e.g., in critical decision-making domains~\cite{Rudin2019}. Tasks that require explainability or involve abstract symbolic data structures~\cite{WolframMathematica} naturally favor symbolic systems, which are inherently self-explanatory~\cite{10.1093/nsr/nwac035}. For instance, graph-based symbolic reasoning reduces many queries to reachability checks, yielding verifiable answers. Due to these complementary advantages and limitations, researchers are increasingly focused on neurosymbolic integration~\cite{tran2025reasoningneurosymbolicai} for complex AI challenges that cannot be solved by either approach individually.


\subsubsection{KG with Neural Network: Unifying the Representations}
\label{subsubsec:unifying}

In many tasks, the solution is not to pit symbolic and neural approaches against each other but to combine them in hybrid, modular systems. Neural networks discover statistical patterns through gradient descent; symbolic layers then manipulate extracted structure efficiently~\cite{neurosymboic_3rd}. This integration enables knowledge extraction from neural models. Within such a neurosymbolic framework, KGs can naturally serve as symbolic memory and rule repositories. Coupled with neural networks, they provide modularity and cross-representation translation. KGs may handle tasks better suited to symbolic representation, mitigating the weaknesses of implicit neural reasoning. Moreover, when a KG is distilled directly from a neural network itself, it provides a transparent view of the learned representations of the model, fostering trust and enabling cross-domain transfer.
By decoupling learning (implicit, neural) from reasoning (explicit, symbolic), KGs address interpretability, verifiability, and factuality gaps in modern AI systems that are dominated by neural approaches~\cite{besold2017neuralsymbolic}. 

\subsubsection{KG Applications in Neurosymbolic Frameworks}
\label{subsubsec:kg_applications}
KGs with high-quality expert knowledge enable a range of downstream applications. 

\paragraph{Explicit reasoning and knowledge transfer:} Symbolic inference over KGs is efficient and interpretable~\cite{ns_methods_kg_reasoning}. For instance, retrieving all drugs ($d$) that (1) target a protein ($p$) associated with Type 2 Diabetes and (2) are approved by the FDA, expressed as the conjunctive query:
\begin{equation}
\exists\ p, d \;
\big(
    \text{associated\_with}(p, \text{Type\_2\_Diabetes}) \;
    \wedge \;
    \text{targets}(d, p) \;
    \wedge \;
    \text{approved\_by}(d, \text{FDA})
\big)
\end{equation}
Explicit rules can also be transferred across related domains. Neural reasoning complements this by leveraging KG embeddings to exploit graph structure while ignoring explicit constraints, enabling it to handle ambiguity and subtle semantic variations~\cite{ZHANG202114}. 
 
\paragraph{Reliability in critical fields:}
In sensitive domains, auditable and editable KGs can serve as a persistent knowledge base: Facts can be inspected, verified, and updated directly~\cite{pan_et_al:TGDK}. LLMs, by contrast, embed knowledge implicitly in parameters, making tracing and verification difficult~\cite{akyurek-etal-2022-towards}, and are prone to hallucinations~\cite{Huang_2025_survey}. Updating LLMs is resource-intensive, requiring fine-tuning or retrieval-augmented generation (RAG), and resolving contradictions demands complex methods such as context-aware decoding. Verification of generated content against a factual KG minimizes the risk of inaccuracies or hallucinations~\cite{luo2024rog, hron2024training}. Furthermore, a KG offers the ``ability to forget'': Information can be erased if required by legal regulations or upon user request. By contrast, removing knowledge from LLM requires complex interventions and sophisticated strategies that risk catastrophic unlearning~\cite{si2023knowledgeunlearningllms}, raising concerns about access to harmful content, user privacy, and copyright violations~\cite{tian-etal-2024-forget}.

\paragraph{RAG factuality and scalability:}
To enhance factuality, RAG can be replaced with GraphRAG~\cite{edge2024localglobalgraphrag}. In standard RAG, answer accuracy depends heavily on the relevance and quality of retrieved text. This often fails when information is buried in long passages or scattered across documents. Further, vector-based RAG depends on textual embeddings that may fail to correlate semantically similar concepts dispersed in spatiotemporally disparate sources. KGs provide an anchoring structure for LLMs to maintain context, and make evidence clear, verifiable, and lightweight for efficient retrieval. This improves scalability and lowers generation costs. Recent work by~\citet{brissette2024LLMKG} shows GraphRAG outperforming vector RAG and HybridRAG~\cite{sarmah2024hybridrag} on arXiv datasets, demonstrating superior factual accuracy and reasoning. These results highlight GraphRAG’s potential, especially for tasks requiring multi-hop reasoning and sustained context understanding.

\paragraph{Reinforcement learning:}
KGs are effective for policy-guided walks in reinforcement learning with logical rules~\cite{Uddin2025, 10.1007/978-3-030-77385-4_22}, as they encode explicit relations and allow rule-based constraints on transitions. This structured setting maintains logical consistency and supports optimal path learning. By contrast, using unstructured text as the environment leads to noisy and inconsistent transitions, making rule-aware exploration more challenging.

\paragraph{Interpretability of neural network decisions:}
Extracting a KG from a model, which externalizes neural associations into explicit, structured relations, unveils acquired knowledge~\cite{swamy2021interpreting-llm_kg-extraction}. It maps hidden representations onto human-understandable concepts, enabling tracing of predictions, post-hoc explanation, and auditing. Unlike raw embeddings or hidden activations, which remain opaque, KGs provide a symbolic layer for inspecting reasoning chains, thus bridging neural representations with semantic knowledge. 

\paragraph{Discoveries:}
KGs uncover hidden insights by linking concepts, which appear unconnected, through explicit graph paths. This connectivity underpins applications such as recommendations, where relevant entities are suggested based on graph neighborhoods, and analogical discovery, where cross-domain parallels emerge through reasoning over paths. Unlike unstructured text or embeddings, KGs provide verifiable relations, enabling trustworthy discoveries. In biomedicine, for example, a path like drug → protein target → pathway → disease can reveal repurposing opportunities, such as a hypertension drug acting on pathways implicated in diabetes.

\subsection{KGs for Domain-specific Superintelligence}
\label{subsec:superintelligence}

In the near future, a natural scarcity of high-quality text sources may impede the scalability of ever-larger LLMs, necessitating a paradigm shift that favors smaller language models with domain-specific expertise. This would shift focus from general-purpose artificial general intelligence to domain-specific superintelligence: from breadth to depth. 

Superintelligence~\cite{bostrom2014superintelligence}, an entity whose intelligence surpasses human capability and may diverge from anthropomorphic reasoning, could excel in task-specific reasoning, generate disruptive insights, and exceed human-level creativity~\cite{Szocik2020}. A recent study by~\citet{dedhia2025} illustrates one path toward this goal by translating multi-hop KG paths to composite natural language statements, effectively linking knowledge depth to KG path length. They demonstrate how to boost reasoning ability in a chosen domain using a high-quality domain-specific KG: fine-tuning a small language model based on deep, multi-hop KG knowledge. The longer the paths used for fine-tuning, the better their reasoning --- a key property for domain-specific superintelligence. Seen from this perspective, conventional text datasets resemble one-hop knowledge: They only touch the surface and thus produce models that elicit surface-level understanding. In this context, a high-quality, domain-specific KG that explicitly encodes the wealth of knowledge through concise and unambiguous semantic relationships serves as a powerful tool for unlocking deeper intelligence. Such a structured knowledge simplifies learning of meaningful semantic representations, particularly where models would otherwise be overwhelmed with the syntactic complexity of lengthy yet less informative sentences. This indicates that a scalable method for automatic extraction of high-quality KGs would open up a path to scalable superintelligence.

\subsection{Existing KG-extraction Methods}
\label{subsec:existin_kg_extraction_methods}

Next, we briefly review existing KG-extraction methods. For further details, we direct the reader to a comprehensive review on KG construction~\cite{survey_atuomatic_kgc}. We discuss the following categories: (1) task-specific natural language processing (NLP) methods, (2) triple embedding-based, and, the most recent, (3) generative, or LLM-based, which we discuss in greater detail, given its current prominence.

Given the growing attention to LLM-driven KG construction~\cite{LLM_for_kg_construction}, we further differentiate \emph{KG extraction} from \emph{KG generation}, to clarify their respective advantages, limitations, and application scopes. We classify methods where an LLM plays a pivotal role in the KG construction pipeline, either generating triples conditioned on input texts or distilling knowledge from model weights, under the category of KG generation. We refer to all other approaches as KG extraction.

\subsubsection{Task-specific NLP Methods Scale Badly}
\label{subsubsec:nlp_methods}
Early rule-based information extraction systems demanded heavy feature engineering and domain expertise. Modern pipelines sequentially chain machine learning components---named entity recognition, coreference resolution, and relation extraction~\cite{Jaradeh2023}---often relying on structured or semi-structured data~\cite{info15080509}. These systems require sophisticated text preprocessing heuristics~\cite{brissette2024LLMKG}, e.g., as in the case of conditional random fields. Long short-term memories and convolutional neural networks introduce locality bias. Another drawback is that errors propagate over the pipeline. Overall, these methods can be accurate, but are very labor-intensive, not fully automatic, and hard to scale. 

\subsubsection{Triple Embeddings are Local, Closed-domain, and Miss Long-range Dependencies}
\label{subsubsec:triple_embeddings}
An embedding-based approach seeks to train ML methods on KGs to capture semantic and structural patterns of the graph into embeddings by optimizing a scoring function. Embeddings enable the model to predict missing links (triple completion) and estimate the likelihood of new relations (link prediction; ~\citealt{ns_methods_kg_reasoning}). However, this approach suffers from several limitations, including selection bias, lack of scalability, brittleness to KG errors, and limited external/world knowledge~\cite{reasoning_over_kg_with_logic_2025}. Because most KG embedding models operate on local triple patterns, they struggle to compose long multi-hop chains, handle negation, or respect ontological constraints --- particularly when relations are n-ary, qualified (e.g., temporal, provenance), or context-dependent. They also assume a largely closed-world, static graph: Cold-start entities/relations and evolving KGs typically require expensive retraining or ad-hoc heuristics, and performance degrades under distribution shift. More concretely, embedding-based approaches face the following limitations.

\paragraph{Sparsity, limited information, and vocabulary:} The scale of the largest publicly available KGs, e.g., Wikidata (118M+ entities), PubGraph (385M+ entities; ~\citealt{ahrabian2023pubgraph}), is of the order of $10^8$ entities. The scale is incompatible with text corpora sizes: In 2024, top-tier LLMs reported up to $10^{13}$-token training datasets~\cite{position_data_scale}, and the pretraining corpora of leading projects can collectively surpass 700TB~\cite{liu2024datasetslargelanguagemodels}.

\paragraph{Insufficient utilization of semantic information:} Learning an embedding representation that incorporates equally good graph structural and semantic information remains challenging. This is an active research frontier itself. Multiple efforts are targeted at developing architectures and approaches that produce embeddings that are not overly localized~\cite{10490589}, incorporate multiple relation types~\cite{translational_kgc_model}, and better integrate contextual (including semantic) information for improved reasoning~\cite{10742302}. This suggests that embedding methods are useful in task-specific applications based on KGs, but on their own, fall short in extending KGs.

\paragraph{Generalizability:} Embedding methods do not generalize well across different KGs. Each KG is characterized by its own set of relations, attributes, and ontology, making it impossible or impractical to unite many KGs for training. In practice, embeddings entangle schema-specific signals (relation vocabularies, type hierarchies, qualifier formats). Hence, representations learned on one KG transfer poorly to another with different ontologies or naming conventions. Cross-KG use then requires costly alignment steps --- entity linking, relation mapping, and negative-sampling redesign --- and even after alignment, out-of-vocabulary entities/relations and schema drift often degrade performance~\cite{chen2023generalizing}.

\subsubsection{LLM-based KG Generation is Not Reliable}
\label{llm_kg_is_not_reliable}

With the tremendous success of LLMs on all kinds of NLP tasks, modern research on KG extraction is skewed towards extracting relational knowledge from LLM weights using prompts~\cite{LLM_for_kg_construction, carta2023iterativezeroshotllmprompting, LI2025104769, 10.1145/3691352}. The widespread adoption of LLMs can be attributed to their versatility across tasks, adaptability to diverse domains, and ease of use, which together make them attractive as general-purpose AI systems. LLMs capture relational knowledge unevenly: They are more accurate for some types (e.g., commonsense or hierarchical `is-a’ links) and less accurate for others (e.g., detailed encyclopedic facts or multi-hop traversal relations;~\citealt{pan_et_al:TGDK}).
In addition, a range of inherent drawbacks raises concerns about the use of LLMs for reliable domain-specific KG generation. The drawbacks we  discuss next appear to be intrinsic to generative methods of KG construction.

\paragraph{LLMs are brittle with respect to prompts:} Instruction fine-tuning does not fully address this problem~\cite{natureZhou2024}. KG extraction with prompts is biased towards prompt structure~\cite{cao-2021-knowledgeable}. LLMs are sensitive to task-framing: answer consistency can shift with small syntactic changes~\cite{hagstrom-etal-2023-effect}, with slight prompt variations~\cite{mousavi-etal-2024-dyknow, wang2024astute}, and even with respect to basic logical constraints~\cite{ghosh2025logical}. Retrieval augmentation can mitigate inconsistency and knowledge-cutoff issues~\cite{shuster-etal-2021-rag}, but introduces new failure modes: conflicts between retrieved evidence and LLM's parametric knowledge~\cite{DBLP:conf/aaai/0013XXZJC0W25, zeng-etal-2025-towards}, imperfections in retrieval and ranking~\cite{jin-etal-2024-bider}, and weak relevance estimation, resulting in incorrect utilization of the retrieved knowledge~\cite{wang2024rear}.
  
\paragraph{LLMs hallucinate:} Beyond reproducibility, LLMs hallucinate outputs that are nonsensical or unfaithful to the provided source content~\cite{10.1145/3571730, zhang2024knowledgeovershadowing, li-etal-2024-dawn}. Hallucinations persist regardless of model size or training data scale~\cite{shuster-etal-2021-rag}. The current consensus is that hallucinations are an inherent, unavoidable property of LLMs. Some scholars formally show that, from the theoretical viewpoint, hallucinations may be innate to probabilistic generative methods \cite{xu2025hallucinationinevitable}. All methods to strengthen LLM reasoning, such as chain-of-thoughts with self-consistency, fine-tuning, augmented generation, and greedy decoding, improve accuracy, but are unable to eliminate nontrivial hallucinations~\cite{kim2025medicalhallucinations}. In the context of KG generation, LLMs struggle with inverse inference (the ``reversal curse''): They may learn $\langle \text{A},\ \text{is-a},\ \text{B}\rangle$ yet fail to infer the inverse relation~\cite{2024reversalcurse}, undermining triple extraction in domains where inverse relations are common and semantically critical. 

\paragraph{LLMs are not factually accurate:} Factuality and hallucination are distinct: Factuality errors occur when a model fails to learn or apply factual knowledge accurately, whereas hallucinations are ungrounded or unfaithful content relative to the provided source~\cite{2024-factuality}. As demonstrated in our motivating example (Sec.~\ref{sec:motivation}), even state-of-the-art models can return factually divergent answers to equivalent queries. Because models acquire factual knowledge during pretraining and can add more via continued pretraining~\cite{fact_knowledge_pretrain}, rigorous dataset cleaning is essential. Yet, verifying or synthesizing high-quality data at the LLM scale is infeasible in practice, creating a fundamental size-quality trade-off (see Sec.~\ref{subsec:impact_of_data}). No current LLM offers the factuality needed for trust. Even the most advanced commercial systems make significant factual errors, which spike on rare entities and remain less factual than humans~\cite{min-factscore}, with mistakes persisting even in search-augmented tools~\cite{wired_perplexity_2024}. Consequently, any method that relies on LLMs needs robust verification. In high-stakes domains (e.g., medicine~\cite{Thirunavukarasu2023LLM}, autonomous driving, aeronautics, and cybersecurity), verification alone is insufficient: Outputs must also be interpretable and explainable for decision-makers. However, today's LLMs provide little insight into their decision rationale~\cite{madsen-etal-2024-self, osti_10380030, bowman2023thingsknowlargelanguage}, falling short of the guarantees these applications demand.

\subsection{Impact of Data Quality and Dataset Size}
\label{subsec:impact_of_data}

Prior work shows that state-of-the-art LLM capabilities emerge only when model size, dataset scale, and compute reach sufficient magnitude~\cite{NEURIPS2020_gpt3, kaplan2020scalinglaws, wei2022emergentabilities}. As a result, modern pretraining corpora often favor scale over domain fidelity. Given LLM propensity to memorization, flawed data sources with misinformation and biases are a primary driver of hallucinations~\cite{Huang_2025_survey}. Domain adaptation with fine-tuning~\cite{hu2022lora} can improve factuality and coherence, but risk catastrophic forgetting and cross-domain interference~\cite{wang-etal-2024-role}; continued pretraining adapts knowledge more smoothly to a target domain, but demands substantial additional data. In medicine, researchers warn that scarcity of diverse, high-quality data at the scale required by LLMs leads to a ``garbage in, garbage out'’ dynamic and remains a key barrier to clinical LLM deployment~\cite{Thirunavukarasu2023LLM}. Compounding this, large-scale public text corpora lack established cross-verification mechanisms. Privacy and copyright concerns restrict access to private datasets for training purposes~\cite{pereira2022, wang-etal-2024-notechat}. In cases where models are trained on closed-source training data that are not accessible for scrutiny, the lack of transparency blocks the public's ability to conduct a thorough investigation~\cite{nguyen-etal-2024-culturax}.

As a response to the limited availability of verified sources~\cite{gandhi-etal-2024-better, surveydatasynthesis} and unaffordable training costs, a growing line of work seeks to maximize LLM performance with less data, emphasizing the quality-quantity trade-off. The NLP field's post-GPT-3 trajectory~\cite{NEURIPS2020_gpt3}, prioritized ever-larger unlabeled web corpora, arguably underweighting data quality. Recent efforts formalize data-quality criteria~\cite{zheng2025properdatasetvaluation} and show that small, high-quality datasets can outperform substantially larger but unvalidated corpora~\cite{iskander-etal-2024-quality}; likewise, compact models trained on carefully curated, domain-specific text sometimes surpass frontier general-purpose models on in-domain tasks~\cite{Kadosh10938441, chen2023maybe05dataneeded}. 

Our approach also advocates for ``data quality first'': We contend that high-quality data --- not the sheer volume of data --- is crucial to creating a reliable KG. 

\subsection{GraphRAG}
\label{subsec:graphrag}
\paragraph{Retrieval-augmented generation:}
RAG enhances the capabilities of LLMs by connecting them to external data sources. In a typical RAG system, a user's query is used to retrieve relevant documents or text chunks from a large corpus. This retrieved information is then combined with the original query into a prompt for the LLM, which generates a response grounded in the provided context~\cite{ram2023context}. This approach is particularly effective when the knowledge base is too large to fit within the LLM context window. The most common retrieval method, vector RAG, involves embedding the text corpus into a vector space and retrieving chunks that are semantically closest to the query vector~\cite{gao2023retrieval}. However, this approach struggles with queries that require a holistic understanding of the entire dataset.

\paragraph{GraphRAG:}
To strengthen standard retrieval, many systems incorporate KGs. GraphRAG \cite{edge2024localglobalgraphrag} leverages the inherent modularity of KGs to support global sense-making in two stages: indexing and querying. In the indexing stage, an LLM builds an entity-level KG from source documents, then partitions the graph into a hierarchy of nested communities based on the density of connections between entities. The LLM then produces bottom-up summaries for each community, yielding a hierarchical summary tree that aggregates local insights into global ones. In the querying stage, GraphRAG extracts a subgraph based on the pre-generated community summaries and the query, and uses that subgraph as context to generate answers. With its hierarchical design, GraphRAG reports improvements over conventional vector-based RAG~\cite{edge2024localglobalgraphrag}.

\paragraph{Using GraphRAG for KG evaluation:}
The global sense-making capability of GraphRAG also enables a unique method for evaluating the quality of the KG and benchmarking it across various tasks. The accuracy of the answers to user queries is directly dependent on the coverage, validity, and factuality of the KG. A poorly constructed KG (e.g., one with incomplete entities or incorrect relationships) results in fragmented, inaccurate, or nonsensical responses. Therefore, by assessing the quality of GraphRAG's output to queries, one can indirectly measure the quality of the KG itself.

\subsection{Graph Attention Networks}
\label{subsec:gan}
The transformer architecture~\cite{NIPS2017_attention_is_all_you_need} has established itself as the de facto standard 
for NLP tasks. Self-attention lets a transformer model long-range dependencies in parallel, dramatically speeding up training compared to their predecessors. Their stackable blocks enable scaling to billions of parameters. Due to their scalability and unprecedented versatility on language tasks, transformers currently dominate the NLP field. Yet, the native transformer self-attention module handles only sequential input.

\subsubsection{Graph Transformer Architecture}
\label{subsubsec:gta}

To enable training of a transformer on graphical input, the graph structure must either be encoded into the input or the attention module must be modified. We do both: (1) encode relation embedding into input graph sequences (see Sec.~\ref{subsubsec:h-gat}) and (2) modify attention weights to reflect spatial distance in input graphs (see Sec.~\ref{subsec:glm_architecture_and_training}). To implement our model, we take inspiration from Graphormer~\cite{NEURIPS2021_graphormer} but design alternative graph encodings tailored to language tasks as well.

\subsubsection{Hierarchical Graph Attention Networks}
\label{subsubsec:h-gat}
To incorporate semantic relations into \ours, we combine a generic transformer architecture with a hierarchical graph attention network (H-GAT;~\citealt{hgat2021, nathani-etal-2019-learning}). H-GAT fuses relation embeddings into semantic graph nodes before passing the graph sequences to the transformer layers. The original H-GAT architecture hierarchically combines intra-relation and inter-relation attention to derive node embeddings by aggregating the embeddings of the neighbors of the graph node. To tailor H-GAT to our input (i.e., chain vocabulary graphs that we describe later), we discard the unnecessary inter-relation representations and use the simplified architecture with token embeddings instead of graph node embeddings.

In the \ours implementation, given a triple $<h, r, t>$ --- head, relation, tail, where head and tail are represented by $\{h_1, .., h_m\}$ and $\{t_1, .., t_n\}$ at the token level, for any given tail token $t_i$ we have:

\begin{equation}
e_{ij}^{(r)} \;=\; \operatorname{LeakyReLU}\!\Big(
  a_r^{\top}
  \big[\, \mathbf{W}_r t_i \mathbin{\|} \mathbf{W}_r h_j \,\big]
\Big).    
\end{equation}


\noindent
where $\mathbf{W}_r$ is a learnable relation embedding matrix, $a_{r}$ is a learnable relation embedding, and $\operatorname{LeakyReLU}$ is an activation function.

\begin{equation}
\alpha_{ij}^{(r)}
\;=\;
\operatorname{softmax}_{j}\!\big(e_{ij}^{(r)}\big)
\;=\;
\frac{\exp\!\big(e_{ij}^{(r)}\big)}{\sum_{k=1}^{m}\exp\!\big(e_{ik}^{(r)}\big)} .
\end{equation}

Then the final node embedding for the tail token is given by:

\begin{equation}
t_i'
\;=\;
\sum_{j=1}^{m} \alpha_{ij}^{(r)} \,\mathbf{W}_r h_j .
\end{equation}

Thus, the new tail token $t'_i$ fuses the relation embedding via $W_r$ and $a_r$ with its initial tail token embedding $t_i$ and all head token embeddings $\{h_1,..., h_m\}$.

\section{Motivational Example}
\label{sec:motivation}
What follows is a motivating example to demonstrate the importance of reliability in our proposed pipeline. To do this, we design a simple ``reverse test'' using Unified Medical Language System (UMLS; ~\citealt{UMLS}) triples. First, we sample a ground-truth triple from UMLS: $\langle$\texttt{chronic kidney disease}, \texttt{has\_finding\_site}, \texttt{kidney structure}$\rangle$ [SNOMED CT United States Edition vocabulary in UMLS lists kidney structure as the only finding site for chronic kidney disease (CKD)]. Next, we manually create a sequence that implies a weak connection between CKD and cerebellar gray matter abnormalities (so that it is appropriate for the \texttt{associated\_with} relation, i.e., $\langle$\texttt{chronic kidney disease}, \texttt{associated\_with}, \texttt{cerebellar gray matter}$\rangle$ would be an ontologically appropriate triple). We craft this sequence so that it reflects recent medical studies on brain imaging in patients with CKD ~\cite{xiao2024brain}, which show indirect abnormalities in the cerebellar gray matter. Even with this indirect correlation, the logical triple should remain $\langle$\texttt{chronic kidney disease}, \texttt{has\_finding\_site}, \texttt{kidney structure}$\rangle$, as found in UMLS.

Next, we prompt strong general-purpose LLMs (Gemini 2.5 Pro, Claude Sonnet 4.5, GPT-5, and Grok 4) to infer the triple from the sentence. Despite fluent rationales, these models frequently hallucinate relations or return ontologically invalid outputs, yielding triples that are non-factual or misaligned with UMLS constraints. Their outputs seem to show spurious correlations instead of semantic connections. In contrast, our \ours model, trained as detailed in the following sections, recovers the correct UMLS triple from the same sentence. This example underscores that adhering to biomedical ontologies matters: Terms like “gray matter” should be used with an \texttt{associated\_with} relation rather than being miscast with \texttt{finding\_site}; structure-aware training is essential for preventing such errors. This is only possible with the proposed \ours pipeline.


% ==========================
\begin{tcolorbox}[enhanced,breakable,colback=gray!5,colframe=gray!40,
                  left=6pt,right=6pt,top=6pt,bottom=6pt,boxsep=0pt]
\noindent\textbf{Prompt.} Please complete the following medical KG triple (head, relation, tail):\\
\texttt{(chronic kidney disease, has\_finding\_site, \ldots)}

\vspace{\baselineskip}
based on the sequence:\\[0.5em]
Chronic kidney disease (CKD) is a renal disorder. However, neuroimaging cohorts of people with CKD have reported subtle, group-level abnormalities within the cerebellar gray matter. Studies note modest reductions or regional alterations in cerebellar gray-matter volume, cortical thickness, and diffusion metrics. These observations are heterogeneous and not diagnostic, yet they place observable findings at the cerebellar gray matter in CKD. Thus, CKD may be associated with cerebellar gray-matter changes.
\end{tcolorbox}
% -------------------------------

\noindent
Different LLMs produced mostly wrong answers to this triple-generation query:

\begin{tcolorbox}[enhanced,breakable,colback=gray!5,colframe=gray!40,
                  left=6pt,right=6pt,top=6pt,bottom=6pt,boxsep=0pt]
\noindent\textbf{Gemini 2.5 Pro: }{\small
(chronic kidney disease, has\_finding\_site, \underline{cerebellar gray matter})\\

The provided text explicitly states that neuroimaging studies ``place observable findings at the cerebellar gray matter in CKD.''
}
\end{tcolorbox}
% -------------------------------

\begin{tcolorbox}[enhanced,breakable,colback=gray!5,colframe=gray!40,
                  left=6pt,right=6pt,top=6pt,bottom=6pt,boxsep=0pt]
\noindent\textbf{Claude Sonnet 4.5: }{\small
Based on the provided sequence, the medical KG triple should be completed as:\\
(pediatric chronic kidney disease (pckd), has\_finding\_site, \underline{kidney})

\vspace{\baselineskip}
Reasoning:\\
The primary and defining finding site for chronic kidney disease is the kidney itself, as explicitly stated in the passage: ``Chronic kidney disease (CKD) is a renal disorder.''
While the passage does discuss secondary neuroimaging findings in the cerebellar gray matter, these are described as:
\begin{itemize}
    \item Subtle and group-level abnormalities
    \item Heterogeneous observations
    \item Not diagnostic
    \item Modest and regional alterations
\end{itemize}
In contrast, the kidney is the primary pathological site where CKD occurs by definition. The cerebellar changes represent associated findings or potential complications, but they are not the primary finding site of the disease entity itself.
For medical knowledge graphs, the has\_finding\_site relation typically indicates the primary anatomical location where the disease process occurs, which for CKD is definitively the kidney.
If you needed to represent the cerebellar findings, that would be better captured with a different relation such as has\_associated\_finding or may\_affect, rather than has\_finding\_site.
}
\end{tcolorbox}

% -------------------------------

\begin{tcolorbox}[enhanced,breakable,colback=gray!5,colframe=gray!40,
                  left=6pt,right=6pt,top=6pt,bottom=6pt,boxsep=0pt]
\noindent\textbf{GPT-5: }{\small
Based on the medical knowledge graph triple format and the provided sequence, the completed triple would be:
(chronic kidney disease, has\_finding\_site, \underline{cerebellar gray matter})
\vspace{\baselineskip}

This is based on the text where chronic kidney disease (CKD) is associated with observable findings in the cerebellar gray matter, specifically mentioning reductions in cerebellar gray-matter volume, cortical thickness, and diffusion metrics.
}
\end{tcolorbox}

\begin{tcolorbox}[enhanced,breakable,colback=gray!5,colframe=gray!40,
                  left=6pt,right=6pt,top=6pt,bottom=6pt,boxsep=0pt]
\noindent\textbf{Grok 4: }{\small
Based on the medical knowledge graph triple format and the provided sequence, the completed triple would be:
(chronic kidney disease, has\_finding\_site, \underline{cerebellar gray matter})
\vspace{\baselineskip}

The triple (chronic kidney disease, has\_finding\_site, cerebellar gray matter) is completed based on the provided sequence, which states that neuroimaging studies of chronic kidney disease (CKD) patients have identified subtle abnormalities in the cerebellar gray matter, such as reductions in volume, cortical thickness, and diffusion metrics. The relation ``has\_finding\_site'' indicates the anatomical location where these abnormalities are observed, making ``cerebellar gray matter'' the appropriate tail entity.
}
\end{tcolorbox}

Next, based on the top 20 predicted tokens from our model with top-5 softmax probabilities of 0.789, 0.173, 0.01, 0.003, 0.003, our pipeline yields ``kidneys'' (with Gemini 2.5 Pro as a helper LLM), which corresponds to the UMLS gold answer:
\begin{tcolorbox}[enhanced,breakable,colback=gray!5,colframe=gray!40,
                  left=6pt,right=6pt,top=6pt,bottom=6pt,boxsep=0pt]
\noindent\textbf{\ours:}{\small\\
structure, kidney, system, function, renal, disease, heart, kidneys, of, physiology, arterial, underlying, pancreas, organ, impaired, health, cerebrovascular, pathophysiology, injury, morphology

\vspace{\baselineskip}
\textbf{Gemini 2.5 Pro reasons over \ours triple candidates:} 

1. Analyze the Input:\\
Sequence: The text describes Chronic Kidney Disease (CKD) as a ``renal disorder,'' indicating its primary location is the kidneys. It also explicitly mentions that in people with CKD, neuroimaging has found ``abnormalities within the cerebellar gray matter.''\\
\ldots\\
2. Generate Potential Tails:\\
The most obvious and primary finding site for a ``renal disorder'' is the kidneys. \underline{The token list contains kidneys. This is a very strong candidate.}\\

I can also form renal system from the tokens renal and system. This is also correct but is more general than kidneys. Specificity is preferred.\\

3. Verify and Filter Tails:\\
Candidate 1: kidneys
\begin{itemize}
    \item Factual: Yes, CKD is a disease of the kidneys.
    \item Supported by sequence: Yes, the text calls CKD a ``renal disorder.''
    \item Valuable \& Specific: Yes, it pinpoints the primary organ affected. It is specific and not vague.
    \item Uses only candidate tokens: Yes, ``kidneys'' is in the list.
    \item Conclusion: This is a high-quality, factual tail.
\end{itemize}
\ldots\\
\underline{The text also mentions findings in the ``cerebellar gray matter.''} I can look for tokens to construct this. Cerebrovascular is available, and structure is available. I could potentially form cerebrovascular structure. However, the cerebellum is not primarily a vascular structure, and this tail is very general and not as precise as the information in the text (``cerebellar gray matter''). \underline{More importantly, the key tokens cerebellar and gray matter are missing. Therefore,} \underline{I cannot accurately represent this secondary finding site.}\\
\ldots\\
4. Final Decision: \textbf{The best and most directly supported tail I can construct from the given tokens is \underline{kidneys}. While the text mentions other sites, the provided tokens do not allow for their accurate creation.}
}
\end{tcolorbox}

As we noted above, there is an indirect correlation between CKD and cerebellar gray matter in the source sentence; however, it does not imply the triple suggested by most of the LLMs in their responses. This illustrative case reinforces recent evidence that LLMs regrettably answer based on word correlations in language rather than honing in on the semantic meaning of text and vernacular syntax, making them surprisingly brittle~\cite{bert_attack}. Recent works also show that, despite the high accuracy on various language tasks, when examined more closely, LLMs are only learning surface-level information, such as word overlap, perplexity, sentence lengths, etc., and not the underlying task at hand~\cite{spurious_corr_lms}. Taken together, the example above and the additional results we present later make us skeptical about deploying these models in high-stakes use cases for constructing reliable, domain-specific KGs.


\section{\ours KG-extraction Framework}
\label{sec:methodology}
This section introduces the KG extraction pipeline and the \ours architecture. It also introduces an LLM-based KG generation method that is used to obtain the baseline KG.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{pictures/methodology.pdf}
\caption{Overview of the \ours framework. It is trained on the fusion of syntactic and semantic examples \textbf{(II)} and augments syntactic data with semantic tails \textbf{(I)}; an LLM helps determine the linguistic structure of tails proposed by \ours \textbf{(III)}.
\textbf{(I)}: Chain graph \textbf{(Ic)} combines syntactic knowledge from text corpora \textbf{(Ib)} with semantic examples and relations from a seed KG \textbf{(Ia)}: Roots hold syntactic knowledge (in orange), sparse leaves hold semantic examples (in blue), and edges encode semantic relations (purple arrows).
\textbf{(II)}: \ours is trained on chain graphs to align semantic examples with their syntactic context \textbf{(IIa)}. It then predicts novel semantic token completions for chain graphs without injections, using their syntactic information as context \textbf{(IIb)}.
\textbf{(III)}: An LLM combines raw semantic token completions from \ours into grammatically well-formed triple tails, producing complete triples. After filtering them by similarity to the source syntactic context and dropping duplicate triples, we obtain the final KG.
}
\label{fig:methodology}
\end{figure}


One of the earliest studies on relational factual knowledge extraction from pre-trained encoder-only models used cloze-style prompts, as discussed by~\citet{petroni-etal-2019-language}. E.g., “\textit{John Lennon plays} [MASK]” to complete $\langle$John Lennon, PersonInstrument, ?$\rangle$. Later work showed that such a prompt-based retrieval is heavily biased toward prompt syntax rather than factual content~\cite{cao-2021-knowledgeable}. For instance,~\citet{Silva_entailment} report that BERT~\cite{devlin2019bert} completes the above example with “\textit{guitar, piano, drums, himself, harmonica},” where syntactically plausible but non-factual predictions like “\textit{drums}” rank high and, surprisingly, more specific prompts, such as “\textit{John Lennon plays instrument [MASK]},” produce irrelevant outputs like “\textit{here, there, too, himself, onstage}.”

These shortcomings stem from the syntactic form of the prompt overshadowing the factual knowledge of the model, which is mostly \emph{semantic}. Our key innovation overcomes this obstacle by implanting relations into an encoder via graph attention and training relation embeddings in a dedicated semantic space under the unified MLM + MNM objectives. Our approach directly teaches the model relational knowledge abstracted away from prompt syntax. In parallel, \ours learns the syntactic structure too and leverages syntactic information as a context for semantic knowledge.

A high-level overview of the pipeline is shown in Fig.~\ref{fig:methodology}. \ours is a multi-directional encoder-only transformer. To complete a triple, it predicts a masked tail (a node in the KG, hence MNM). Just like typically text-based encoder-only models, \ours also learns syntactic representations from text corpora via the MLM learning objective~\cite{devlin2019bert}. To enable an encoder-only extraction, we create a new textual data format that encapsulates semantic triples and engineer \ours to work in this space.

\subsection{Syntactic and Semantic Spaces: Merging Semantic Triples and Syntactic Text into a Unified Graphical Format}
\label{subsec:syntactic_semantic_spaces}

\begin{figure}[t]
\centering
\includegraphics[width=0.6\linewidth]{pictures/connectivity.pdf}
\caption{Chain graph. Roots are in orange, leaves are in blue. Conceptual representation (A, B): term level, each circle is a term. Actual representation in training (C): token level, each square is a token. Each term can be multi-token. (A) No injections, all leaves are empty. (B) One root node has a leaf term. (C) Token-level representation for the 3-leaf case. Here, the leaf in (B) is encoded with a maximum of three tokens and padded to the maximum length if needed. Root term comprises two tokens, and tail term also comprises two tokens that are connected to the first root token.}
\label{fig:connectivity}
\end{figure}

In essence, \textit{\ours performs syntactic-to-semantic knowledge conversion during prediction.} The sentences in the dataset represent the \emph{syntactic space}. The KG triples represent the \emph{semantic space} that includes \emph{semantic relations}. To enable knowledge form conversion, we propose leafy chain graph encoding that unifies the semantic and syntactic representations into a joint representation (see Fig.~\ref{fig:connectivity}) in which chain graph roots lie in the syntactic space and leaves, along with their relations, lie in the semantic space. As we demonstrate later, leaves play a crucial role in training semantic relation embeddings. 

Leafy chain graphs follow a regular structure, which enables sequential encoding of the graphical information. All chain graphs have a fixed number of root nodes; the number of leaves per root node is also fixed. Leaves of the same root are connected, introducing a shortest-path linkage between them. All edges are undirected (the directionality of relations is implied in the architecture).

To create a unified representation for the training data, we parse the dataset into chain graphs with \texttt{<pad>} tokens in all leaf positions, keeping only root nodes non-empty. Next, we populate the empty leaves with semantic nodes and their relations from the seed KG in a separate pipeline step, as detailed in Sec.~\ref{subsec:dataset_preprocessing}. Thereafter, the dataset consists of leafy chain graphs with a \emph{regular structure}. Most leaf nodes are pads, while some contain semantic tail tokens from the seed KG. This regularity in graphical input informs and simplifies the choice of graph encodings in \ours, as discussed further in Sec.~\ref{subsubsec:glm_architecture}.

\subsection{\ours Architecture and Training}
\label{subsec:glm_architecture_and_training}

The core architectural challenge in graph transformer design lies in encoding graphs into a sequential input for attention-based learning, in order to enable robust graphical representations. We discuss this in the next two subsections.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{pictures/graphmert.pdf}
    \caption{Main \ours architectural components. \ours is a RoBERTa transformer with two modifications.
    (I) In the embedding layer, H-GAT encodes semantic triples. (IA) There are leaves connected to a root node; hence, the node feature is equal to the token embedding. (IB) There are leaves connected to a root node; H-GAT fuses leaves, relations, and head embeddings resulting in fused node feature.
    (II) In the attention layers, attention weights are multiplied by a function that exponentially decreases with pairwise distance. They encode graph relations and graph distance, respectively. The input is either a node feature or a fused node feature.}
    \label{fig:graphmert}
\end{figure}



\subsubsection{\ours Architecture with Graph Encodings}
\label{subsubsec:glm_architecture}

The proposed \ours $\mathcal{F}(x, \theta)$, $x$: chain graph input, $\theta$: trainable parameters, is a RoBERTa-style~\cite{roberta2019} encoder-only transformer integrated with H-GAT, trained with the MLM + MNM objective. Fig.~\ref{fig:graphmert} illustrates the \ours architecture. 
Our choice of graph encodings is informed by the regularity of the input graph: The input consists of chain graphs with a fixed number of root and leaf nodes. Therefore, to describe the input graph class, node encoding, semantic leaf relation encodings, and spatial distances between node pairs are sufficient. Though common in graph transformers, degree and sparsity encodings offer little value in our setup. The two core components of \ours that encapsulate graph encoding are the input embedding layer and the attention decay mask.

The embedding layer processes input nodes, concretely, root nodes along with their leaves, and semantic relations for each injected leaf node. For every injected triple, its head (multi-token) lies in the root space and its tail (also multi-token) lies in the leaf space. The embedding block fuses every leaf token embedding with its relation and all its head tokens via H-GAT (see Sec.~\ref{subsubsec:h-gat}):    
\begin{align}
& t'_i = t_i + \text{H-GAT}(t_i, r, \{h_1, .., h_m\}),\\
& dim(t'_i) = dim(t_i), \notag
\end{align}
where $t_i$ is the tail token embedding, $h_j$ is the head token embedding, and $dim$ is the embedding dimension.
The derived embedding replaces the initial leaf embedding, effectively encoding the whole semantic triple into the leaf embedding space, as shown in Fig~\ref{fig:h-gat}. During training, masking leaf nodes enables the training of relation embeddings with backpropagation, as shown in Fig.~\ref{fig:backprop}.

\begin{figure}[t]
\centering
\includegraphics[width=0.9\linewidth]{pictures/h-gat.pdf}
\caption{Semantic embedding derivation on leaves (only three leaves are shown). $h_i$: head token, $l_i$: leaf token, $t$: syntactic context token. For every injected triple, H-GAT fuses each leaf token with the relation and all the head tokens, yielding an embedding of the same dimension as the initial leaf token embedding. The derived embedding replaces the initial leaf embedding.}
\label{fig:h-gat}
\end{figure}

\begin{figure}[t!]
\centering
\includegraphics[width=\linewidth]{pictures/backprop.pdf}
\caption{Relation embedding training. The sequence with updated leaf embeddings is passed to the transformer layers. The masked nodes (both roots and leaves) contribute to the loss calculation. For masked leaves, the gradient flows back to them through H-GAT, updating the relation embeddings.}
\label{fig:backprop}
\end{figure}

The attention decay mask encodes the spatial distance between graph nodes. The core idea is that attention between two nodes should decrease with respect to their distance. Following Gradformer~\cite{gradformer}, we use an exponential function with base $0 < \lambda < 1$ and the shortest path in the exponent. To adjust Gradformer's exponential mask for vocabulary sequence graphs that experimentally need a smoother attention decay with respect to the shortest path, we introduce a square root in the exponent.

The shortest path for every node pair is calculated using the Floyd-Warshall algorithm:
\begin{equation}
sp(i, j) \in \mathbb{R}^{N \times N},\; N = \text{number of input nodes}
\end{equation}

The exponential decay mask is an $N \times N$ matrix defined as:
\begin{align}
& f(sp(i, j)) \triangleq \lambda^{GELU(\sqrt{sp(i, j)} - p)},\; 0<\lambda<1 \\
& GELU = x \Phi(x), \notag \\
& \Phi(x) \text{ --- Gaussian cumulative distribution function,} \notag
\end{align}
where $p$ is a learnable parameter and $\lambda$ is a hyperparameter. For $sp(i, j) \leq p$, the activation function, $GELU$~\cite{hendrycks2023gaussianerrorlinearunits}, zeroes the exponent, making the mask close to zero for nodes with the shortest path less than or equal to the learned $p$. Finally, we multiply the attention weights $A \in \mathbb{R}^{N \times N}$ by the mask elementwise, effectively incorporating spatial distance in the attention mechanism. 
\begin{align}
& \tilde{\text{A}} = \text{A} \odot f,\quad \tilde{A} \in \mathbb{R}^{N \times N}
\end{align}
The exponential mask is shared across all attention layers.

\subsubsection{\ours Training}
\label{subsubsec:graphmert_training}

As explained earlier, we represent each sentence as a leafy chain graph \(G\) whose root nodes are text tokens, and graft semantic leaf nodes from a seed KG onto entity/mention spans via relation edges. \ours jointly pretrains using MLM over syntactic tokens and MNM over semantic leaves: We randomly select spans in either space and train the model to reconstruct the missing words and/or KG leaves. This setup couples the transformer’s token encoder with the H-GAT relation encoder so that the surface form and KG semantics align during pretraining.

\begin{align}
\mathcal{L}_{\text{MLM}}(\theta)
&= - \sum_{t \in M_x} \Bigl( \log p_\theta\!\left(x_t \,\middle| G_{\setminus M_g \cup M_x}\right) \;+\; \mathcal{L}_{\text{SBO}}\left(x_t \,\middle| G_{\setminus M_g \cup M_x}\right) \Bigr), \\
\mathcal{L}_{\text{MNM}}(\theta)
&= - \sum_{\ell \in M_g} \log p_\theta\!\left(g_\ell \,\middle| G_{\setminus M_g \cup M_x}\right), \\
\mathcal{L}(\theta)
&= \mathcal{L}_{\text{MLM}}(\theta) \;+\; \mu\,\mathcal{L}_{\text{MNM}}(\theta),
\end{align}

\noindent
where \(x\) denotes the input token sequence; \(G\) is the text chain graph augmented with KG leaf nodes; \(M_x\) and \(M_g\) are the masked text and leaf spans, respectively; \(x_t\) is a masked token target; \(g_\ell\) is a masked semantic leaf target (as a leaf-span explained below); $\mathcal{L}_{\text{SBO}}(x_t)$ is span boundary loss on $M_x$ (explained below); \(\theta\) denotes all model parameters (transformer + H-GAT); and \(\mu>0\) balances the two losses (we use $\mu = 1$). Both objectives use span-wise masking.

In \ours, H-GAT is responsible for training semantic relation embeddings. Dropout on relation embeddings prevents overfitting on scarce semantic examples. In parallel, transformer attention trains the remaining network parameters, attending jointly to tokens from both the syntactic and semantic spaces. \ours is trained with a span-masking schema. Empirically, span masking tightens alignment among the top-$k$ tokens predicted within a single leaf. Later, better-aligned tokens result in more nuanced combinations when constructing complete tails. We discuss the top-$k$ predicted token combination stage in Sec.~\ref{subsec:pipeline_for_extraction}.

From the syntactic stream, we exactly follow the span masking implementation, which sums regular masked objective with span boundary loss of SpanBERT~\cite{joshi-etal-2020-spanbert}. In the semantic space, however, we introduce a modification: Whenever a leaf span is selected (with a standard MLM/MNM probability of 0.15), we mask all the leaf tokens rather than sampling span length from the geometric distribution, which could mask a leaf subset for a given root. In other words, $M_g$ and $M_x$ follow different masking schemas. The rationale for this stems from backpropagation. Relation embeddings must receive gradients from the entire tail so that they can capture its full meaning, not just fragments from individual tokens. Since the semantic meaning of a relation manifests only across the complete tail expression, masking the whole leaf ensures that gradients reflect the whole semantic unit.  

\subsection{High-quality Text Sources and Dataset Preprocessing for \ours Training}
\label{subsec:dataset_preprocessing}

Our framework focuses on domains with stringent factuality requirements. The compact \ours model makes it feasible to train exclusively on limited expert-verified open-source texts. This approach takes an essential step toward reliable KG extraction: Data cleaning reduces hallucinations and training on scientific corpora can substantially limit domain-specific errors~\cite{li-etal-2024-dawn}. Reliance on expert-verified data prevents importing spurious facts from vast, tainted corpora.

Data quality requirements apply equally to the seed KG. The seed KG is a set of domain-specific triples that serve as initial relation examples for the model. As the full training set in the semantic space, the seed KG defines the relation set for the extracted KG. Its quality, therefore, is of utmost significance in training of robust semantic relation embeddings in \ours. A seed KG can be obtained from an external source, provided that it satisfies the following two conditions:
\begin{enumerate}
    \item It contains clean, domain-specific data.
    \item It has a sufficiently diverse vocabulary.
\end{enumerate}
Condition (1) provides the foundation for learning accurate relation embeddings and Condition (2) prevents relation embedding overfitting on a small set of tokens. To obtain a domain-specific seed KG that meets these requirements, we suggest either selecting a relevant, well-curated external KG or generating one (see Sec.~\ref{subsec:create_llm_extracted_kg}), followed by thorough cleaning. 

After obtaining a KG, we apply a similarity filter to it against the training data (see Sec.~\ref{subsubsec_similarity_matching}), which ensures alignment with the target domain [Condition (1)] and identification of the triples most relevant to the context (see Sec.~\ref{subsubsec:contextual_triple_selection}). The algorithm for selecting the best matches, described in Sec.~\ref{subsubsec_kg_injection}, addresses vocabulary diversity [Condition (2)]. This algorithm selects triples from the external KG to position them within the semantic space in the chain graphs, given the potential spots for triple heads within the syntactic space.  For domain-specific head discovery in the dataset, we use a helper LLM.

\subsubsection{Entity Linking}
\label{subsubsec_similarity_matching}

Although \ours is applicable to any domain-specific text corpora, we leverage biomedical knowledge in the UMLS to develop a multi-stage process to link entities discovered in the text to concepts in UMLS. This process ensures that identified entities are mapped to standardized Concept Unique Identifiers (CUIs), facilitating the subsequent retrieval of structured information from the UMLS KG. We combine both embedding similarity and string similarity.

\paragraph{Stage 1: Embedding-based candidate retrieval:}
The initial stage aims to rapidly identify a set of potential candidate concepts from the vast UMLS ontology. This is achieved by representing both the discovered source entities (queries) and the target UMLS entities in a shared high-dimensional vector space.

We use SapBERT~\cite{lim2022sapbert}, an encoder-only language model specifically pre-trained on biomedical knowledge from UMLS, which is able to capture the nuanced difference between biomedical terms. Each discovered head entity and every UMLS entity is processed through SapBERT to produce a unique vector embedding.

To efficiently search through millions of UMLS entity embeddings, we employ an Approximate Nearest Neighbor (ANN) algorithm. ANN constructs a pre-computed index of the UMLS embeddings, enabling highly efficient retrieval of the top-$k$ most similar vectors for a given query vector. For each discovered entity, we use this method to retrieve the top 10 UMLS candidates based on the cosine similarity of their embeddings.

\paragraph{Stage 2: Fine-grained filtering with string matching:}
The top candidates from embedding-based retrieval are then subjected to a more rigorous filtering process based on string similarity. 
We use character-level 3-grams (char-3grams) as a robust string comparison. Each entity name is represented as a set of char-3grams, which can be easily compared with others using standard set-based similarity metrics. We compute Jaccard similarity between the 3-gram sets of the source entity and each of the 10 candidate entities:
\[
J(A, B) = \frac{|A \cap B|}{|A| + |B| - |A \cap B|}
\]
 
A candidate entity is confirmed as a valid link only if its Jaccard similarity score is greater than the threshold. The threshold is set to 0.5 based on manual inspection. The entities that successfully pass both stages are considered the final Linked UMLS Entities and are used for the following task.

\subsubsection{Contextual Triple Selection}
\label{subsubsec:contextual_triple_selection}

Following the entity linking stage, each input sequence is associated with a set of UMLS concepts. While these links grant access to the structured knowledge within UMLS, a single concept can be involved in hundreds of triples, many of which may be irrelevant to the specific context of the source text. Therefore, a crucial subsequent step is to identify and select only the most contextually relevant triples for each sequence.

We perform this selection using an embedding-based relevance-ranking procedure. For each sequence, we begin by retrieving the complete set of triples from the UMLS KG where any of the linked entities from that sequence appear as the head entity. We compute a semantic relevance score for each triple with respect to the original input sequence. We exclude triples with undesired relations from the search: relations that are not useful to have in the KG (see Table~\ref{tab:relations-split} in Appendix A).

Specifically, each retrieved triple, consisting of a head, relation, and tail, is transformed into a linearized sentence by concatenating its components with spaces. We encode both the original input sequence and the sentence formed by each triple into high-dimensional vectors using the Gemini embedding model, text-embedding-004, and use cosine similarity as the semantic relevance score. For each linked entity, we rank its associated triples by their semantic relevance scores and retain the top 40 triples. The resulting contextually filtered set of triples is then used in the subsequent injection process.


\subsubsection{Seed KG Injection}
\label{subsubsec_kg_injection}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{pictures/injection_process.pdf}
\caption{Data preparation for \ours. To find the most relevant triples, we perform semantic similarity matching of triples to dataset sequences. The triple head should almost literally match one of the entities discovered in Step (I); from them, we pick the top triples whose tails are semantically close to the sequence. All matched triples are subject to the injection algorithm (III), which selects the top-scoring triples and limits the number of equivalent triples. The injected triples together comprise \textit{a seed KG}.}
\label{fig:injection_process}
\end{figure}

The KG injection algorithm prepares a \ours-compatible dataset of leafy chain graphs by selecting relevant triples from an external KG source (potentially, limited in size) based on their similarity score to the input sequence, thresholded with a hyperparameter $\alpha$. At the same time, the algorithm maintains diversity in the injected relations and semantic vocabulary. All triples selected by the algorithm comprise the seed KG. In this process (see~Fig.~\ref{fig:injection_process}), triples are mapped to the chain graph semantic space: The head is placed at a root node and the tail at the root's leaf node. Critically, the injected triples must be contextually relevant to the sequence. This aligns transformer attention with H-GAT during training on the chain graphs, as both attend to the semantic and syntactic spaces simultaneously; otherwise, the attention layers would get a noisy signal from extraneous tokens. Furthermore, because both H-GAT and transformer attention jointly train \ours relation embeddings, alignment between leaf and root tokens enables vocabulary transfer from the syntactic root space into the semantic leaf space. This process forms integrated representations that support the retrieval of novel tails from a shared embedding space during prediction.

\paragraph{Why do we need an injection algorithm?} A naive strategy would be to inject the top-scoring triple for each head out of all the matched ones. We now demonstrate why a triple with the best similarity to a sequence may be suboptimal for both (a) populating the semantic space and (b) \ours training.
\paragraph{(a) Limitations for the semantic space:}
Similarity matching favors frequent terms in the dataset \cite{zhou-etal-2022-problems}. Hence, triples with common domain-specific keywords in tails score highly across many sequences. As a result, a small set of triples achieves high similarity scores across a large number of sequences. If only top matches were to be chosen, these ubiquitous tails would dominate the semantic space, suppressing rarer but semantically valuable tails that introduce novel terms into the semantic space. The scoring is also biased towards classificatory relations like \texttt{isa} or \texttt{inverse\_isa}, since they often restate the head (e.g., \emph{$\langle$fibrosing interstitial lung diseases, isa, fibrosis of lung$\rangle$} in a biomedical KG). Such close textual matches contribute little new information.

\paragraph{(b) Limitations for \ours training:}
Over-injecting frequent tokens leads to a skewed training distribution. If the semantic space vocabulary is dominated by a few tokens, relation embeddings overfit on them, causing \ours to predict a narrow token subset. Likewise, certain relations, e.g., ``isa'' and ``inverse\_isa,'' would dominate the limited spots for injected triples, suppressing all other relations. Thus, \ours will be undertrained on the other relations.

\paragraph{Design goals:}  To address these limitations, we design the injection algorithm around three goals:
\begin{enumerate}
    \item Eliminate low-relevance matched triples.
    \item Select one triple (``inject'') per head out of all matched with the sequence.
    \item Diversify injected relations by balancing examples across all relations.
\end{enumerate}

Goal (1) can be satisfied by thresholding similarity scores. However, Goals (2) and (3) cannot be enforced independently: Selecting only the top match achieves (2) but undermines (3), amplifying certain top-scored relations, such as ``isa''. The algorithm must enforce (2) and (3) jointly, balancing contextual relevance with relation diversity. The proposed KG injection algorithm iteratively drops undesired triples from all matched triples in two interleaving phases: first, by maximizing the score; second, by maximizing the diversity of relations. The surviving triples are ``injected'' into the semantic space and comprise the seed KG. We defer the description and implementation details to Appendix~\ref{app:injection_algorithm}.

\subsection{\ours Pipeline for Knowledge Graph Extraction}
\label{subsec:pipeline_for_extraction}

We distill internal \ours representations from the trained \ours into explicit graph triples by adding leaf nodes. Using purely-MNM prediction, we distill semantic knowledge directly from \ours weights, conditioned on a sequence from which we want to extract a triple. The role of a sequence in our framework is analogous to the role of a prompt in LLM-based KG generation, but our prediction is unambiguous and deterministic. 

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{pictures/methodology_temporal.pdf}
\caption{\ours Pipeline flowchart with temporal execution ordering of the main components.}
\label{fig:temporal}
\end{figure}  

\begin{figure}[t!]
\centering
\includegraphics[width=0.7\linewidth]{pictures/prediction.pdf}
\caption{Prediction of triple tails. The trained \ours predicts the top $k$ tokens for a masked leaf and the chosen relation, resulting in a set of raw triples with the same head.}
\label{fig:prediction}
\end{figure}  

Fig.~\ref{fig:temporal} shows the execution order of the framework components. Fig.~\ref{fig:prediction} illustrates the first pipeline step. Starting with the syntactic corpus (no KG injections), we sample head ($h$) spans from the syntactic space (root nodes) and assign an outgoing relation ($r$). We then create the corresponding tail slot ($t$, leaf) for the chosen relation and initialize it with masks. We then ask the model to predict the masked tail (leaf) conditioned on the head, relation, and the rest of the sequence. The top-$k$ predictions for each masked leaf yield $k$ candidate tokens, which serve as building blocks for tails of each head–relation pair $\langle h, r \rangle$. Next, a helper LLM combines predicted tail tokens into coherent phrases, followed by a cleaning step. We further filter the generated triples by computing semantic similarity between each triple and its source sequence, discarding those with a score below the user-defined \emph{similarity check threshold $\beta$}. \ours may predict a tail that connects a head with a semantically related concept across the training corpus, and hyperparameter $\beta$ regulates the fraction of such triples in the output. A higher $\beta$ yields fewer but more sequence-specific triples, often explicitly included in the text. A lower $\beta$ allows for broader, more general (yet semantically related) triples that may not be explicitly mentioned in the sequence. However, if $\beta$ is set too low, the output becomes flooded with triples that merely restate general truths, reflecting statistically dominant statements in the training dataset.  

The surviving $\langle h, r, t \rangle$ triples expand the KG with novel facts. Importantly, each prediction is traceable to its source sequence, while at the same time accumulating knowledge distilled from the entire training corpus. This global knowledge accumulation contrasts with RAG methods, which remain local to retrieved documents. Further, RAG is a post-hoc data attribution mechanism, unlike ours.

\paragraph{Role of the helper LLM:}
The laborious (grammatical) part of the work is carried out with help from an LLM and the essential (triple extraction) part done with \ours. In this pipeline, the helper LLM performs three auxiliary tasks: discovering head entities, selecting relations for subsequent \ours prediction, and combining single-token predictions into meaningful, relation-aware tail phrases. Critically, the LLM is constrained: It cannot invent new entities or relations, as heads must be present in the dataset, relations are restricted to the seed KG, and only \ours-predicted tokens are allowed to be in tails.

\paragraph{Why do we need a helper LLM for combining tokens?}
An encoder-only model does not address span decoding. Masked span prediction would still be challenging for a small model, given the very limited number of semantic examples. Here, two factors come into play:
\begin{enumerate}
    \item For a coherent span prediction, each token should be conditioned on other tokens. However, in an encoder-only model prediction, each masked token in the span is conditioned on the sequence independently of other span tokens.
    \item In our experiments, training with span masking results in a prediction where tokens in the syntactic space are better aligned with each other within a span; however, the semantic space has orders of magnitude fewer examples ($10^4$ in our experiments). Given the limited scale of the semantic space, how to achieve the same effect on leaves remains an open question.
\end{enumerate}
Thus, training on small corpora to some extent trades English proficiency for data quality. Exploring methods to augment \ours and thereby remove the LLM-based combining-tokens step is an avenue for future work.

\subsection{LLM-generated KG}
\label{subsec:create_llm_extracted_kg}
To provide a fair and robust comparison for our framework, we construct a baseline LLM KG using a standard LLM-based pipeline that follows the GraphRAG indexing methodology, with a filtering step to align its schema with the \ours KG. 

The process begins by segmenting the source documents into smaller, manageable text chunks. An LLM is then prompted to perform open information extraction on each chunk, which involves identifying entities, extracting the relationships between them, and generating short, descriptive summaries for each entity. We retain only the relationships that are part of our pre-defined relation set, discarding all others. This ensures that the resulting LLM KG shares exactly the same relational schema as our framework, enabling a direct and equitable comparison of performance. After filtering, the extracted elements are subsequently aggregated and consolidated into a final graph structure. To handle multiple mentions of the same concept, exact string matching is used to resolve entities into unique nodes. Relationships that are generated multiple times between the same two entities are aggregated into a single edge. 

\subsection{KG Verification}
\label{subsec:verification}
We can subdivide KG verification methods into two categories: \textit{graph-level} and \textit{triple-level}. 

\subsubsection{Graph-level Verification}

This method evaluates the KG as a whole. It focuses on its logical coherence, internal consistency, comprehensiveness, coverage, important aspects of domain-relevant knowledge, and depth (considering whether it contains rich, insightful connections beyond surface-level facts). Graph-level approaches typically operate by retrieving relevant subgraphs and evaluating their quality with respect to these criteria.

\paragraph{GraphRAG:}
We employ GraphRAG to evaluate the KGs and benchmark them across various tasks. The KGs are used as the primary source of information in GraphRAG to answer medical questions, which enables us to compare their effectiveness directly.
In our implementation, we use the Local Search method from GraphRAG and modify it to rely exclusively on the entities and relations in the querying stage. This process begins by identifying a set of entities within the KG that are semantically related to the user query. These entities act as entry points for the retrieval of connected entities and relationships. The retrieved data sources are then ranked and filtered to fit within a single predefined context window, which is used to generate a response to the user query.

\subsubsection{Triple-level Verification}
At the triple level, verification can enhance \emph{factuality} and \emph{validity}, as we describe next. 

\paragraph{FActScore:} 
The FActScore framework~\cite{min-factscore} provides a fine-grained method for evaluating factual precision in long-form LLM outputs. Its principles transfer naturally to KG verification. FActScore evaluates atomic facts, i.e., short, self-contained statements, against a trusted text source that does not have any knowledge conflicts or overlaps. KG triples can be treated as atomic facts of equal importance. In our setting, each triple can be paired with a reliable text source: \ours triples with sequences and LLM triples with short chunks, both drawn from the same trustworthy source. The short context length minimizes conflicts and overlaps.

\paragraph{FActScore*:} 
We follow the \emph{Retrieve $\rightarrow$ LM} variant of automatic evaluation, in which an atomic fact is concatenated with the knowledge source and provided to the model. However, we strengthen triple evaluation with validity: In the prompt, we require verification of triple logical alignment in addition to context support, since the fact may appear in the text, yet the triple may still be malformed. Malformed triples should not be deemed reliable facts and would inflate the score. Because our prompt departs from the original FActScore prompt, we denote the modified version as FActScore*. 
Formally, let $\mathcal{G}$ be a set of triples $\tau$, and $\mathcal{C}(\tau)$ the text source of $\tau$. Then FActScore* for $\mathcal{G}$ is:
\begin{align}
    \begin{gathered}
        f(\tau) = \mathbb{I}[\tau \text{~is supported by~} \mathcal{C}(\tau)], \\
        \text{FActScore*}(\mathcal{G}) =
        \mathbb{E}_{\tau \in \mathcal{G}}[f(\tau)].
    \end{gathered}
    \label{eq:factscore}
\end{align}

\paragraph{ValidityScore:}
Triples should follow semantic rules of a KG. For example, in UMLS~\cite{UMLS}, the triple \textit{$\langle$beta-receptor, part\_of, plasma membrane$\rangle$} is valid, since \texttt{part\_of} denotes a meronymic (structural/spatial) relation: Every instance of the part must be a constituent of some instance of the whole. It follows, then, that \textit{$\langle$beta-receptor, part\_of, adrenergic signaling$\rangle$} is invalid, because it links a physical structure to a biological process. Similarly, \textit{$\langle$beta-receptor, part\_of, human$\rangle$} is factually plausible but still an illegitimate usage, since it violates granularity: The correct wholes are specific structures (e.g., plasma membrane), not entire organisms. 

To quantitatively measure the validity of triples, we propose ValidityScore. It isolates ontological alignment of triples as an independent mode of evaluation. Concretely, we use a strong LLM judge to semantically validate a triple using the following prompt.

\begin{tcolorbox}[enhanced,breakable,colback=gray!5,colframe=gray!40,
                  left=6pt,right=6pt,top=6pt,bottom=6pt,boxsep=0pt]
\noindent\textbf{Prompt.} Evaluate if these medical KG triples are valid (yes/no/maybe) and give a very short reason why: $\langle\text{list of triples}\rangle$.
\end{tcolorbox}
Then ValidityScore counts the number of ``yes'' responses.

\section{Experimental Setup}
\label{sec:experimental_setup}

 To demonstrate the effectiveness of our framework, we extract a high-quality diabetes KG from a \ours-compatible diabetes training dataset obtained from expert-verified sources.
This section provides an in-depth explanation of the proposed training, extraction, and evaluation pipeline.

\subsection{\ours Training and Extraction}
\label{subsec:glm_training_and_extraction}
Next, we discuss training data preparation and triple extraction.

\subsubsection{Training Data Preparation}
\label{subsubsec:training_data_preparation}
We showcase our framework in a sensitive medical domain, where concerns over trustworthiness of AI output remain the main barrier to wider AI adoption~\cite{wang2025safetychallengesaimedicine, mishra-etal-2024-synfac}, despite the undeniable potential of AI in medical practice. As a case study, we set the goal of extracting a high-quality diabetes KG. It all starts with high-quality textual data. We build a \ours-compatible diabetes training dataset (Table~\ref{tab:dataset-size}) from two main sources: (1) peer-reviewed medical abstracts from MEDLINE journals accessed via PubMed Central, and (2) a seed KG derived from the UMLS Metathesaurus~\cite{UMLS}.

\paragraph{Text corpus:} 
MEDLINE is the National Library of Medicine’s bibliographic database and is accessible via PubMed. MEDLINE selects journals based on rigorous criteria and indexes them using MeSH terms. We retrieved diabetes-related papers from PubMed Central (see the query in Listing~\ref{lst:pubmed-query} in Appendix~\ref{app:training_data}), removed non-English records, parsed abstracts using the PubMed parser~\cite{Achakulvisut2020}, lowercased all text, and filtered out opening boilerplate words such as “Abstract,” “Background,” “Introduction,” etc., with a regular expression. The resulting dataset contains 350k abstracts for training and 39k for evaluation, totaling 124.7M and 13.9M tokens, respectively.

\begin{table}[t]
  \centering
  \caption{Dataset size}
  \label{tab:dataset-size}
  \begin{tabular}{l c c c}
    \toprule
    \rowcolor{gray!30}
    & \textbf{Abstracts} & \textbf{Tokens} & \textbf{Sequences} \\
    \midrule
    \textbf{Training}      & 350k      & 124.7M  & 989,666 \\
    \rowcolor{white}
    \textbf{Evaluation} & 39k       & 13.9M   & 110,297 \\
    \bottomrule
  \end{tabular}
\end{table}

\paragraph{Seed KG:} From UMLS, we select
\href{https://www.nlm.nih.gov/research/umls/sourcereleasedocs/current/SNOMEDCT_US/stats.html}{SNOMED CT, US} and 
\href{https://www.nlm.nih.gov/research/umls/sourcereleasedocs/current/GO/stats.html}{GO (Gene Ontology)}
vocabularies because together they cover a broad range of biomedical concepts across clinical documentation, molecular biology, and data exchange. We exclude low-value relations (more details in Appendix~\ref{app:training_data}) and retrieve triples relevant to our dataset sequences based on semantic similarity matching (Sec.~\ref{subsubsec_similarity_matching}) with Gemini text-embedding-004. For matched triples, we use an injection algorithm~(Sec.~\ref{subsubsec_kg_injection}) with a similarity threshold $\alpha$ of 0.55, validated experimentally via grid search using GraphRAG evaluation (see Sec.~\ref{subsubsec:grid_search}). The resulting triples that are injected comprise the seed KG (Table~\ref{tab:qwen32b_alpha}). Statistics per relation are presented in Table~\ref{tab:qwen32b_relstats} (Appendix~\ref{app:injection_algorithm})

\begin{table}[t!]
  \centering
  \caption{Seed KG statistics for $\alpha = 0.55$ (after Qwen3-32B).
  We obtain this seed KG based on the 1.1258E+06 matched UMLS triples with a similarity score greater than or equal to $\alpha$.
  }
  \label{tab:qwen32b_alpha}
  \small
  \setlength{\tabcolsep}{6pt}
  \begin{tabular}{c c c c c}
    \toprule
    \rowcolor{gray!30}
    \textbf{\makecell{Triples\\count}} &
    \textbf{\makecell{Similarity\\score mean}} &
    \textbf{\makecell{Similarity\\score median}} &
    \textbf{\makecell{Similarity\\score max}} &
    \textbf{\makecell{Number of\\relations}} \\
    \midrule
    28533 & 0.613 & 0.605 & 0.848 & 28 \\
    \bottomrule
  \end{tabular}
\end{table}

As a helper LLM, we employ Qwen3-32B-FP8 (an 8-bit quantized version of Qwen3-32B, further referred to as Qwen3-32B), an open-source, advanced, and lightweight LLM with ``thinking mode'' turned on. We always use ``thinking mode,'' unless otherwise specified.

\paragraph{Entity discovery and relation matching:} For head discovery, we prompt Qwen3-32B with each abstract sequence in the dataset with few-shot examples, asking it to search for medical entities that are relevant to diabetes and its comorbidities. From our observations, changing an example in the prompt significantly changes the number of discovered entities. The outputs are validated against sequences of origin to eliminate hallucinated or misspelled entities. For relation discovery, Qwen3-32B is few-shot prompted with a relation list from the corresponding training seed KG to match entities with all relations that make sense for a given entity in the context of the current sequence. Prompts and examples are presented in Appendix~\ref{app:graphmert_prompts}.

All Qwen3-32B runs are performed on the Princeton cluster on one H100 GPU using vLLM with the vendor-recommended sampling parameters: $temperature= 0.6$, $top\_p = 0.95$, $top\_k = 20$, and $min\_p = 0$. In addition, $max\_tokens$ and $max\_model\_len$ are set to 8192, which is sufficient for our generation length.

\begin{figure}[t!]
\centering
\includegraphics[width=0.9\linewidth]{pictures/chain_graph_encoding.pdf}
\caption{Leafy chain graph encoded sequentially: 7-leaf case. The sequence has a fixed length of 1024. The first 128 tokens are reserved for roots and leaves reside in the remaining tokens. The first group of seven leaves belongs to the first root, the second group of seven leaves belongs to the second root, and so on. Each leaf token group is padded to the maximum length of seven.}
\label{fig:chain_graph_encoding}
\end{figure}

\paragraph{Chain graphs for training:} Chain graphs are initialized with 128 root nodes, each connected to seven leaves, leading to a 1024-token sequence (see Fig.~\ref{fig:chain_graph_encoding}). These numbers are chosen to fit GPU memory constraints while providing sufficient room for semantic tokens dedicated to leaves. 

\subsubsection{\ours Training}
\label{subsubsec:results_graphmert_training}
We train \ours with 12 hidden layers, eight attention heads, a hidden size of 512, and an intermediate size of a fully-connected layer of 2048, totaling 79.7M trainable parameters. We use the BioMedBERT tokenizer~\cite{biomedbert}, trained on a vast amount of medical vocabulary, to prevent frequent subword tokenization of common medical terms, which is particularly beneficial for the extraction stage. The tokenizer determines the vocabulary size, which is 30,522.

Training runs for 25 epochs on four H100 GPUs with BF16 precision, totaling 90 GPU hours. We use an instantaneous batch size of 32 per GPU, achieving an effective batch size of 128 through gradient accumulation (steps = 2). We set dropout rates at 0.1 for regular, attention, and activation dropouts. In addition, we set an exponential mask with base $\lambda = 0.6$ and relation embedding dropout of 0.3. We train the model using the cosine learning rate scheduler with the maximum learning rate set to $4 \times 10^{-4}$. We use 500 steps for warm-up. The weight decay at each step is 0.01 times the learning rate at that step. We stop training when the learning rate reaches  $1 \times 10^{-5}$. In the span masking training schema, we limit masked spans to a maximum length of seven, matching the number of leaf nodes connected to root nodes. 

\subsubsection{Triple Extraction}
\label{subsubsec:triple_extraction}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{pictures/triple_formation.pdf}
\caption{I. Forming triple tails for a given sequence with \ours. (1) Given a sequence as a context, a triple head in the sequence, and a relation, \ours predicts the tail token (we obtain the top 20). (2) The helper LLM attempts to combine the tokens into a complete, coherent medical term. It may output several or no tail candidates to complete the triple. (3) We evaluate the similarity score between triples from the previous step and the sequence of origin. Only triples with a score higher than a preset threshold pass.\\
II. An example of a triple extracted with the LLM pipeline (Qwen3-32B) from the same context. Here, LLM misinterprets the ``has\_finding\_site'' relation, treating ``site'' as a location instead of an anatomical structure, which results in an invalid triple.}
\label{fig:triple_formation}
\end{figure}

The triple extraction pipeline runs the following steps, as shown in Fig.~\ref{fig:triple_formation}. We begin with a leaf-masked prediction over the training dataset, given head entities and their relations. This produces a vocabulary distribution for each masked leaf. From this distribution, we select the top 20 tokens per leaf and use them to prompt the helper LLM, Qwen3-32B. Conditioned on the head, relation, and the originating sequence, the LLM combines these tokens into coherent, relevant, and medically meaningful multi-token tails (see Appendix~\ref{app:graphmert_prompts} for the prompt specification). When no valid tail can be formed, the corresponding $\langle$sequence, head, relation$\rangle$ is skipped. Next, since the LLM may hallucinate tails outside the \ours-predicted token space, we discard any output tails that contain out-of-scope tokens. The output of this stage is a set of completed candidate tails.

Next, each candidate triple is evaluated with a similarity matching algorithm (Sec.~\ref{subsubsec_similarity_matching}). Specifically, we compute the cosine similarity between the triple and its originating sequence using Gemini embeddings. All triples with a score below the similarity check threshold $\beta=0.67$ (obtained through grid search, see Sec.~\ref{subsubsec:grid_search}) are discarded. The remaining set forms the final collection of extracted triples.

\begin{table}[t]
  \centering
  \caption{Examples of \ours-extracted triples (UMLS-style) from a single sequence. We pass the sequence with two marked heads together with relations for these heads to the trained \ours. After \ours makes a prediction in the semantic space, we use the top 20 predicted tokens to form complete triples with a helper LLM. ``\#\#'' is a separator for subword tokens.}
  \label{tab:seq-triples}
  \small
  \rowcolors{1}{gray!10}{white}
  % Column spec: first col = p{0.30\textwidth}, second col = wide (2X)
  \begin{tabularx}{\textwidth}{p{0.25\textwidth} >{\raggedright\arraybackslash}X >{\raggedright\arraybackslash}X}
    \toprule
    \rowcolor{antiquewhite!50}
    \multicolumn{3}{>{\raggedright\arraybackslash}p{0.98\linewidth}}{
      \textbf{Sequence 1:} \ldots\#\#2 The Authors. The Journal of Pathology published by John Wiley \& Sons Ltd on behalf of The Pathological Society of Great Britain and Ireland.
      \textcolor{burntorange}{\bf Non-alcoholic fatty liver disease} (NAFLD) is one of the main causes of chronic liver disease worldwide. Flavonoids, a group of natural compounds, have garnered a great deal of attention in the management of NAFLD because of their profitable effects on glucose and lipid metabolism, inflammation, and oxidative stress which are the pivotal pathophysiological pathways in NAFLD. \textcolor{burntorange}{\bf Naringenin} is a citrus-derived flavonoid with a broad spectrum of potential biological effects including anti-inflammatory and antioxidant properties, which may\ldots
    }\\
    \midrule
    \rowcolor{gray!30}
    \textbf{Head, Relation} & \multicolumn{2}{c}{\textbf{Top-20 \ours-predicted tokens}} \\
    \midrule
    \textcolor{burntorange}{naringenin}, isa &
      \multicolumn{2}{>{\raggedright\arraybackslash}p{0.7\textwidth}}{\textcolor{blue}{flavonoid} \#\#in flav nar \#\#idin \#\#arin \#\#flav compound \#\#ce hydrolase plant \#\#ingen \#\#ono protein polysaccharide polyphenol family \#\#anol - \#\#onin} \\

    \textcolor{burntorange}{naringenin}, plays\_role &
      \multicolumn{2}{>{\raggedright\arraybackslash}p{0.7\textwidth}}{\textcolor{blue}{therapeutic} neuroprotective \#\#ingen antidepressant \textcolor{blue}{role} medicinal \#\#arin flavonoid action baical inhibitory antibacterial bioactive medicine antimicrobial \#\#idin quercetin \#\#flav potential flavon} \\
    
    \textcolor{burntorange}{naringenin}, has\_disposition &
      \multicolumn{2}{>{\raggedright\arraybackslash}p{0.7\textwidth}}{nar \textcolor{blue}{flavonoid} \#\#in hydrolase \#\#arin inhibitor - \#\#anol compound \#\#idin amy \#\#flav \#\#ingen derivative amide receptor acid family alkaloid product} \\

    \textcolor{burntorange}{non-alcoholic fatty liver disease}, cause\_of &
      \multicolumn{2}{>{\raggedright\arraybackslash}p{0.7\textwidth}}{liver disease alcoholic fatty \#\#osclerosis nafld steatosis disorder \#\#atitis \#\#ohep hypercholesterolemia hepatic - with syndrome mellitus myopathy \textcolor{blue}{fibrosis} diseases hyperlipidemia } \\
    
    \textcolor{burntorange}{non-alcoholic fatty liver disease}, associated\_with &
      \multicolumn{2}{>{\raggedright\arraybackslash}p{0.7\textwidth}}{hyperlipidemia disorder alcoholic \textcolor{blue}{obesity} - dyslipidemia myopathy liver hereditary hypercholesterolemia syndrome \#\#ament \#\#tr associated \#\#lip fatty disease \#\#oid mellitus related} \\

    \midrule
    \rowcolor{gray!30}
    \textbf{Head} & \textbf{Relation} & \textbf{Tail (formed from the \ours-predicted tokens)} \\
    \midrule
    \textcolor{burntorange}{naringenin} & isa & \textcolor{blue}{flavonoid} \\
    \textcolor{burntorange}{naringenin} & plays\_role & \textcolor{blue}{therapeutic role} \\
    \textcolor{burntorange}{naringenin} & has\_disposition & \textcolor{blue}{flavonoid} \\
    \textcolor{burntorange}{non-alcoholic fatty liver disease} & cause\_of & \textcolor{blue}{fibrosis} \\
    \textcolor{burntorange}{non-alcoholic fatty liver disease} & associated\_with & \textcolor{blue}{obesity} \\
    \bottomrule
  \end{tabularx}
\end{table}

Table~\ref{tab:seq-triples} illustrates the step-by-step triple extraction process for a representative sequence. Given a sequence with its head and relation, \ours predicts a token in a masked tail. From the output distribution, we select the top 20 tokens as candidate tails, striking a balance between prediction quality and diversity, and providing a sufficient pool for subsequent token combination. We may construct zero, one, or several novel triples out of these.

\subsection{LLM-extracted KG}
\label{subsec:llm_extracted_kg}
Following the default GraphRAG indexing parameters, our diabetes corpus is split into 2,000-token chunks and processed by Qwen3-32B to extract entities and relationships. We enable the model's thinking mode and set $temperature= 0.6$, $top\_p=0.95$, $top\_k=20$, $max\_tokens=8192$. The detailed prompt for extraction is shown in Appendix~\ref{prompt:GraphRAG-index} and an example is shown in Appendix~\ref{prompt:GraphRAG-index-example}. After parsing and cleaning, the final LLM-generated KG contains 272,346 triples.
% 118,045 entities in the LLM KG

\subsection{Evaluation with GraphRAG}
\label{evaluation_graphrag}

Next, we provide details of GraphRAG evaluations.

\subsubsection{GraphRAG Settings} 
Our experimental setup uses Qwen3-14B as the backbone LLM in GraphRAG, with inference accelerated using the vLLM library. For all evaluations, we enable the model's thinking mode and set $temperature= 0.6$, $top\_p=0.95$, $top\_k=20$, and $max\_tokens=8192$. To ensure the reliability of our findings, each experiment is conducted three times with different random seeds (1, 2, and 3). We report the average accuracy across these runs in our final results.

The GraphRAG query process is configured with nomic-embed-text-v1 as the embedding model. To construct the context for each query, the system retrieves the top 30 entities and the top 10 relationships per entity, with the maximum context length capped at 12,000 tokens. Furthermore, we tailor the system prompts and simplify the output table structure to better suit our tasks. The complete prompt is given in Appendix~\ref{prompt:GraphRAG-query}. The modified table schema is also detailed in Appendix~\ref{prompt:GraphRAG-context}.

\subsubsection{Benchmark Evaluation} 
\label{subsec:benchmark}
We further verify the quality of our extracted KGs on the diabetes subsets of medical benchmarks: ICD-Bench~\cite{dedhia2025}, MedMCQA, MedQA, and MMLU (medical). ICD-Bench is a targeted question-answering benchmark aligned with the International Classification of Diseases (ICD) taxonomy~\cite{whoICD10}, designed to evaluate domain-specific medical reasoning in language models across 15 medical sub-specialties. We primarily evaluate the extracted KGs on ICD-Bench, as it is the only benchmark that has a dedicated endocrinology subset; for this reason, we defer the other results to Appendix~\ref{app:public_benchmark}. For evaluation, we employ GraphRAG to answer benchmark questions.  

To address the domain mismatch between our diabetes-specialized model, \ours, and general medical benchmarks, we create domain-specific evaluation subsets. First, we synthesize our training corpus into an approximately 2000-word summary using Gemini 2.5 Pro. Subsequently, we use this summary to prompt a Qwen3-32B model to filter the question-answer pairs in each benchmark, retaining only questions relevant to the summary. All GraphRAG evaluations are then conducted on these filtered subsets to ensure a fair assessment.

\subsubsection{Traceability and User Verification} 
Allowing users to judge correctness is a simple but powerful principle. OpenAI’s WebGPT~\cite{nakano2022webgpt} addresses this by browsing the web and citing sources, a strategy also adopted by Perplexity AI. In these systems, the user decides (1) whether the source is credible and (2) whether the output is factual. Our framework builds on this idea: Each triple is directly traceable to its originating sequence. This enables automatic cross-checking, rather than searching external sources. The system retrieves the sequence from which the triple is derived. Since this sequence originates from a verified paper abstract, users can further validate the fact by consulting the source publication if needed.

\section{Experimental Results}
\label{sec:experimental_results}
Next, we provide evaluation results for \ours and LLM KGs on the benchmarks.

\subsection{Description of the Extracted KG}
\label{subsec:kg_descritption}

Table~\ref{tab:extracted-kgs-stats} summarizes the statistics of the extracted KG. It inherits the 28 relations from the seed KG (mentioned in Table~\ref{tab:qwen32b_alpha}) but contains approximately four times as many triples; the comparison chart (Fig.~\ref{fig:relation_distribution_in_kgs}) is presented in Appendix~\ref{app:extracted_kgs}. The extracted triples include vocabulary that extends beyond the seed KG. As expected, novel \textit{heads} originate from the training dataset, which has a richer vocabulary than the seed KG. \ours also generates novel \textit{tails}, predicting tokens from the dataset vocabulary that are absent in the seed KG (see an example given in Table~\ref{tab:novel-triple} in Appendix C).  

\begin{table}[t]
\centering
\caption{\ours-extracted KG. Number of triples over extraction stages; similarity threshold $\beta=0.67$. We show the number of triples after the helper LLM combines the tail tokens, next after discarding tails hallucinated by the LLM, then after similarity filtering, and finally after dropping triple repetitions.}
\label{tab:extracted-kgs-stats}
\begin{tabular}{lcccc}
\toprule
\rowcolor{gray!30}
$\alpha$ value
 & \makecell{Formed tails\\(non-unique)}
 & \makecell{Formed tails excluding \\ LLM hallucinated\\(non-unique)}
 & \makecell{After $\beta$-filtering\\(non-unique)}
 & \makecell{Final: repetitions\\ dropped (unique)} \\
\midrule
$\alpha = 0.55$ & 1,760,088 & 1,536,581 & 139,565 & \textbf{109,293}\\
\bottomrule
\end{tabular}
\end{table}

\subsection{Triple-level Evaluation}
\label{subsec:factsocre_validity}

Next, we present results for triple-level evaluations.

\subsubsection{FActScore* Results}
\label{subsubsec:factscore_results}

We use two prompts to evaluate a triple based on:
\begin{enumerate}
    \item context only,
    \item context and model's internal knowledge of general truth.
\end{enumerate}
The first prompt includes the text in black only, the second prompt includes the text in black and teal:

\begin{tcolorbox}[enhanced,colback=gray!5,colframe=gray!40,
                  left=6pt,right=6pt,top=6pt,bottom=6pt,boxsep=0pt]
\noindent\textbf{Prompt.} \small You will evaluate the quality of triples for a medical knowledge graph on diabetes and its comorbidities. For each triple, you are given:
        \begin{itemize}
            \item A sequence providing context
            \item A head entity, a relation, and a tail entity
        \end{itemize}
        Your task: Accept the triple (``[yes]'') or reject it (``[no]'') based on:
        \begin{itemize}
            \item \textbf{Logical alignment}: the tail must logically align with the head and relation; relation must match entity types.
            \item \textbf{Context support}: the sequence should support the triple. \textcolor{teal}{Allow statements that are factual and general truth, even if not perfectly aligned with context, but still avoid contradictions. If the triple has no reliable support, reject the triple.}
            \item \textcolor{teal}{\textbf{Knowledge value}: the triple must add new, medically meaningful information to the graph.}
        \end{itemize}
        Output \textbf{only} ``[yes]'' or ``[no]'' as your final judgment. Wrap your reasoning in \texttt{<think>...</think>}.
\end{tcolorbox}

Table~\ref{tab:factscore} reports the FActScore* of \ours KG versus the LLM KG using Qwen3-32B as the validator. We conduct two variants of the evaluation, differing only in an extra accept condition in the prompt:  
1) based on \emph{context only}, closely following the original FActScore, and  
2) based on \emph{context + model’s internal knowledge}, which additionally accepts triples that express general truths even if not explicitly stated in the context.  
Variant (2) acknowledges cross-dataset concept linking, whereas (1) restricts evidence to the local context. Because the same triple can originate from multiple sequences, some triples are checked more than once against different texts. As a result, the reported number of triples exceeds the count in the final deduplicated KGs.
We also score the seed KG with respect to the sequences into which its triples are injected. In this setting, FActScore primarily reflects the injection relevance.

\begin{table}[t]
  \centering
  \caption{FActScore* KG evaluation (in percentage points)}
  \label{tab:factscore}
  \small
  \setlength{\tabcolsep}{6pt}
  \rowcolors{1}{gray!10}{white}
  \begin{tabular}{l c c c}
  %p{2cm} p{2cm} p{2.5cm}}
    \toprule
    \rowcolor{gray!30}
    \textbf{KG type} &
    \textbf{\#triples} &
    \textbf{Context only} &
    \textbf{Context and General truth} \\
    \midrule
    LLM (baseline) & 515,460 & 40.2 & 48.1 \\
    \ours & 139,565 & \textbf{69.8} & \textbf{72.2} \\   
    \bottomrule
  \end{tabular}
\end{table}

\paragraph{Why are LLM KG FActScores so low?}
Surprisingly, the LLM KG scores poorly even when validated by the same model (Qwen3-32B), which flags many of its own errors. This underscores weak prompt steerability: Knowledge may exist in parameters, yet prompt-based generation fails to elicit correct, ontology-respecting triples.
Our analysis points to three recurring failure modes:
\begin{enumerate}
  \item \underline{Relation misinterpretation:} The model maps relations by lexical similarity rather than ontological meaning, drawing on its internal knowledge.
  \item \underline{Systematic malformed repetition:} The same ill-formed triples reappear across different text chunks.
  \item \underline{Overlinking a head–tail pair:} Multiple, largely invalid relations are assigned to the same entity pair.
\end{enumerate}

\noindent\textbf{Illustrative errors}
\begin{itemize}
  \item \emph{Misinterpreted relation:}
  $\langle\text{diabetes}, \texttt{has\_finding\_site}, \text{urban area}\rangle$.
  Here, ``site'' is treated as a location (anatomical structure is expected).
  Likewise, $\langle\text{telehealth intervention}, \texttt{has\_part}, \text{diabetes}\rangle$ misuses \texttt{has\_part} to describe care; a correct variant:
  $\langle\text{diabetes}, \texttt{focus\_of}, \text{telehealth intervention}\rangle$.
  \item \emph{Overlinking (redundant relations):}
  For $\langle\text{obesity}, \text{diabetes}\rangle$, the LLM proposed
  \texttt{is\_modification\_of}, \texttt{part\_of}, \texttt{plays\_role}, \texttt{cause\_of},
  \texttt{causative\_agent\_of}, \texttt{has\_component}, \texttt{has\_pathological\_process}.
  However, only \texttt{associated\_with} is valid.
  \item \emph{Spurious co-occurrence link:}
  $\langle\text{diabetes}, \texttt{has\_method}, \text{mammography}\rangle$.
  The entities merely co-occurred in an abstract; mammography is not used for diabetes, yet the LLM infers a spurious relation. 
\end{itemize}

\emph{``Spurious co-occurrence link''} illustrates a disadvantage of local KG extraction methods vs.~global methods that capture how concepts are distributed and connected across the full dataset. As a result, spurious links between concepts that rarely co-occur remain weak and are less likely to be predicted. 

\noindent\textbf{Implication:}
Broad, pre-trained LLMs tend to project general associations into domain-specific relations, yielding invalid triples. High-stakes domains benefit from compact, domain-tailored models and pipelines that enforce ontology and provenance.

\subsubsection{ValidityScore}
\label{subsubsec:validity_check}

\paragraph{Validity check on the whole KG with Qwen3-32B:} To get a sense of the triple semantic quality of \ours- and LLM-extracted KGs, we conduct a validity check: Test the validity of each triple in the graph with Qwen3-32B as the judge LLM.


\begin{table}[t]
  \centering
  \caption{Validity check with Qwen3-32B (in percentage points)}
  \label{tab:validitycheck}
  \small
  \begingroup
  \setlength{\tabcolsep}{6pt}
  \rowcolors{1}{gray!10}{white}
  \begin{tabular}{l c c c c}
  %p{2cm} p{2cm} p{2cm} p{2cm}}
    \toprule
    \rowcolor{gray!30}
    \textbf{KG type} &
    \textbf{\# triples} &
    \textbf{yes (ValidityScore)} &
    \textbf{maybe} &
    \textbf{no}
    \\
    \midrule
    LLM (baseline) & 515,460 & 43.0 & 24.1 & 31.4\\
    UMLS Seed KG & 28,533 & 53.4 & 10.0 & 34.7 \\
    \ours  & 139,565 & \textbf{68.8} & 18.3 & 10.8 \\
    \bottomrule
  \end{tabular}
  \endgroup
\end{table}

Table~\ref{tab:validitycheck} summarizes the validity checks across KGs. 
\ours attains a markedly higher ``yes'' rate, i.e., ValidityScore, than the LLM baseline (68.8\% vs.\ 43.0\%) with far fewer ``no'' judgments (10.8\% vs.\ 31.4\%), indicating substantially cleaner relation usage and predicate hygiene. Overall, the LLM KG is less conservative (more details ahead) and less valid. The typical malformed triples are shown in Tables~\ref{tab:glm-malformed_triple} (\ours) and~\ref{tab:llm-malformed_triple} (LLM).

\begin{figure}[t!]
\centering
\includegraphics[width=\linewidth]{charts/verdict_bars.pdf}
\caption{Validity check with GPT-5 Thinking, 100 random triples per keyword. The keywords are lined on the $x$-axis. Within each category, the left bars represent \ours triples and right bars LLM triples. The captions below the labels show yes/maybe/no split counts for \underline{G}LM and \underline{L}LM.}
\label{fig:verdict_bars}
\end{figure}

\begin{table}[h]
  \centering
  \caption{\ours-extracted KG triples: Malformed triple examples}
  \label{tab:glm-malformed_triple}
  \small
  \setlength{\tabcolsep}{4pt} % slightly tighter padding
  \rowcolors{1}{gray!10}{white}
  \begin{tabularx}{\linewidth}{p{0.67\linewidth} X}
    \toprule
    \rowcolor{gray!30}
    \textbf{Triples with an incomplete tail} & \textbf{Comment} \\
    \midrule
    CKD risk prediction model, associated\_with, validated & adjectival tail\\ 
    gestational diabetes mellitus, associated\_with, twin & should be twin pregnancy \\
    elastin, has\_modification, carbam & should be carbamylation \\
    \midrule
    \rowcolor{gray!30}
    \textbf{Triples with overstated causality} & \textbf{Comment} \\
    \midrule
    insulin resistance in CKD, cause\_of, metabolic syndrome & IR is a criterion \\
    insulin resistance in CKD, cause\_of, vascular disorder & vague tail \\
    diabetes mellitus, cause\_of, tuberculosis & cause is mycobacterium; DM is risk \\
    \midrule
    \rowcolor{gray!30}
    \textbf{Triples with vague tails} & \textbf{Comment} \\
    blind patients with DM, associated\_with, diabetic retinopathy & head is a cohort\\
    atherosclerotic cardiovascular disease, associated\_with, lifestyle & vague tail \\
    \midrule
    \rowcolor{gray!30}
    \textbf{Triples with predicate misuse} & \textbf{Comment} \\
    atherosclerotic cardiovascular disease, has\_causative\_agent, apolipoprotein b & should be apolipoprotein B–containing lipoproteins\\
    type 2 diabetes mellitus, associated\_with, parasitic infection & weak/unspecific link\\
    glucocorticoid-induced hyperglycaemia, has\_finding\_site, tissue & wrong site (should be blood) \\
    \bottomrule
  \end{tabularx}
\end{table}

\begin{table}[h]
  \centering
  \caption{LLM-extracted KG triples: Malformed triple examples}
  \label{tab:llm-malformed_triple}
  \small
  \begingroup
  \setlength{\tabcolsep}{4pt} % slightly tighter padding
  \rowcolors{1}{gray!10}{white}
  \begin{tabularx}{\linewidth}{p{0.55\linewidth} X}
    \toprule
    \rowcolor{gray!30}
    \textbf{Triples with reversed relation} & \textbf{Comment} \\
    \midrule
    cryptogenic stroke, causative\_agent\_of, paradoxical embolism & reversed causality \\
    ischemic stroke, cause\_of, ps & PS — Protein S deficiency; reversed \\
    glucocorticoids, has\_part, steroidogenesis & steroidogenesis produces GCs \\
    \midrule
    \rowcolor{gray!30}
    \textbf{Triples with predicate misuse} & \textbf{Comment} \\
    \midrule
    ischemic stroke, cause\_of, telomere length & biologically wrong\\
    elastin,  finding\_site\_of, vascular smooth muscle cell & cells aren't ``found'' in elastin\\
    CKD awareness, has\_component, race/ethnicity & CKD = chronic kidney disease; ontologically wrong\\
    lockdown, associated\_with, bone mineral density & mismatches ontological types\\
    elastin, has\_pathological\_process, matrix metalloproteinase & metalloproteinase is an enzyme\\
    \midrule
    \rowcolor{gray!30}
    \textbf{Triples with ill-defined target} & \textbf{Comment} \\
    chronic kidney disease (CKD), cause\_of, renal retinopathy & tail isn’t a standard retinal disorder term\\
    \bottomrule
  \end{tabularx}
  \endgroup
\end{table}


\paragraph{Validity check on a KG subset with GPT-5:} We also perform a validity check on small KG subsets with GPT-5 (Thinking). GPT-5 is particularly strict in evaluating consistency of triples and whether the relation is used in the correct direction, aligning with our goal of obtaining dependable judgments. We retrieve 100 random triples across seven diabetes-related keywords: \textit{T2DM, hyperglycemia, dyslipidemia, adiponectin, metformin, SGLT2, and CVD}. Each triple is assigned a \textit{yes/maybe/no} verdict (Fig.~\ref{fig:verdict_bars}). The keywords are drawn from the 2000-word dataset-level summary generated with Gemini 2.5 Pro, representing some of the most frequent and clinically relevant terms related to diabetes and its comorbidities. 

Overall, the \ours KG consistently produces a higher proportion of valid (\textit{yes}) triples and fewer incorrect (\textit{no}) triples across all keywords, whereas the LLM KG shows more relation misuse and ontology violations, reflected in a greater share of \textit{maybe} and \textit{no} verdicts. This highlights the more conservative but domain-appropriate character of the \ours KG compared to the noisier LLM KG. According to verdicts \textit{(a very short reason why)}, the LLM often violates ontology, confusing methods with diseases, and misuses relations, with the two errors reinforcing each other. Next, we analyze the main errors of each KG separately.

\paragraph{\ours:} The main issues in \ours triples are vagueness and incomplete tails, though they remain domain-appropriate. Tail incompleteness arises when the helper LLM accepts an incomplete token as a valid tail during token combination. We observe this effect across all helper LLMs we tested, including Gemini Flash 2.0 and 2.5. Tail vagueness occurs when \ours does not rank the required tail tokens within its top-20 predictions; the helper LLM then stitches together a completion that is contextually acceptable but semantically weak. This also explains most of the cases of overstated causality and predicate misuse: missing key tokens required for high-quality tail completion, the LLM still attempts a plausible but semantically weak completion. A practical mitigation is to exclude low-information 128-token sequences in the prediction stage (e.g., segments dominated by measurements, dosages, or numbers). We currently do not filter these sequences, which may contaminate the extracted KG.

\paragraph{LLM:} In contrast, we observe that the LLM misinterprets UMLS biomedical relations and substitutes them with its broader internal knowledge, resulting in approximations that violate the ontology. Designing prompts to fully explain all relations is impractical. Our experiments show that even multiple examples fail to steer the model consistently: It defaults to its own internal semantics. We also observe systematic relation reversal, consistent with findings in~\cite{2024reversalcurse}. More broadly, extracting well-formed triples requires capturing semantic rather than syntactic representations, which is challenging for LLMs trained primarily on surface text.

\paragraph{Examples:}
To exemplify the difference between \ours and LLM KGs explicitly, for the same prompt, we provide 
Tables~\ref{tab:glm_triples-check-igf}, \ref{tab:llm_triples-check-igf}, \ref{tab:glm_triples-check-glucocorticoid}, \ref{tab:llm_triples-check-glucocorticoid} with GPT-5 verdicts of triples sampled from \ours-extracted KG and LLM-extracted KG in Appendix~\ref{app:extracted_kgs}. 

\subsection{Graph-level Evaluation}
\label{subsec:graph_rag_eval_result}

To evaluate the extracted KG, we apply GraphRAG to the filtered benchmarks and assess accuracy based on the model's success rate in answering the questions. We compare the accuracy by using different KGs as the primary source of information in response generation.

We evaluate \ours KG against the baseline LLM KG on the filtered endocrinology subset of ICD-Bench, reporting the average question-answering accuracy across three runs. The evaluation is stratified by difficulty level, a label from the original benchmark. Our filtered test set includes 20 trivial questions (corresponding to 1-hop QA), and a total of 49 questions across difficulty levels ranging from very easy to very hard, based on the difficulty definitions from ICD-Bench. The detailed results are presented in Table~\ref{tab:graphrag}.

We also test the KGs on public benchmarks, which are described in Appendix~\ref{app:public_benchmark}. The findings demonstrate that the \ours KG consistently outperforms the baseline. Across the whole filtered subset, our framework achieves an overall accuracy gain of 9.2\% on ICD-Bench, and 1.7\% to 3.7\% gain on other medical benchmarks. This highlights the advantages of the \ours KG for downstream medical question-answering tasks.

\begin{table}[t!]
  \centering
  \caption{GraphRAG KG evaluation}
  \label{tab:graphrag}
  \small
  \begingroup
  \setlength{\tabcolsep}{6pt}
  \rowcolors{1}{gray!10}{white}
  \begin{tabular}{l p{1.3cm} p{1.3cm} p{1.3cm} p{1.3cm} p{1.3cm} p{1.3cm} p{1.3cm}}
    \toprule
    \rowcolor{gray!30}
    \textbf{Difficulty} & \textbf{Trivial} & \textbf{Very Easy} & \textbf{Easy} & \textbf{Medium} & \textbf{Hard} & \textbf{Very Hard} & \textbf{Average} \\
    \textbf{\# Questions} & 20 & 26 & 8 & 6 & 2 & 7 & 69 \\
    \midrule
    LLM KG (baseline) & 56.7 & 68.0 & 33.3 & 38.9 & 0.0 & 9.5 & 50.2 \\   
    Seed KG & 56.7 & 71.8 & 37.5 & 55.6 & 0.0 & 4.8 & 53.1 \\
    \ours & 66.7 & 79.5 & 50.0 & 50.0 & 0.0 & 0.0 & \textbf{59.4} \\   
    \bottomrule
  \end{tabular}
  \endgroup
\end{table}

\subsection{Ablations}
\label{subsec:ablation}
To validate our design choices and understand the contribution of different components, we conduct a series of ablation studies. We analyze the sensitivity of \ours to its core hyperparameters, its robustness to the density of the seed KG, and the effect of similarity and fact-checking.

\subsubsection{Training \ours with Less-relevant Injections}
\label{subsubsec:grid_search}
Our framework relies on two key similarity thresholds: the injection threshold $\alpha$, which determines the relevance threshold of seed triples used for training, and the acceptance threshold $\beta$, which filters the final triples generated by the pipeline. To find the optimal configuration, we perform a grid search over a range of values for both parameters.

Hyperparameter $\alpha$ controls the trade-off between the quality and quantity of knowledge injected during training. A higher $\alpha$ imposes a stricter relevance filter, ensuring high-quality injections but limiting their volume and diversity. Hyperparameter $\beta$ serves as a grounding function for the final triples on the source sentence. The baseline LLM KG is filtered in each experiment to include only the relations available for the corresponding $\alpha$ value, ensuring an equitable evaluation.

The results are presented in Figure~\ref{fig:ablations}. We observe that performance peaks at $\alpha = 0.55$ and $\beta = 0.67$. Lower $\alpha$'s likely introduce noisy, contextually irrelevant triples that degrade performance. Conversely, higher $\alpha$'s appear overly restrictive, preventing the model from leveraging a sufficient breadth of knowledge. In addition, $\beta = 0.67$ gives good results, underscoring the importance of a final quality check on generated triples. A higher optimal $\beta$ strengthens the importance of cross-document understanding for triple generation, which is not possible in LLM-generated KGs. The optimal configuration achieves a 9.2\% improvement over the baseline LLM KG. 

\begin{table}[t]
  \centering
  \caption{GraphRAG accuracies for different $\alpha$ and $\beta$. Performance gain versus LLM KG is shown in parentheses.}
  \label{tab:alpha_beta}
  \small
  \begingroup
  \setlength{\tabcolsep}{6pt}
  \rowcolors{1}{gray!10}{white}
  \begin{tabular}{l c c c c c c}
  %p{1.7cm} p{1.7cm} p{1.7cm} p{1.7cm} p{1.7cm} p{1.7cm}}
    \toprule
    \rowcolor{gray!30}
    \textbf{$\alpha$ value} & \textbf{LLM KG} & \textbf{Seed KG} & \textbf{$\beta=0.62$} & \textbf{$\beta=0.65$} & \textbf{$\beta=0.67$} & \textbf{$\beta=0.69$} \\
    \midrule
    0.50 & 58.0 & 51.7 & 55.1 (-2.9) & 55.1 (-2.9)& 54.6 (-3.4)& - \\   
    0.53 & 55.1 & 51.2 & 60.4 (+5.3)& 55.1 (+0.0)& 58.9 (+3.8)& - \\
    0.55 & 50.2 & 53.1 & 57.5 (+7.3)& 59.2 (+9.0)& 59.4 (\textbf{+9.2})& 52.2 (+2.0)\\   
    0.57 & 51.2 & 53.1 & 56.0 (+4.8)& 54.6 (+3.4)& 52.2 (+1.0)& - \\   
    0.60 & 53.1 & 49.8 & 53.6 (+0.5)& 52.7 (-0.4)& 53.6 (+0.5)& - \\   
    \bottomrule
  \end{tabular}
  \endgroup
\end{table}

\begin{figure}[t!]
\centering
\includegraphics[width=0.9\linewidth]{pictures/ablation.pdf}
\caption{GraphRAG accuracies for different $\alpha$ and $\beta$. Bubble size corresponds to absolute accuracy and color indicates the accuracy gain relative to the LLM KG baseline (red denotes positive gain, blue denotes negative gain).}
\label{fig:ablations}
\end{figure}

\subsubsection{Training \ours with a Smaller Seed KG}
To evaluate the dependency of \ours on the density of provided knowledge, we conduct an experiment to measure its performance with a sparser seed KG. A robust system should be able to function effectively even when the initial knowledge base is incomplete.

Using the optimal hyperparameters identified previously ($\alpha=0.55$ and $\beta=0.67$), we simulate varying levels of knowledge sparsity by randomly removing 25\%, 50\%, and 75\% of the triples from the original seed KG before executing our pipeline.

The results, shown in Table~\ref{tab:seed_size}, indicate that while performance generally decreases as the seed KG becomes sparser, our framework remains effective. Even with 75\% of the seed knowledge removed, \ours still outperforms the baseline LLM KG by 3.86\%. This finding highlights the robustness of our approach, demonstrating that it can effectively leverage even a sparse set of seed triples to generate a high-quality KG.

\begin{table}[t!]
  \centering
  \caption{GraphRAG accuracies upon dropping part of the seed KG. Performance gain versus LLM KG is shown in the last column.}
  \label{tab:seed_size}
  \begingroup
  \small
  \setlength{\tabcolsep}{6pt}
  \rowcolors{1}{gray!10}{white}
  \begin{tabular}{l c c}
  %p{4cm} p{4cm}}
    \toprule
    \rowcolor{gray!30}
    \textbf{KG type} & \textbf{GraphRAG accuracy} & \textbf{Gap vs. LLM KG} \\
    \midrule
    \ours & 59.4 & \textbf{+9.2}\\   
    \ours (remove 25\% of seed KG) & 54.6 & +4.4\\
    \ours (remove 50\% of seed KG) & 56.5 & +6.3\\   
    \ours (remove 75\% of seed KG) & 54.1 & +3.9 \\  
    \bottomrule
  \end{tabular}
  \endgroup
\end{table}

\subsubsection{Ablating \ours Components}
\label{subsubsec:abalting_components}

Next, we ablate architectural components and the training objective:
\begin{description}
    \item[No-span MLM/MNM:] Replace the span masking objective with a simple one-token MLM/MNM masking objective.
    \item[No H-GAT:] Switch off graph attention. This implies training without relation embeddings; the model predictions are purely syntactic. 
    \item[No dropout:] Switch off dropout on relation embeddings.
\end{description}

Before discussing the ablation results, we outline the main observations for each one:
\begin{itemize}
    \item In the ``no H-GAT'' ablation, we observe a large number of irrelevant tokens in the top-$k$ predicted tokens, with commas and articles being primarily predicted in the top 3. This is supported by findings by~\citet{Silva_entailment}:  They attempted to complete triples with top-$k$ tokens extracted from the BERT model in a purely syntactic manner, and had to rely on a stop word list to make the results usable.
    \item Disabling dropout leads to overfitting on the seed KG vocabulary; hence, less diverse tails.
    \item Training with a no-span MLM/MNM objective produces simpler tail completions (1--2 tokens long), because each individual candidate is not well aligned with the others. 
\end{itemize}


\begin{table}[t]
  \centering
  \caption{GraphRAG accuracies when ablating \ours features}
  \label{tab:graphrag-components}
  \small
  \begingroup
  \setlength{\tabcolsep}{6pt}
  \rowcolors{1}{gray!10}{white}
  \begin{tabular}{l p{0.9cm} p{0.8cm} p{0.8cm} p{1.2cm} p{0.8cm} p{0.8cm} p{1.1cm}}
    \toprule
    \rowcolor{gray!30}
    \textbf{KG type} & \textbf{Trivial} & \textbf{Very Easy} & \textbf{Easy} & \textbf{Medium} & \textbf{Hard} & \textbf{Very Hard} & \textbf{Average} \\
    \midrule
    \ours & 66.7 & 79.5 & 50.0 & 50.0 & 0.0 & 0.0 & \textbf{59.4} \\
    \midrule
    \ours (no-span MLM/MNM) & 66.7 & 76.9 & 29.2 & 61.1 & 0.0 & 9.5 & 58.0 \\   
    \ours (no H-GAT)  & 65.0 & 68.0 & 33.3 & 55.6 & 0.0 & 0.0 & 53.1\\
    \ours (no dropout) & 56.7 & 65.4 & 45.8 & 50.0 & 0.0 & 14.3 & 52.2 \\   
    \bottomrule
  \end{tabular}
  \endgroup
\end{table}


\begin{table}[t!]
  \centering
  \caption{FActScore* KG evaluation (in percentage points) when ablating \ours features}
  \label{tab:factscore_arch_ablation}
  \small
  \begingroup
  \setlength{\tabcolsep}{6pt}
  \rowcolors{1}{gray!10}{white}
  \begin{tabular}{l c c c}
  %p{2cm} p{2cm} p{2.5cm}}
    \toprule
    \rowcolor{gray!30}
    \textbf{KG type} &
    \textbf{\#triples} &
    \textbf{Context only} &
    \textbf{Context and General truth} \\
    \midrule
    \ours & 139,565 & \textbf{69.8} & \textbf{72.2} \\
    \midrule
    \ours (no-span MLM/MNM) & 188,211 & 69.0 & \textbf{72.2} \\
    \ours (no H-GAT) & 167,443 & 68.3 & 70.0 \\
    \ours (no dropout) & 149,952 & 68.9 & 70.9 \\
    % \ours ($\alpha$=0.57) & 129,985 & 69.3 & 71.0 \\
    % LLM ($\alpha$=0.45)& 614,031 & 38.8 & 46.1 \\   
    \bottomrule
  \end{tabular}
  \endgroup
\end{table}

Table~\ref{tab:graphrag-components} demonstrates that the full \ours KG configuration achieves the highest performance. While the full model achieves the best results, the variant without span-masking performs only slightly worse. In contrast, the removal of either dropout or H-GAT leads to a substantial decrease in accuracy, underscoring their importance for robust performance.

Table~\ref{tab:factscore_arch_ablation} demonstrates that the full model FActScore* is the highest: removing dropout or H-GAT lowers acceptance and increases rejections. \ours without span-masking reaches the same FActScore* in ``Context and General truth'' case.


\begin{table}[t!]
  \centering
  \caption{Validity check with Qwen3-32B (in percentage points) when ablating \ours features}
  \label{tab:validity_arch_ablation}
  \small
  \begingroup
  \setlength{\tabcolsep}{6pt}
  \rowcolors{1}{gray!10}{white}
  \begin{tabular}{l c c c c}
  %p{2cm} p{2cm} p{2cm} p{2cm}}
    \toprule
    \rowcolor{gray!30}
    \textbf{KG type} &
    \textbf{\# triples} &
    \textbf{yes} &
    \textbf{maybe} &
    \textbf{no}
    \\
    \midrule
    \ours  & 139,565 & 68.8 & 18.3 & 10.8 \\
    \midrule
    \ours (no-span MLM/MNM) & 188,211 & \textbf{69.4} & 18.1 & 10.2 \\
    \ours (no dropout) & 149,952 & 66.5 & 19.2 & 12.0 \\
    \ours (no H-GAT) & 167,443 & 68.2 & 17.2 & 12.2 \\
    \bottomrule
  \end{tabular}
  \endgroup
\end{table}


\ours without span-masking achieves the best acceptance (69.4\% yes, 10.2\% no) (Table~\ref{tab:validity_arch_ablation}), but, according to GraphRAG, the KG remains less informative overall. In effect, the KG obtained from this variant tends to be populated with trivially correct tiples, i.e., $\langle$diabetes, \texttt{is\_a}, disease$\rangle$. This leads to a higher rate of successful tail completion: 188k against 140k with span masking. Such short, obvious facts pass a validity check, manifesting in a higher ValidityScore. However, this simplicity comes at a cost: No-span-masked completions lack the nuance and granularity provided by span masking, resulting in poorer coverage and a loss of fine-grained domain details. This trade-off stresses the importance of evaluating KGs both at the graph and triple levels. We advise employing token-level MLM/MNM when the simplicity of triples is not a limitation.

\section{Discussion, Limitations, and Future work}
\label{sec:discussion}

\paragraph{Discussion:}
Our results show that \ours yields higher factuality and validity of triples than LLM-based KG extraction while preserving ontology fidelity. 
We observe a consistent difference between \ours and LLM KGs in relation usage and predicate hygiene. \ours generally employs relations correctly, aligns tails with heads, and preserves biomedical categories (diseases, syndromes, complications). The LLM KG often misuses or reverses the direction of relations, mixes categories in ontology-violating ways (e.g., socio-economic with biomedical), and produces inverse/ill-typed statements. The \ours KG has far fewer ontology violations and hews closer to UMLS. The \ours KG vocabulary is more conservative: The conservativenss can be explained based on the limited scope of the seed KG’s vocabulary and its tendency to restate head tokens (mimicking UMLS’s tautological triples). 

We attribute the difference in factuality and validity of triples to the usage of semantic relation embeddings, which move predictions toward ontology-aligned ones. Triple-level error analysis shows \ours may sometimes extract incomplete and vague but often domain-appropriate tails with factual relations, whereas LLMs usually produce diverse tails but frequently misuse or reverse the direction of relations. Our graph-level evaluation may conflate the KG signal with backbone model knowledge under GraphRAG; future work will include graph-level metrics that isolate the contribution of KGs as well as relation-aware retrieval. 

\paragraph{Limitations:}
The main limitation of \ours is its reliance on the seed KG. First, running the framework requires a high-quality seed with 100–1000 examples per relation. Second, once training is complete, the relation set is fixed. Hence, adding new relations requires retraining. A further limitation is its dependence on a helper LLM for tail combination, which introduces occasional incompleteness in the extracted triple tails. As a neural model, \ours also tends to prioritize frequent entities, potentially overlooking rare but meaningful ones. In addition, we did not evaluate the model’s ability to predict or handle numerical tokens, which remains an open direction for future work. Finally, its robustness to unseen entities remains untested; extracting entirely novel concepts from new texts would likely require fine-tuning, limiting adaptability in fast-changing domains.

\paragraph{Future work:}
We plan to improve the KG quality by unifying entity spellings, replacing the fixed top-$k$ limit with high-probability token selection. We aim to extend \ours to direct multi-token span prediction in the semantic space, reducing reliance on the helper LLM for tail token combining. Further, we plan to conduct more rigorous graph-level evaluations, since GraphRAG alone often blends KG information with model knowledge, and does not guarantee retrieval of the most relevant subgraph, as its retrieval is entity-guided and relations play only a secondary role. It may also be valuable to investigate how the \ours evaluation loss correlates with the quality of the extracted KGs. We also plan to extract KGs in other domains and employ them in various downstream applications.

\section{Conclusion}
\label{sec:conclusion}

We proposed a new framework for automatically extracting domain-specific KGs from unstructured, sentence-level text. Central to this work is our idea of encoding both semantic and syntactic information into textual chain graphs, a new representation we introduced. To operate in this space, we presented \ours, a transformer-based model that unifies an encoder-only architecture with graph attention. Together, these contributions enable the distillation of explicit semantic structures from trained neural networks, bridging neural and symbolic representations and advancing interpretable, reliable KG construction.

We also outlined KG-powered applications, underscoring that future progress hinges on reliable, factual KGs and other high-level explicit abstractions that embody collective knowledge while remaining compatible with AI inference and downstream tasks. We argue that neural-KG integration is a key step toward domain-specific superintelligence. Looking ahead, we foresee the research community embracing the neurosymbolic paradigm, where explicit, auditable, and evolving KGs complement neural inference that is approximate, efficient, and capable of handling ambiguity.

\vspace*{1mm}
\noindent
\textbf{Acknowledgment:} This work was supported by NSF under Grant No. CNS-2216746.

\medskip
\bibliographystyle{tmlr}
\bibliography{paper}

\begin{appendices}

\section{Training Data}
\label{app:training_data}
% restart table numbering and prefix with “A”
\setcounter{table}{0}
\renewcommand{\thetable}{A\arabic{table}}

\setcounter{figure}{0}
\renewcommand{\thefigure}{A\arabic{figure}}

We exclude some relations from the UMLS KG that add little semantic value, i.e.: (i) custom-defined mappings of outdated-to-new UMLS relations for backward compatibility, which cannot be inferred from external data; (ii) relations used only for cross-vocabulary mappings; (iii) relations where tails are largely redundant with respect to heads; and (iv) relations with very few possible tails. For example, relation has\_associated\_finding is a redundant relation:
\begin{itemize}[noitemsep,leftmargin=*]
  \item family history of diabetes, \texttt{has\_associated\_finding}, diabetes mellitus
  \item family history of glaucoma, \texttt{has\_associated\_finding}, glaucoma
  \item parental history of diabetes, \texttt{has\_associated\_finding}, diabetes mellitus
  \item family history of hypertension, \texttt{has\_associated\_finding}, hypertensive disorder
  \item family history of cvd, \texttt{has\_associated\_finding}, congenital heart disease
\end{itemize}
In these cases, the tail subject is the same as the head subject.
\texttt{has\_laterality} is an example of a relation with very few possible tails, with almost all tails being ``side''.

\begin{tcolorbox}[enhanced, breakable=false, colback=gray!5, boxrule=0pt]
\begin{lstlisting}[caption={PubMed search query},label={lst:pubmed-query}]
(
    (
      diabetes[Abstract] OR diabetes[Body - All Words] OR diabetes[Body - Key Terms]
    )
  OR (
    ''diabetes mellitus''[MeSH Terms] OR ''diabetes insipidus''[MeSH Terms]
  )
)
OR diabetes[Title]
NOT (
  ''sars-cov-2''[MeSH Terms] OR ''covid-19''[MeSH Terms]
)
AND (
  medline[sb] AND ''2019/04/01''[PubDate] : ''2025/04/01''[PubDate]
)
\end{lstlisting}
\end{tcolorbox}


\begin{table}[t]
  \centering
  \caption{Excluded UMLS relations}
  \label{tab:relations-split}
  \rowcolors{1}{gray!10}{white}
  \begin{tabular}{lll}
    \toprule
    acted\_on\_by\_process           & has\_scale\_type                    & regulated\_by \\
    active\_ingredient\_of           & has\_specimen                       & replaced\_by \\
    associated\_procedure\_of        & has\_subject\_relationship\_context & replaces \\
    basis\_of\_strength\_substance\_of & has\_temporal\_context           & was\_a \\
    component\_of                    & inverse\_was\_a                     & has\_intent \\
    consider\_from                   & mapped\_from                        & referred\_to\_by \\
    direct\_device\_of               & mapped\_to                          & refers\_to \\
    direct\_substance\_of            & moved\_to                           & characterizes \\
    has\_associated\_finding         & negatively\_regulated\_by           & substance\_used\_by \\
    has\_finding\_context            & positively\_regulated\_by           & specimen\_source\_topography\_of \\
    has\_interpretation              & possibly\_replaces                  & specimen\_substance\_of \\
    has\_laterality                  & precise\_active\_ingredient\_of     & has\_active\_ingredient \\
    has\_realization                 & realization\_of                     & has\_property \\
    \bottomrule
  \end{tabular}
\end{table}


\clearpage
\section{KG Injection Algorithm}
\label{app:injection_algorithm}
\setcounter{table}{0}
\renewcommand{\thetable}{B\arabic{table}}
\setcounter{figure}{0}
\renewcommand{\thefigure}{B\arabic{figure}}

\underline{Input}: Sequences with heads: Each head $h$ may have multiple triples $T = \langle h,r,t\rangle = T(seq)$  in sequence $seq$ (we cap this number at 40 triples to ensure enough diversity and leaving room for further dropping); triple embedding similarity score with respect to the sequence: $score(T)$; similarity matching threshold $\alpha$ (a hyperparameter).

\noindent
\underline{Output}: Each head has one injection $T(seq)$ or does not have any, in all sequences.

\paragraph{Preprocessing} 
\begin{enumerate}
    \item Drop all triples with a score less than threshold $\alpha$.
    \item Make all triples unique: If a triple matches multiple sequences, retain the triple $\tilde{T}$ with the highest score, i.e., in the sequence to which the triple is most relevant: 
    $$\tilde{T} = \argmax_{seq} score(T(seq))$$
\end{enumerate}
The second preprocessing step prevents overfitting in the semantic space on common triples.

\paragraph{Triple selection for each head:}
To balance contextual relevance with relation diversity: 1st priority: maximize injection score, 2nd priority: maintain relation diversity. Relation diversity is measured by the number of unique triples that contain the relation.

\underline{Maximize diversity}
\begin{enumerate}
    \item Split relations into relation buckets based on the number of unique triples at step $k$ and assume that within each bucket all relations are equally diverse (e.g., $k=20$ implies that relations with \#triples 100-120, 120-140.., are treated as equally diverse).
    \item Within each relation bucket, sort all triples by score regardless of relation.
    \item Start with the lowest-numbered bucket (rarest relations). Within it, start with the triple with the highest score and retain only it for its head, removing all other matched triples, which may have a higher score but may be in a higher relation bucket. As a result, one of the rarest possible relations in the dataset would survive for this head, increasing relation diversity overall.
\end{enumerate}

\underline{Maximize score then diversity}
\begin{enumerate}
    \item Order triples by score.
    \item Split into score buckets: Assume that within each score bucket, triples are equally good.
    \item Then, within each score bucket, apply \underline{Maximize diversity}.
\end{enumerate}

Altogether, we group triples by how “low” the score is (higher scores are assigned to lower-bucket IDs). Then, within each score bucket, we favor relation types that are less frequent. Finally, we choose the highest-scoring triple for each head.

The algorithm is implemented using the Pandas framework and presented in Algorithm~\ref{alg:max_score_diversity}.
\begin{algorithm}[t]
\caption{Maximize Score then Diversity}
\label{alg:max_score_diversity}
\Input{$df$ (a Pandas DataFrame where each row comprises a triple $(h,r,t)$, unique $matched\_head\_id$, and an associated $score$), $score\_bucket\_size$, $relation\_bucket\_size$}
\Output{Filtered DataFrame $result$}
\BlankLine
% 1
$max\_s \leftarrow \max(df.score)$\;
% 2
\ForEach{row \In $df$}{
  $row.score\_bucket \leftarrow \lfloor (max\_s - row.score) / score\_bucket\_size \rfloor$\;
}
% 3
$rel\_counts \leftarrow$ count occurrences of each relation\_type $r$ in $df$\;
% 4
\ForEach{row \In $df$}{
  $rel\_count \leftarrow rel\_counts[row.r]$\;
  $row.rel\_bucket \leftarrow \lfloor rel\_count / relation\_bucket\_size \rfloor$\;
}
% 5
Sort $df$ by ascending $(score\_bucket,\,rel\_bucket)$ and by descending $score$\;
% 6
$result \leftarrow [\,]$\;
$seen\_heads \leftarrow \emptyset$\;
\ForEach{row \In sorted $df$}{
  \If{$row.matched\_head\_id \notin seen\_heads$}{
    append row to $result$\;
    $seen\_heads \leftarrow seen\_heads \cup \{row.matched\_head\_id\}$\;
  }
}
\tcp{Equivalent to \texttt{df.drop\_duplicates(subset=''matched\_head\_id'', keep=''first'')}}
% 7
\Return{$result$}\;
\end{algorithm}

In our experiments, we use $score\_bucket\_size=0.01$, $relation\_bucket\_size=100$. The bucket size and relation bucket size are chosen under the assumption that, within each bucket, score and relation diversity remain approximately balanced.



\begin{table}[t!]
  \centering
  \caption{Seed KG: Relation statistics for $\alpha = 0.55$ in the training split (Qwen3-32B)}
  \label{tab:qwen32b_relstats}
  
  \setlength{\tabcolsep}{4pt}
  \begin{subtable}[t]{0.48\textwidth}
    \centering
    \small
    % \caption{Relation statistics (1)}
    \label{tab:qwen32b_train_relstats_1}
    \rowcolors{1}{gray!10}{white}
    \begin{tabular}{c l r}
      \toprule
      \rowcolor{gray!30}
      \# & Relation & \# injections \\
      \midrule
       1 & isa                          & 8627 \\
       2 & inverse\_isa                 & 5512 \\
       3 & cause\_of                    & 1440 \\
       4 & interprets                   & 1268 \\
       5 & associated\_finding\_of      & 1145 \\
       6 & has\_disposition             & 1084 \\
       7 & focus\_of                    & 1038 \\
       8 & is\_interpreted\_by          & 962  \\
       9 & has\_associated\_morphology  & 863  \\
      10 & causative\_agent\_of         & 809  \\
      11 & finding\_site\_of            & 741  \\
      12 & associated\_morphology\_of   & 598  \\
      13 & has\_method                  & 515  \\
      14 & has\_finding\_site           & 477  \\
      \bottomrule
    \end{tabular}
  \end{subtable}\hfill
  \begin{subtable}[t]{0.48\textwidth}
    \centering
    \small
    % \caption{Relation statistics (2)}
    \label{tab:qwen32b_train_relstats_2}
    \rowcolors{1}{gray!10}{white}
    \begin{tabular}{c l r}
      \toprule
      \rowcolor{gray!30}
      \# & Relation & \# injections \\
      \midrule
      15 & possibly\_equivalent\_to     & 446 \\
      16 & has\_component               & 433 \\
      17 & due\_to                      & 365 \\
      18 & has\_part                    & 350 \\
      19 & has\_modification            & 310 \\
      20 & associated\_with             & 254 \\
      21 & part\_of                     & 211 \\
      22 & plays\_role                  & 194 \\
      23 & occurs\_before               & 187 \\
      24 & has\_clinical\_course        & 144 \\
      25 & occurs\_in                   & 138 \\
      26 & same\_as                     & 134 \\
      27 & has\_causative\_agent        & 127 \\
      28 & has\_focus                   & 118 \\
      \bottomrule
    \end{tabular}
  \end{subtable}
\end{table}


%=============================================


\clearpage
\section{Extracted KGs}
\label{app:extracted_kgs}
\setcounter{table}{0}
\renewcommand{\thetable}{C\arabic{table}}
\setcounter{figure}{0}
\renewcommand{\thefigure}{C\arabic{figure}}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{charts/relation_distribution_in_kgs.pdf}
\caption{Relation distribution in the \ours-extracted KG vs.~seed KG. The shapes differ: While ``isa'' prevails in the seed KG, the \ours KG is heavily skewed towards ``associated\_with.'' This reflects the helper LLM's inclination to select ``associated\_with'' during relation matching as the most appropriate, given a sequence.}
\label{fig:relation_distribution_in_kgs}
\end{figure}

\subsection{Relation Distribution: \ours-extracted KG vs.~Seed KG} 
Fig.~\ref{fig:relation_distribution_in_kgs} shows the relation distribution on a logarithmic scale. While ``isa'' is the most represented relation in the training data, in the to-be-extracted KG, the helper LLM tends to select ``associated\_with'' most frequently during relation matching.



\begin{table}[t]
  \centering
  \caption{Example of \ours-extracted triple with novel tail vocabulary from a 128-token sequence.
  The seed KG does not include the token ``nlrp3'' --- it was learned and extracted from the text.
  ``Pathway'' is a token that \ours learned from the training corpus.
  Here, ``pathway'' is implicitly supported: The phrase ``specific inhibitors of NLRP3 inflammasome activation'' links the activation to NLRP3-mediated signaling (i.e., the NLRP3 pathway).}
  \label{tab:novel-triple}
  \small
  \setlength{\tabcolsep}{6pt}
  \begin{tabular}{p{0.3\textwidth} p{0.2\textwidth} p{0.45\textwidth}}
    \toprule
    \rowcolor{antiquewhite!50}
    \multicolumn{3}{p{\textwidth\relax}}{\textbf{Sequence:} \dots\textcolor{burntorange}{inflammasome activation} and regulation is highlighted, including its putative roles in adipose tissue dysfunction and insulin resistance. Specific inhibitors of \textcolor{blue}{NLRP3} inflammasome activation which can potentially be used to treat metabolic disorders are also discussed. Identifying a quantitative biomarker of neuropsychiatric dysfunction in people with HIV (PWH) remains a significant challenge in the neuroHIV field. The strongest evidence to date implicates the role of monocytes in central nervous system (CNS) dysfunction in HIV, yet no study has examined monocyte subsets in blood as a correlate and/or predictor of neuropsychiatric function in virally suppressed PWH. METHODS In 2 independent cohorts of virologically suppressed women with HIV (vsWWH\dots}\\
    \midrule
    \rowcolor{gray!30}
    Head & Relation & Tail \\
    \midrule
    \textcolor{burntorange}{inflammasome activation} & associated\_with & \textcolor{blue}{nlrp3} pathway\\
    \bottomrule
  \end{tabular}
\end{table}


\subsection{Sanity Check} 
We ran a lightweight screening with GPT-5 Thinking on small, comparable samples from each KG. This screening should be viewed as complementary to benchmark-based verification, providing useful diagnostic signals but not replacing factual evaluation. \\

\paragraph{Setup:} For each KG, we retrieved all triples whose head contains the keywords “insulin-like growth factor~1 (IGF-1)” and “glucocorticoid receptor (GR).” These terms are highly relevant to diabetes, yield comparable sample sizes per KG (22--29), and remain small enough for human inspection.
\begin{tcolorbox}[enhanced,breakable,colback=gray!5,colframe=gray!40,
                  left=6pt,right=6pt,top=6pt,bottom=6pt,boxsep=0pt]
\noindent\textbf{Prompt.} Evaluate if these medical KG triples are valid (yes/no/maybe) and give a very short reason why: $\langle\text{list of triples}\rangle$.
\end{tcolorbox}

% For ``insulin-like growth factor-1, \ours KG scored 15/0/7 out of 22 (0.68/0/0.32), LLM KG scored 8/18/3 out of 29 (0.28/0.62/0.1); for ``glucocorticoid receptor``, \ours KG scored  15/6/7 out of 28 (0.54/0.21/ 0.25), LLM KG scored 4/8/12 out of 24 (0.17/0.33/0.5).

\begin{table}[t!]
\centering
\caption{GPT-5 Thinking screening (counts with proportions)}
\label{tab:appendix_screening}
\small
\rowcolors{1}{gray!10}{white}
\begin{tabular}{l l c c c c}
\toprule
\rowcolor{gray!30}
\textbf{Keyword} & \textbf{KG} & \textbf{Yes} & \textbf{Maybe} & \textbf{No} & \textbf{Total} \\
\midrule
IGF\textendash 1 & LLM    & 8  (0.28) & 3 (0.10)  & 18 (0.62) & 29 \\
IGF\textendash 1 & \ours  & 15 (0.68) & 0 (0.00)  & 7 (0.32)  & 22 \\
\midrule
GR               & LLM    & 4  (0.17) & 12 (0.50) & 8  (0.33) & 24 \\
GR               & \ours  & 15 (0.54) & 7 (0.25)  & 6 (0.21)  & 28 \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:appendix_screening} summarizes screening results. Tables~\ref{tab:glm_triples-check-igf},~\ref{tab:llm_triples-check-igf},~\ref{tab:glm_triples-check-glucocorticoid},~\ref{tab:llm_triples-check-glucocorticoid} provide GPT-5 Thinking responses for each triple in the \ours and LLM KG samples.


\begin{table}[t]
\centering
\caption{\ours KG. Sanity check with GPT-5 Thinking. We select all KG triples that contain ``insulin-like growth factor-1,'' 22 triples in total, \emph{yes/no/maybe: 15/0/7}.}
\label{tab:glm_triples-check-igf}
\rowcolors{1}{gray!10}{white}
\begin{tabularx}{\linewidth}{
  >{\hsize=1.3\hsize\raggedright\arraybackslash}X  % wider first column
  p{3em}                                           % narrow verdict column
  >{\hsize=0.7\hsize\raggedright\arraybackslash}X  % slimmer last column
}
\toprule
\rowcolor{gray!30}
\textbf{Triple} & \textbf{Verdict} & \textbf{GPT-5 explanation} \\
\midrule
insulin-like growth factor 1, isa, peptide hormone & \textcolor{Green}{\bf yes} & 70-aa peptide hormone. \\
insulin-like growth factor 1, isa, growth factor & \textcolor{Green}{\bf yes} & Canonical growth factor. \\
insulin-like growth factor 1, associated\_with, insulin resistance & \textcolor{Green}{\bf yes} & Correlates with IR. \\
insulin-like growth factor 1 receptor (IGF1R), plays\_role, target & \textcolor{YellowOrange}{\bf maybe} & ``Target'' is vague. \\
insulin-like growth factor 1 (IGF-1), associated\_with, diabetes mellitus & \textcolor{Green}{\bf yes} & Linked with DM. \\
insulin-like growth factor 1 (IGF-1), associated\_with, biomarker & \textcolor{YellowOrange}{\bf maybe} & Biomarker is a role. \\
insulin-like growth factor 1 (IGF-1), associated\_with, insulin & \textcolor{Green}{\bf yes} & Strong crosstalk. \\
insulin-like growth factor 1 (IGF-1), associated\_with, type 2 diabetes & \textcolor{Green}{\bf yes} & Epidemiologic link. \\
insulin-like growth factor 1 (IGF-1), associated\_with, insulin resistance & \textcolor{Green}{\bf yes} & Well supported. \\
insulin-like growth factor 1, plays\_role, growth factor & \textcolor{Green}{\bf yes} & Acts as growth factor. \\
insulin-like growth factor 1 (IGF-1), plays\_role, biomarkers & \textcolor{YellowOrange}{\bf maybe} & Plural/ontology mismatch. \\
insulin/insulin-like growth factor 1 (IGF-1) signaling pathway, has\_component, IGF-1 receptor & \textcolor{Green}{\bf yes} & IGF-1R is in pathway. \\
insulin-like growth factor 1 (IGF-1), associated\_with, insulin & \textcolor{Green}{\bf yes} & Duplicate; supported. \\
insulin-like growth factor 1, plays\_role, growth hormone action & \textcolor{Green}{\bf yes} & Mediates GH effects. \\
insulin-like growth factor 1 receptor (IGF1R), plays\_role, signaling receptor & \textcolor{Green}{\bf yes} & RTK signaling receptor. \\
insulin-like growth factor 1 receptor, plays\_role, signaling & \textcolor{Green}{\bf yes} & Mediates signaling. \\
insulin-like growth factor 1 (IGF1), associated\_with, androgen & \textcolor{YellowOrange}{\bf maybe} & Context-specific crosstalk. \\
insulin-like growth factor 1 (IGF1), associated\_with, transcription & \textcolor{YellowOrange}{\bf maybe} & Indirect gene-expression effects. \\
insulin-like growth factor 1 (IGF-1), associated\_with, hyperglycemia & \textcolor{YellowOrange}{\bf maybe} & Context dependent. \\
insulin-like growth factor 1 level, associated\_with, growth hormone treatment & \textcolor{Green}{\bf yes} & GH raises IGF-1. \\
insulin/insulin-like growth factor 1 (IGF-1) signaling pathway, has\_component, insulin receptor & \textcolor{Green}{\bf yes} & IR is in pathway. \\
insulin-like growth factor 1 (IGF1), plays\_role, downstream target & \textcolor{YellowOrange}{\bf maybe} & Phrasing vague (GH downstream). \\
\bottomrule
\end{tabularx}
\end{table}


% -------- TABLE 1/2 (start) --------
\begin{table}[t]
\centering
\caption{(Part 1 of 2). LLM KG (Qwen3-32B). Sanity check with GPT-5 Thinking. We select all KG triples that contain ``insulin-like growth factor-1,'' 29 triples in total, \emph{yes/no/maybe: 8/18/3}.}
\label{tab:llm_triples-check-igf}
\rowcolors{1}{gray!10}{white}
\begin{tabularx}{\linewidth}{
  >{\hsize=1.3\hsize\raggedright\arraybackslash}X  % wider first column
  p{3em}                                           % narrow verdict column
  >{\hsize=0.7\hsize\raggedright\arraybackslash}X  % slimmer last column
}
\rowcolor{gray!30}
\textbf{Triple} & \textbf{Verdict} & \textbf{GPT-5 explanation} \\
\midrule
insulin-like growth factor 1, associated\_with, diabetes & \textcolor{Green}{\bf yes} & Linked with DM. \\
insulin-like growth factor 1 (IGF-1), has\_direct\_substance, type 2 diabetes & \textcolor{red}{\bf no} & Disease $\neq$ substance. \\
insulin-like growth factor 1, associated\_with, insulin resistance & \textcolor{Green}{\bf yes} & Strong physiologic link. \\
insulin-like growth factor 1, associated\_with, metabolic syndrome & \textcolor{Green}{\bf yes} & Levels track MetS. \\
insulin-like growth factor 1 (IGF-1), associated\_with, chronic kidney disease & \textcolor{Green}{\bf yes} & GH/IGF axis altered. \\
insulin-like growth factor 1 (IGF-1), has\_direct\_substance, gestational diabetes mellitus (GDM) & \textcolor{red}{\bf no} & Disease $\neq$ substance. \\
insulin-like growth factor 1 (IGF1), has\_direct\_substance, angiogenesis & \textcolor{red}{\bf no} & Process, not substance. \\
insulin-like growth factor 1 (IGF-1), has\_direct\_substance, short-chain fatty acids (SCFAs) & \textcolor{red}{\bf no} & Unrelated metabolites. \\
insulin-like growth factor 1 (IGF-1), has\_component, insulin signaling pathway & \textcolor{red}{\bf no} & Pathway $\neq$ component of ligand. \\
insulin-like growth factor 1 (IGF-1), plays\_role, cardiovascular health & \textcolor{YellowOrange}{\bf maybe} & Broad/context-dependent. \\
insulin-like growth factor 1 (IGF-1), associated\_with, bone metabolism & \textcolor{Green}{\bf yes} & Anabolic for bone. \\
insulin-like growth factor 1, has\_part, growth hormone & \textcolor{red}{\bf no} & GH regulates; not part. \\
insulin-like growth factor 1 (IGF-1), cause\_of, prostate cancer (PCA) & \textcolor{red}{\bf no} & Association $\neq$ causation. \\
insulin-like growth factor 1 (IGF-1), has\_modification, left ventricular global longitudinal strain (LVGLS) & \textcolor{red}{\bf no} & Clinical metric, not modification. \\
insulin-like growth factor 1 receptor, cause\_of, epithelial-mesenchymal transition & \textcolor{YellowOrange}{\bf maybe} & Signaling can induce EMT. \\
insulin-like growth factor 1 (IGF-1), has\_modification, HAAT-MSCs & \textcolor{red}{\bf no} & Not a molecular modification. \\
insulin-like growth factor 1, has\_direct\_substance, testis & \textcolor{red}{\bf no} & Organ produces IGF-1. \\
insulin-like growth factor 1, plays\_role, endocrine-related cancers & \textcolor{YellowOrange}{\bf maybe} & Vague class-level claim. \\
\bottomrule
\end{tabularx}
\end{table}
% -------- TABLE 1/2 (end) --------

% -------- TABLE 2/2 (continued) --------
\begin{table}[t]
\ContinuedFloat % (ADDED) keep same figure/table number, mark as continued
\centering
\caption{(Part 2 of 2). LLM KG (Qwen3-32B). (continued)}
\rowcolors{1}{gray!10}{white}
\begin{tabularx}{\linewidth}{
  >{\hsize=1.3\hsize\raggedright\arraybackslash}X  % wider first column
  p{3em}                                           % narrow verdict column
  >{\hsize=0.7\hsize\raggedright\arraybackslash}X  % slimmer last column
}
\rowcolor{gray!30}
\textbf{Triple} & \textbf{Verdict} & \textbf{GPT-5 explanation} \\
\midrule
insulin-like growth factor 1, associated\_with, oocyte cohort quality & \textcolor{Green}{\bf yes} & Follicular IGF-1 correlates. \\
insulin-like growth factor 1 (IGF1), has\_component, phosphoinositide 3-kinase (PI3K) & \textcolor{red}{\bf no} & Downstream effector, not component. \\
insulin-like growth factor 1 receptor, isa, receptor & \textcolor{Green}{\bf yes} & Canonical RTK. \\
insulin-like growth factor 1 receptor, has\_direct\_substance, lactate dehydrogenase a & \textcolor{red}{\bf no} & Not a receptor substance. \\
insulin-like growth factor 1 (IGF1), has\_pathological\_process, core body temperature & \textcolor{red}{\bf no} & Not a pathology. \\
insulin-like growth factor 1 (IGF1), has\_component, IGF-binding protein 3 (IGFBP3) & \textcolor{red}{\bf no} & Binding partner, not component. \\
insulin-like growth factor 1 receptor, has\_method, JAKS2/STAT3 pathway & \textcolor{red}{\bf no} & Wrong relation/atypical pathway. \\
insulin-like growth factor 1 (IGF1), has\_component, ZYG11A & \textcolor{red}{\bf no} & Unrelated protein. \\
insulin-like growth factor 1 receptor (IGF-1R), has\_part, nositide 3-kinase (PI3K) & \textcolor{red}{\bf no} & PI3K is recruited, not part. \\
IGF-1 (insulin-like growth factor 1), has\_direct\_substance, PPAR$\gamma$ (peroxisome proliferator-activated receptor gamma) & \textcolor{red}{\bf no} & transcription factor $\neq$ substance of IGF-1. \\
insulin-like growth factor 1, plays\_role, cardiac development & \textcolor{Green}{\bf yes} & Supports cardiac growth. \\
\bottomrule
\end{tabularx}
\end{table}
% -------- TABLE 2/2 (end) --------




\begin{table}[t]
\centering
\caption{\ours KG. Sanity check with GPT-5 Thinking. We select all KG triples that contain ``glucocorticoid receptor,'' 28 triples in total, \emph{yes/no/maybe: 15/6/7}.}
\label{tab:glm_triples-check-glucocorticoid}
\rowcolors{1}{gray!10}{white}
% \begin{tabularx}{\linewidth}{>{\raggedright\arraybackslash}X c >{\raggedright\arraybackslash}X}
\rowcolors{1}{gray!10}{white}
\begin{tabularx}{\linewidth}{
  >{\hsize=1.3\hsize\raggedright\arraybackslash}X  % wider first column
  p{3em}                                           % narrow verdict column
  >{\hsize=0.7\hsize\raggedright\arraybackslash}X  % slimmer last column
}
\rowcolor{gray!30}
\textbf{Triple} & \textbf{Verdict} & \textbf{GPT-5 explanation} \\
\midrule
endothelial glucocorticoid receptor, plays\_role, therapeutic role & \textcolor{YellowOrange}{\bf maybe} & Role is vague/context-dependent. \\
endothelial glucocorticoid receptor, plays\_role, glucocorticoid signaling & \textcolor{Green}{\bf yes} & Mediates GC signaling in endothelium. \\
endothelial glucocorticoid receptor, plays\_role, protective role & \textcolor{YellowOrange}{\bf maybe} & Protective effects reported but not universal. \\
glucocorticoid receptor (GR), plays\_role, glucocorticoid signaling & \textcolor{Green}{\bf yes} & Canonical function. \\
glucocorticoid receptor, associated\_with, insulin signaling & \textcolor{Green}{\bf yes} & Well-known pathway crosstalk. \\
glucocorticoid receptor, has\_part, ligand-binding & \textcolor{YellowOrange}{\bf maybe} & Should be ``ligand-binding domain.'' \\
endothelial glucocorticoid receptor, has\_disposition, signaling & \textcolor{red}{\bf no} & Wrong relation; participates in signaling. \\
glucocorticoid receptor, plays\_role, glucocorticoid receptor signaling & \textcolor{Green}{\bf yes} & Tautologically true. \\
glucocorticoid receptor (GR), plays\_role, hypothalamic signaling & \textcolor{YellowOrange}{\bf maybe} & HPA feedback involvement but broad. \\
endothelial glucocorticoid receptor, has\_disposition, immunomodulator & \textcolor{red}{\bf no} & ``Immunomodulator'' is an agent role, not a disposition. \\
glucocorticoid receptor (GR), plays\_role, steroid signaling & \textcolor{Green}{\bf yes} & Nuclear steroid receptor pathway. \\
glucocorticoid receptor (GR), associated\_with, glucocorticoids & \textcolor{Green}{\bf yes} & Binds GC ligands. \\
glucocorticoid receptor agonists, plays\_role, therapeutic & \textcolor{Green}{\bf yes} & Used clinically. \\
glucocorticoid receptor (GR), plays\_role, steroid hormone receptor & \textcolor{red}{\bf no} & This is an is\_a identity, not a role. \\
glucocorticoid receptor gene (NR3C1), associated\_with, hormone receptor & \textcolor{YellowOrange}{\bf maybe} & Gene encodes one; relation imprecise. \\
glucocorticoid receptor, plays\_role, signaling & \textcolor{Green}{\bf yes} & Ligand-activated signaling/TF activity. \\
glucocorticoid receptor (GR), associated\_with, steroid & \textcolor{Green}{\bf yes} & Glucocorticoids are steroids. \\
selective glucocorticoid receptor agonists, plays\_role, therapeutic role & \textcolor{YellowOrange}{\bf maybe} & Class intended for therapy; phrasing vague. \\
endothelial glucocorticoid receptor, cause\_of, renal fibrosis & \textcolor{red}{\bf no} & GR generally anti-fibrotic; causation unsupported. \\
selective glucocorticoid receptor modulators, plays\_role, pharmacological modulator & \textcolor{red}{\bf no} & Category is is\_a, not a role. \\
glucocorticoid receptor, associated\_with, glucocorticoids & \textcolor{Green}{\bf yes} & Duplicate; correct. \\
glucocorticoid receptor (GR), plays\_role, glucocorticoid signaling & \textcolor{Green}{\bf yes} & Duplicate; correct. \\
glucocorticoid receptor $\alpha$, plays\_role, steroid receptor & \textcolor{red}{\bf no} & $\alpha$ isoform is\_a steroid receptor, not a role. \\
glucocorticoid receptor (GR), associated\_with, transcription & \textcolor{Green}{\bf yes} & Ligand-activated transcription factor. \\
glucocorticoid receptor, plays\_role, signaling & \textcolor{Green}{\bf yes} & Duplicate; correct. \\
glucocorticoid receptor (GR), associated\_with, hormone & \textcolor{Green}{\bf yes} & Responds to hormones (GCs). \\
glucocorticoid receptor (GR), associated\_with, signaling & \textcolor{Green}{\bf yes} & Broad but true. \\
selective glucocorticoid receptor modulators, plays\_role, pharmacological modulator & \textcolor{red}{\bf no} & Duplicate; is\_a, not a role. \\
\bottomrule
\end{tabularx}
\end{table}



\begin{table}[t]
\centering
\caption{LLM KG (Qwen3-32B). Sanity check with GPT-5 Thinking. We select all KG triples that contain ``glucocorticoid receptor,'' 24 triples in total, \emph{yes/no/maybe: 4/18/2}.}
\label{tab:llm_triples-check-glucocorticoid}
\rowcolors{1}{gray!10}{white}
\begin{tabularx}{\linewidth}{
  >{\hsize=1.2\hsize\raggedright\arraybackslash}X  % wider first column
  p{3em}                                           % narrow verdict column
  >{\hsize=0.8\hsize\raggedright\arraybackslash}X  % slimmer last column
}
\rowcolor{gray!30}
\textbf{Triple} & \textbf{Verdict} & \textbf{GPT-5 explanation} \\
\midrule
glucocorticoid receptor haploinsufficiency, cause\_of, hypertension & \textcolor{Green}{\bf yes} & GR resistance drives mineralocorticoid excess → hypertension. \\
glucocorticoid receptor, associated\_with, insulin resistance & \textcolor{Green}{\bf yes} & GC/GR signaling induces IR. \\
glucocorticoid receptor locus (GRL) polymorphisms, associated\_with, type 2 diabetes (T2D) & \textcolor{Green}{\bf yes} & NR3C1 variants linked to T2D risk. \\
podocyte-specific glucocorticoid receptor knockout (GR pKO) mice, has\_pathological\_process, diabetic nephropathy & \textcolor{Green}{\bf yes} & Podocyte GR loss worsens DN phenotype. \\
glucocorticoid receptor, has\_component, autophagy & \textcolor{red}{\bf no} & Autophagy is a process, not a receptor component. \\
glucocorticoid receptor (GR), finding\_site\_of, liver & \textcolor{red}{\bf no} & Relation reversed; GR is located in liver. \\
glucocorticoid receptor, has\_component, diabetic complications & \textcolor{red}{\bf no} & Diseases aren’t receptor components. \\
endothelial glucocorticoid receptor (GR), has\_causative\_agent, renal fibrosis & \textcolor{red}{\bf no} & Relation misuse; fibrosis doesn’t “cause” the receptor. \\
glucocorticoid receptor, has\_disposition, cortisol & \textcolor{red}{\bf no} & Cortisol is a ligand, not a disposition. \\
glucocorticoid receptor alpha, has\_component, coronavirus disease 2019 & \textcolor{red}{\bf no} & COVID-19 isn’t a component. \\
glucocorticoid receptor, has\_component, estrogen & \textcolor{red}{\bf no} & Hormone $\neq$ receptor component. \\
endothelial glucocorticoid receptor (GR), has\_pathological\_process, WNT signaling & \textcolor{red}{\bf no} & WNT signaling isn’t inherently pathological. \\
glucocorticoid receptor, has\_pathological\_process, osteoporosis (OP) & \textcolor{YellowOrange}{\bf maybe} & Excess GR signaling leads to GC-induced OP; relation loose. \\
glucocorticoid receptor, has\_direct\_substance, PEPCK & \textcolor{red}{\bf no} & GR regulates PEPCK expression; not a substance of GR. \\
glucocorticoid receptor (GR), has\_direct\_substance, Kupffer cells & \textcolor{red}{\bf no} & Cells aren’t receptor substances. \\
glucocorticoid receptor (GR), has\_component, Leydig cells & \textcolor{red}{\bf no} & Tissues/cells aren’t components of a receptor. \\
glucocorticoid receptor, has\_direct\_substance, stress response & \textcolor{red}{\bf no} & Process $\neq$ substance. \\
glucocorticoid receptor, associated\_with, miR-32-5p & \textcolor{YellowOrange}{\bf maybe} & Limited, context-specific miRNA linkage. \\
glucocorticoid receptor, has\_direct\_substance, G6P & \textcolor{red}{\bf no} & Metabolite isn’t a receptor substance. \\
glucocorticoid receptor, has\_component, SETD1A/COMPASS complex & \textcolor{red}{\bf no} & Possible cofactor interaction, not a component. \\
glucocorticoid receptor (GR), has\_direct\_substance, STAT6 & \textcolor{red}{\bf no} & TF interactor, not a substance of GR. \\
glucocorticoid receptor (GR), has\_direct\_substance, adipose tissue macrophage (ATM) & \textcolor{red}{\bf no} & Cells $\neq$ substances. \\
glucocorticoid receptor (GR), has\_component, peripheral sensory neurons & \textcolor{red}{\bf no} & Neurons aren’t receptor components. \\
glucocorticoid receptor $\beta$ (GR$\beta$)has\_direct\_substancers948820149 & \textcolor{red}{\bf no} & SNP pertains to gene, not isoform “substance.” \\
\bottomrule
\end{tabularx}
\end{table}




\clearpage 
\section{GraphRAG evaluation on Medical Benchmarks}
\label{app:public_benchmark}
\setcounter{table}{0}
\renewcommand{\thetable}{D\arabic{table}}
\setcounter{figure}{0}
\renewcommand{\thefigure}{D\arabic{figure}}


\begin{table}[ht!]
  \centering
  \caption{GraphRAG KG evaluation on public benchmarks}
  \label{tab:graphrag_public_benchmark}
  \small
  \setlength{\tabcolsep}{6pt}
  \rowcolors{1}{gray!10}{white}
  \begin{tabular}{l c c c}
  %p{1.8cm} p{1.8cm} p{2.5cm} }
    \toprule
    \rowcolor{gray!30}
      & MedMCQA & MedQA & MMLU (medical) \\
    \textbf{\# Questions} & 61 & 75 & 62 \\
    \midrule
    LLM KG (baseline) & 72.1 & 85.3 & 71.0 \\ 
    Seed KG & \textbf{76.5} & 81.3 & 73.1 \\
    \ours & 73.8 & \textbf{88.0} & \textbf{74.7} \\   
    \bottomrule
  \end{tabular}
\end{table}

In this evaluation, we select questions related to diabetes and its comorbidities from popular medical benchmarks and run GraphRAG evaluation on the selected questions. We first filter the benchmarks with the Qwen3-32B model, as stated in Section~\ref{subsec:benchmark}, and then manually review and remove some questions that are irrelevant to diabetes. 


\clearpage 
\section{Helper LLM prompts}
\label{app:graphmert_prompts}
\setcounter{table}{0}
\renewcommand{\thetable}{D\arabic{table}}
\setcounter{figure}{0}
\renewcommand{\thefigure}{D\arabic{figure}}


We include only one example for all few-shot settings.  For each prompt, all examples are mined from the dataset. Then, we prompt GPT-o3 with the zero-shot prompt, edit the reply if needed, and include it as a few-shot example.

All sequences in our datasets are lower-case; here, we use normal case to improve readability.

\begin{tcolorbox}[
    title=Entity Discovery Prompt,
    halign title=center,
    colback=white,        % background
    colframe=blue,        % frame color
    breakable,
    boxrule=0.8pt,        % frame thickness
    arc=4mm,              % corner radius
    left=4pt,             % inner left padding
    right=4pt,            % inner right padding
    top=4pt,              % inner top padding
    bottom=4pt,           % inner bottom padding
    fontupper=\small
]

You are a medical-domain extractor building a diabetes KG of $\langle$head, relation, tail$\rangle$. 
You possess advanced medical academic knowledge.  
\vspace{\baselineskip}

Given an input sequence, identify entities specifically relevant to diabetes, its complications, 
comorbidities, therapeutics, and related biomedical entities that help to clarify or contextualize them.  
Output a Python list of up to 6-word entity ``heads'' following these rules:

\begin{enumerate}
  \item Select a precise and medically-specific span (e.g., ``myocardial infarction,'' not ``infarction'').  
        Avoid generic terms like ``disease,'' ``condition,'' ``patients,'' and ``comorbidity'' without a specific context.  
        When encountering vague descriptors like ``complication,'' ``symptom,'' or ``effect,'' always prefer explicitly named conditions or symptoms directly linked to diabetes pathology or diabetes comorbidities.
  \item Keep original spelling, casing, and abbreviations from the sequence.
  \item Choose only entities that add meaningful medical knowledge to the diabetes KG.  
        Do not include COVID-related terms.  
        Do not include head entities that describe findings in animal models (mice, rats, etc.).
  \item A few examples of low-value entities you \textbf{should not} include:
    \begin{itemize}
      \item `$\geq$\verb| 10 % weight reduction'| (too context-dependent).  
      \item \verb|`nhanes 2015 - 2018'| (dataset/survey, not a medical entity).  
      \item \verb|`semaglutide 2.4 mg'| (includes a dosage, which can vary).  
      \item \verb|`60+ women'| (``60+'' is too context-dependent).  
      \item ``anxiety,'' ``home births,'' ``pregnant women,'' ``neonatal deaths,'' ``general practitioners'' (not specific enough to diabetes; only include if explicitly related to diabetes).
    \end{itemize}
  \item If it is not clear whether a term adds diabetes-specific knowledge, look at the context.  
        If the text explicitly links the term to a diabetes-specific concept, include it.  
        Otherwise, exclude it when mentioned only in a generic context.  
        Include such terms when the sequence clearly links them to a diabetes-relevant gene, pathway, cell type, or therapeutic effect.
\end{enumerate}

\vspace{\baselineskip}

You will be provided with incorrect output examples beginning with ``Output (Incorrect).''  
Use them to avoid the common mistakes.  
Wrap your intermediate reasoning steps clearly within $\langle$think$\rangle$ ... $\langle$/think$\rangle$ tags.  
Be strict and discard any entity about which you are uncertain and that is not relevant to diabetes.  
After generating, verify your output.  

\vspace{\baselineskip}

\textbf{Steps:}
\begin{enumerate}
  \item Identify candidate spans.
  \item Filter by medical precision and relevance rules.
  \item Confirm the entity's relevance and contribution to the diabetes KG; discard low-value entities.
\end{enumerate}

\vspace{\baselineskip}

\textbf{Input format:} sequence  

\textbf{Output format:} \verb|[`head1', `head2', …]|.  
If none, output \verb|[]|.

\end{tcolorbox}




\begin{tcolorbox}[
    title=Few-shot Example for Entity Discovery Prompt,
    halign title=center,
  colback=white,        % background
  colframe=blue,       % frame color
  % breakable, 
  boxrule=0.8pt,        % frame thickness
  arc=4mm,              % corner radius
  left=4pt,             % inner left padding
  right=4pt,            % inner right padding
  top=4pt,              % inner top padding
  bottom=4pt,            % inner bottom padding
  fontupper=\small
]
\colorbox{Yellow}{Input:}\\

\begin{mdframed}[backgroundcolor=Yellow!20,skipabove=1em,skipbelow=1em,]
sequence: \dots, its upstream regulator has the opposite effect (Han et al., 2013). Previous studies suggest that CHOP deteriorates ER stress and accelerates cell death via promoting protein synthesis and oxidative stress (Han et al., 2013). In addition, ER stress damages $\beta$-cells, possibly through altering Ca$^{2+}$ homeostasis. It has been indicated that ER stress interferes with the function of RyR located in the membrane of the ER and causes leakage of ER Ca$^{2+}$ (Yamamoto et al., 2019). The destruction of $\beta$-cell ER Ca$^{2+}$ homeostasis results in impaired insulin secretion and further promotion of $\beta$-cell death \dots
\end{mdframed}

Output:\\
\verb|[``chop,'' ``er stress,'' ``ryr,'' ``$\beta$-cells,'' ``impaired insulin secretion'']|
\vspace{\baselineskip}

The rationale behind the output:\\
Candidates: CHOP; ER stress; $\beta$-cells; RyR; impaired insulin secretion; oxidative stress; protein synthesis. Keep precise diabetes-relevant entities: CHOP (ER stress regulator), ER stress, $\beta$-cells, RyR, impaired insulin secretion.

\end{tcolorbox}




\begin{tcolorbox}[
    title=Relation Matching Prompt,
    halign title=center,
    colback=white,        % background
    colframe=blue,        % frame color
    breakable,
    boxrule=0.8pt,        % frame thickness
    arc=4mm,              % corner radius
    left=4pt,             % inner left padding
    right=4pt,            % inner right padding
    top=4pt,              % inner top padding
    bottom=4pt,           % inner bottom padding
    fontupper=\small
]

You are a medical intelligence with academic knowledge in diabetes and comorbidities.  
We are building a diabetes knowledge graph of triples $\langle$head, relation, tail$\rangle$.  

\vspace{\baselineskip}

Given:
\begin{itemize}
  \item a sequence including biomedical context,
  \item a list of heads,
\end{itemize}
return, for each head, all relations chosen from the list below that could form a plausible KG triple and are supported by the sequence.  

\vspace{\baselineskip}

Allowed relations:\\
\verb|###|
\begin{multicols}{3}
associated\_finding\_of \\
associated\_morphology\_of \\
associated\_with \\
causative\_agent\_of \\
cause\_of \\
due\_to \\
finding\_site\_of \\
focus\_of \\
has\_associated\_morphology \\
has\_component \\
has\_disposition \\
has\_finding\_site \\
has\_method \\
has\_modification \\
has\_part \\
interprets \\
inverse\_isa \\
is\_interpreted\_by \\
isa \\
occurs\_before \\
part\_of \\
plays\_role \\
possibly\_equivalent\_to
\end{multicols}

\verb|###|  \\

The relations are taken from UMLS and have the same meaning as in UMLS.  

Examples:  \\
\verb|-------------| 
\begin{itemize}
  \item carotid artery stenosis \,|\, associated\_finding\_of \,|\, history of carotid artery stenosis
  \item fibrosis \,|\, associated\_morphology\_of \,|\, endomyocardial fibrosis
  \item cancer \,|\, associated\_with \,|\, anemia in malignant neoplastic disease
  \item Mycobacterium tuberculosis \,|\, causative\_agent\_of \,|\, Tuberculosis
  \item diabetes mellitus \,|\, cause\_of \,|\, diabetic foot
  \item hypoglycemic alcoholic ketoacidosis \,|\, due\_to \,|\, acute alcohol intoxication
  \item adipose tissue \,|\, finding\_site\_of \,|\, lipoatrophy
  \item renal failure \,|\, focus\_of \,|\, emergency hemofiltration
  \item hepatitis A \,|\, has\_associated\_morphology \,|\, Hepatocellular necrosis
  \item fasting triglyceride \,|\, has\_component \,|\, triacylglycerol
  \item tumor necrosis factor \,|\, has\_disposition \,|\, immunomodulator
  \item melanoma \,|\, has\_finding\_site \,|\, skin
  \item bariatric surgery \,|\, has\_method \,|\, surgical action
  \item glucagon \,|\, has\_modification \,|\, glucagon hydrochloride
  \item nephron \,|\, has\_part \,|\, glomerulus
  \item overweight \,|\, interprets \,|\, body weight measure
  \item adiponectin \,|\, inverse\_isa \,|\, high molecular weight adiponectin
  \item blood eosinophil counts \,|\, is\_interpreted\_by \,|\, asthmatic pulmonary eosinophilia
  \item empagliflozin \,|\, isa \,|\, sodium glucose cotransporter subtype 2 inhibitor
  \item cardiac amyloidosis \,|\, occurs\_in \,|\, old age
  \item coronary syndrome \,|\, possibly\_equivalent\_to \,|\, preinfarction syndrome
  \item MI \,|\, same\_as \,|\, Myocardial infarction
\end{itemize}
\verb|-------------| \\ 

Note the meaning of some relations in UMLS:
\emph{isa} and \emph{inverse\_isa} are exact inverses of each other.
\begin{itemize}
  \item \emph{isa} --- points up the hierarchy: ``Diabetic retinopathy'' isa ``Retinal disease.'' (specific $\rightarrow$ general)
  \item \emph{inverse\_isa} --- points down the hierarchy: ``Retinal disease'' inverse\_isa ``Diabetic retinopathy.''
  \item \emph{cause\_of} --- directional link where the source concept is understood to directly or indirectly produce, trigger, or give rise to the target concept.
  \item \emph{due\_to} --- causal link: the subject condition, finding, or situation results from the object.  
  Inverse: \emph{cause\_of}.
  \item \emph{associated\_with} --- non-directional link indicating that two concepts are statistically or clinically linked without asserting a clear cause-and-effect direction.
  \item \emph{has\_associated\_morphology} --- links a pathological or clinical entity (typically a disease, syndrome, or injury) to the characteristic structural change (``morphology'') it produces.  
  Concretely: source = disorder concept; target = morphologic abnormality (e.g., ``Necrosis,'' ``Hyperplasia,'' ``Fibrosis'').  
  Inverse: \emph{associated\_morphology\_of}.
  \item \emph{associated\_finding\_of} --- reads as:  
  \quad ``X associated\_finding\_of Y''  
  \quad ``Finding X is the clinical finding for which procedure Y is performed.''
\end{itemize}
\verb|-------------|  

\vspace{\baselineskip}

Input format:
\begin{verbatim}
sequence
heads: [head1, head2, ...]
\end{verbatim}

Output format:
\begin{verbatim}
{
    ``head 1'': [``relation 1,'' ``relation 2,'' ...],
    ``head 2'': [...],
}
\end{verbatim}

\vspace{\baselineskip}

Steps:
\begin{enumerate}
  \item Understand Input
    \begin{itemize}
      \item Clearly understand the biomedical context from the sequence.  
      \item For each head, find explicit mentions in the text.  
      \item Check if each head is explicitly linked to other concepts or relations.  
    \end{itemize}

  \item Use the list of allowed relations.  
    Evaluate each head individually. Do not overuse the relation \texttt{associated\_with} — apply it only when appropriate.  

  \item For each head, list only plausible and supported relations.
    Return \verb|[]| if none apply.  
\end{enumerate}

\vspace{\baselineskip}

Think concisely within $\langle$think$\rangle$ ... $\langle$/think$\rangle$.  
Immediately after, output JSON.  

\end{tcolorbox}



\begin{tcolorbox}[
    title=Few-shot Example for the Relation Matching Prompt,
    halign title=center,
    colback=white,        % background
    colframe=blue,        % frame color
    breakable,
    boxrule=0.8pt,        % frame thickness
    arc=4mm,              % corner radius
    left=4pt,             % inner left padding
    right=4pt,            % inner right padding
    top=4pt,              % inner top padding
    bottom=4pt,           % inner bottom padding
    fontupper=\small
]

\colorbox{Yellow}{Input:}
\begin{mdframed}[backgroundcolor=Yellow!20,skipabove=1em,skipbelow=1em,]
\dots interleukin‐1 R6, and receptor activator of nuclear factor kappa‐B (RANK). Together, proteomic data suggest the targeting of several key regulators of inflammation, bone, and adipose turnover, via transforming growth factor‐beta/SMAD, and Wingless‐related integration site/be‐catenin signaling pathways. To the best of the knowledge, this is first evidence of an intervention that drives against bone loss via RANK. Metatranscriptomic analyses of the gut microbiota show P7C3 increased Porphyromonadaceae bacterium, Candidatus Melainabacteria, and Ruminococcaceae bacterium abundance, potentially contributing to the favorable inflammatory\dots
\end{mdframed}

heads: \verb|[`interleukin-1 r6,' `receptor activator of nuclear factor kappa-b,'|\\
\verb|`transforming growth factor‐beta']|

\vspace{\baselineskip}

\colorbox{Yellow}{Output:}
\begin{verbatim}
{
    ``interleukin‐1 r6'': [``associated_with''],
    ``receptor activator of nuclear factor kappa‐b'': [``cause_of''],
    ``transforming growth factor‐beta'': [``part_of''],
}
\end{verbatim}

\vspace{\baselineskip}
\underline{The rationale behind this output:}\\

interleukin‐1 r6 → \verb|associated_with| → Named as a “key regulator of inflammation,” which links it to the inflammatory process without stating direction or hierarchy, so the non-causal \verb|associated_with| relation fits best.\\

receptor activator of nuclear factor $\kappa$-B (RANK) → \verb|cause_of| → The text says the intervention prevents bone loss via RANK, implying that RANK signalling produces or drives bone loss; therefore \verb|cause_of| is appropriate.\\

transforming growth factor-beta → \verb|part_of| → Explicitly mentioned within the “TGF-$\beta$/Smad signalling pathway,” so it is a constituent component (\verb|part_of|) of that pathway.\\

No additional relations are warranted.  

\vspace{\baselineskip}

\end{tcolorbox}

\begin{tcolorbox}[
    title=Combining \ours-predicted Top Tokens Prompt,
    halign title=center,
    colback=white,
    colframe=blue,
    breakable,
    boxrule=0.8pt,
    arc=4mm,
    left=4pt,
    right=4pt,
    top=4pt,
    bottom=4pt,
    fontupper=\small
]

You are completing triples for a medical knowledge graph on diabetes and its comorbidities. For each sample, you're given a sequence, a head entity in that sequence, a relation, and a list of candidate tokens. The relations are from UMLS and have the same meaning.\\[2pt]

Your task is to output a filtered list of high-quality and factual tails in the format: \verb|[``tail 1,'' ``tail 2,'' ...]| or \verb|[]|.\\[4pt]

\textbf{To form the list of candidates:}\\
Step 1: Analyze the sequence to understand the context and identify the head entity and relation.\\
Step 2: Choose candidate tails. You can combine tokens from the candidate list to get the most precise, relevant, and meaningful tails in the context of the head and relation. Combine subword tokens, too.\\
Step 3: Verify each candidate.\\[4pt]

\textbf{Verification.} Each tail must:
\begin{itemize}
  \item Be causally and factually related to the head via the specified relation. Make sure the relation direction is correct: the head implies the tail given the relation. Note that \texttt{isa} is a subclass\(\to\)class relation, and \texttt{inverse\_isa} is a class\(\to\)subclass relation.
  \item Be supported by the sequence, but you can rely on well-established medical knowledge even if the sequence does not spell it out verbatim. If no reliable support exists, reject the tail.
  \item Add valuable medical knowledge to the graph. Tails must be non-redundant. When all tails are factual, prefer specific tails over general and vague (e.g., ``proliferative diabetic retinopathy'' over ``retinopathy''). Terms that include ``level,'' ``disease,'' ``disorder,'' or ``complication'' are too vague and rarely add useful knowledge to the KG.
  \item Include only tokens from the list of candidates.
\end{itemize}

Reason step by step within \texttt{<think>...</think>}.\\[2pt]

You will see incorrect outputs labeled ``Output (Incorrect).'' Avoid similar errors.\\[4pt]

\textbf{Before finalizing:}\\
-- Ensure all output constraints are met.\\
-- Validate that each tail is \textbf{logically, contextually, and factually aligned} with the head and relation.\\
-- Confirm that each triple adds \textbf{meaningful} knowledge to the graph.\\[4pt]

Note the meaning of some UMLS relations you may encounter in the input:\\
\begin{itemize}
  \item \textit{isa} and \textit{inverse\_isa} are exact inverses of each other.
    \begin{itemize}
      \item \textit{isa} – points up the hierarchy: ``Diabetic retinopathy'' \textit{isa} ``Retinal disease.'' (specific $\to$ general)
      \item \textit{inverse\_isa} – points down the hierarchy: ``Retinal disease'' \textit{inverse\_isa} ``Diabetic retinopathy.''
    \end{itemize}
  \item \textit{cause\_of} – directional link where the source concept directly or indirectly produces, triggers, or gives rise to the target concept.
  \item \textit{due\_to} – causal link: the subject condition, finding, or situation results from the object. Inverse: \textit{cause\_of}.
  \item \textit{associated\_with} – non‐directional link indicating that two concepts are statistically or clinically linked without asserting a clear cause–effect direction.
  \item \textit{has\_associated\_morphology} – links a pathological or clinical entity (typically a disease, syndrome, or injury) to the characteristic structural change (``morphology'') it produces.\\
  Concretely: source = a disorder concept; target = a \emph{Morphologic Abnormality} concept (e.g., ``Necrosis,'' ``Hyperplasia,'' ``Fibrosis''). Inverse: \textit{associated\_morphology\_of}.
  \item \textit{associated\_finding\_of} – reads as: \\
    ``X \textit{associated\_finding\_of} Y'' $\Rightarrow$ ``Finding X is the clinical finding for which procedure Y is performed.''
\end{itemize}

\end{tcolorbox}


\begin{tcolorbox}[
    title=Few-Shot Example for Combining \ours-predicted Top Tokens Prompt,
    halign title=center,
  colback=white,        % background
  colframe=blue,       % frame color
  breakable,
  boxrule=0.8pt,        % frame thickness
  arc=4mm,              % corner radius
  left=4pt,             % inner left padding
  right=4pt,            % inner right padding
  top=4pt,              % inner top padding
  bottom=4pt,            % inner bottom padding
  fontupper=\small
]
\colorbox{Yellow}{Input:}\\
\begin{mdframed}[backgroundcolor=Yellow!20,skipabove=1em,skipbelow=1em,]
sequence: \dots 2+ binding to S100A1 EF-hand motifs, the conformation of S100A1 changes and promotes interactions with target proteins. RAGE consists of three domains: the cytoplasmic, transmembrane, and extracellular domains. The extracellular domain consists of C1, C2, and V domains. V domains are the primary receptors for the S100 protein. It was reported several years ago that S100A1 and RAGE V domains interact in a pathway involving S100A1-RAGE signaling, whereby S100A1 binds to the V domain, resulting in RAGE dimerization. The autophosphorylation of the cytoplasmic domain initiates a signaling cascade that regulates cell proliferation, cell growth, and tumor formation. In this study\dots
\end{mdframed}

head: \verb|[``s100a1'']|\\
relation: associated\_with\\
predictions: protein receptor hydrolase structure process pathway factor complex glycoprotein s100a family domain oxidoreductase proteinase ligand extracellular signaling calcium apolipoprotein s100\\

Output:
\begin{verbatim}
[``calcium signaling pathway'']
\end{verbatim}

\underline{The rationale behind this output:}\\
\noindent Available tokens let us build terms such as:
\begin{itemize}
  \item ``calcium signaling pathway'' (calcium + signaling + pathway)
  \item ``extracellular signaling'' (extracellular + signaling) -- too broad
  \item ``s100a family'' -- classification, not an association
\end{itemize}

Other tokens (``glycoprotein,'' ``oxidoreductase,'' etc.) do not represent well-known processes linked to S100A1.\\

\noindent \textbf{Preference \& verification:}\\
``S100A1 modulates intracellular Ca$^{2+}$ handling and is well-documented to participate in the calcium signaling pathway, especially in cardiac and skeletal muscle.'' This is a concrete biological pathway association that adds value to the graph.

\end{tcolorbox}


%=============================================

\clearpage 
\section{GraphRAG Prompts}
\label{app:graphrag_prompts}
\setcounter{table}{0}
\renewcommand{\thetable}{E\arabic{table}}
\setcounter{figure}{0}
\renewcommand{\thetable}{E\arabic{table}}

\begin{tcolorbox}[
    title=E1: GraphRAG Index Prompt,
    %label={prompt:GraphRAG-index},
    halign title=center,
  colback=white,        % background
  colframe=blue,       % frame color
  breakable,
  boxrule=0.8pt,        % frame thickness
  arc=4mm,              % corner radius
  left=4pt,             % inner left padding
  right=4pt,            % inner right padding
  top=4pt,              % inner top padding
  bottom=4pt,            % inner bottom padding
  fontupper=\small
]
\refstepcounter{table}\label{prompt:GraphRAG-index}

-Role-\\
You are an AI assistant specialized in extracting structured information from biomedical texts to build a knowledge graph about diabetes.\\

-Goal-\\
Given some medical paper abstracts, a predefined list of entity types, and a predefined list of relations, identify all entities of those types and the medically meaningful relationships explicitly described among the identified entities within the abstract. You should only extract entities that are relevant to diabetes, its complications, and comorbidites.\\

-Entitiy Types-\\
You should extract entities from the following 5 entity types: \\
Organism, Anatomical Structure, Manufactured Object, Substance, Conceptual Entity.\\ 

Use the subcategories listed below SOLELY as guidance to help you determine the correct main entity type. Only use the 5 main entity types in your output.\\

1. Organism: Plant; Fungus; Virus; Bacterium; Archaeon; Eukaryote; Vertebrate; Amphibian; Bird; Fish; Reptile; Mammal; Human\\
2. Anatomical Structure: Embryonic Structure; Anatomical Abnormality; Congenital Abnormality; Acquired Abnormality; Fully Formed Anatomical Structure; Body Part, Organ, or Organ Component; Tissue; Cell; Cell Component; Gene or Genome\\
3. Manufactured Object: Medical Device; Drug Delivery Device; Research Device; Clinical Drug\\
4. Substance: Chemical; Pharmacologic Substance; Antibiotic; Biomedical or Dental Material; Biologically Active Substance; Hormone; Enzyme; Vitamin; Immunologic Factor; Receptor; Indicator, Reagent, or Diagnostic Aid; Organic Chemical; Nucleic Acid, Nucleoside, or Nucleotide; Amino Acid, Peptide, or Protein; Inorganic Chemical; Element, Ion, or Isotope; Body Substance; Food\\
5. Conceptual Entity: Idea or Concept; Body System; Body Space or Junction; Body Location or Region; Molecular Sequence; Nucleotide Sequence; Amino Acid Sequence; Carbohydrate Sequence; Geographic Area; Finding; Laboratory or Test Result; Sign or Symptom; Organism Attribute; Clinical Attribute; Intellectual Product; Occupation or Discipline; Organization; Group\\
\\
-Relation Types-\\
Please only identify the following 35 relations: [`associated\_finding\_of,' `associated\_morphology\_of,' `associated\_with,' `causative\_agent\_of,' `cause\_of,' `direct\_procedure\_site\_of,' `due\_to,' `finding\_site\_of,' `focus\_of,' `has\_associated\_morphology,' `has\_causative\_agent,' `has\_clinical\_course,' `has\_component,' `has\_direct\_procedure\_site,' `has\_direct\_substance,' `has\_disposition,' `has\_entire\_anatomy\_structure,' `has\_finding\_site,' `has\_focus,' `has\_method,' `has\_modification,' `has\_part,' `has\_pathological\_process,' `interprets,' `inverse\_isa,' `is\_interpreted\_by,' `is\_modification\_of,' `isa,' `method\_of,' 'occurs\_before', `occurs\_in,' `part\_of,' `plays\_role,' `possibly\_equivalent\_to,' `same\_as']\\

The following provides one example for each type of relation, formatted as `head, relation, tail':\\
fetal growth restriction (fgr), associated\_finding\_of, history of fetal growth retardation\\
tumors, associated\_morphology\_of, neoplastic disease\\
neutropenia, associated\_with, neutropenic sepsis\\
s. epidermidis, causative\_agent\_of, staphylococcus epidermidis ventriculitis\\
chronic kidney disease (ckd), cause\_of, renal retinopathy\\
gastric fundus,	direct\_procedure\_site\_of, laparoscopic fundoplication\\
diabetic cardiomyopathy (dbcm), due\_to, diabetes mellitus\\
endocrine pancreas, finding\_site\_of, extreme insulin resistance type a\\
gait abnormalities, focus\_of, prosthetic gait training\\
pyoderma gangrenosum, has\_associated\_morphology, neutrophilic infiltration\\
chronic chagas disease cardiomyopathy, has\_causative\_agent, trypanosoma cruzi\\
membranous nephropathy, has\_clinical\_course, chronic\\
serum creatinine level, has\_component, creatinine\\
fmt, has\_direct\_procedure\_site, gastrointestinal tract structure\\
high-intensity statins, has\_direct\_substance, hmg-coa reductase inhibitor\\
resveratrol (res), has\_disposition, platelet aggregation inhibitor\\
middle occipital gyrus, has\_entire\_anatomy\_structure, entire lateral occipital gyrus\\
diabetes retinopathy, has\_finding\_site, retinal structure\\
on-line hemodiafiltration, has\_focus, renal failure syndrome\\
islet cell transplant, has\_method, surgical transplantation\\
uric acid, has\_modification, calcium urate\\
anaerobic glycolysis, has\_part, pyruvate kinase activity\\
evans syndrome, has\_pathological\_process, autoimmune process\\
endocrine hypertension,	interprets,	blood pressure\\
adaptive thermogenesis, inverse\_isa, diet induced thermogenesis\\
serum triglyceride, is\_interpreted\_by, serum triglyceride levels\\
tau, is\_modification\_of, uridine\\
t cell receptors, isa, antigen receptor\\
amputation, method\_of, cineplastic amputation\\
renal transplantation, occurs\_before, accelerated rejection of renal transplant\\
paediatric obesity, occurs\_in, childhood\\
bone resorption, part\_of, bone remodeling\\
everolimus (eve), plays\_role, antineoplastic therapeutic role\\
non-alcoholic fatty liver disease, possibly\_equivalent\_to, fatty liver\\
retinal cotton wool spots, same\_as, retinal exudates\\
\\

-Steps-\\
1. Identify all entities corresponding to one of the 5 main entity types and relevant to diabetes, using the subcategory examples as guidance for classification. 
For each identified entity, extract the following information:\\
- entity\_name: Name of the entity, lowercase\\
- entity\_type: One of the following types: Organism, Anatomical Structure, Manufactured Object, Substance, Conceptual Entity.\\
- entity\_description: concise description of the entity's attributes and activities.
Format each entity as \texttt{(''entity''\textless\textbar\textgreater\textless entity\_name\textgreater\textless\textbar\textgreater\textless entity\_type\textgreater\textless\textbar\textgreater\textless entity\_description\textgreater)}\\
 
2. From the entities identified in step 1, identify all pairs of (source\_entity, target\_entity) that are clearly related to each other according to the given text, and are medically meaningful. 
Only use the 35 relationships that are in the predefined list.\\

Avoid relationships that are attached to entities that are too general, for example: patients, bodily functions, parameters, management, optimization.
Only keep the relationships that state facts, represent the main idea in the text, or other important relationships that are in the predefined list.
It's acceptable if some entities identified in the previous step are not used.\\

For each pair of related entities, extract the following information:
- source\_entity: name of the source entity, as identified in step 1\\
- target\_entity: name of the target entity, as identified in step 1\\
- relationship: one relation that is in the predefined list, according to the given text\\
- relationship\_strength: a numeric score out of 10 indicating the strength of the relationship between the source entity and target entity\\
Format each relationship as \\
\texttt{(''relationship''\textless\textbar\textgreater\textless source\_entity\textgreater\textless\textbar\textgreater\textless target\_entity\textgreater\textless\textbar\textgreater\textless relationship\_description\textgreater\textless\textbar\textgreater\\
\textless relationship\_strength\textgreater)}\\
 
3. Return output in English as a single list of all the entities and relationships identified in steps 1 and 2. Use \texttt{\#\#} as the list delimiter.\\
 
4. When finished, output \texttt{\textless\textbar COMPLETE\textbar\textgreater}\\

- Constraints and Guidelines\\
- Strict Textual Grounding: Base all extractions only on the provided medical abstract. Do not use external knowledge or make assumptions beyond what is written.\\
- Entity Filtering: Only extract the entities whose type is present in the provided 5 Entity Type, and only extract entities that are relevant to diabetes, its complications, and comorbidites. \\
- Relationship Filtering: Extract only the 35 relationships as defined. Exclude all other relationships.\\
- Delimiter Usage: Strictly adhere to the specified {tuple\_delimiter} within tuples and {record\_delimiter} between records.\\
    
\end{tcolorbox}

\begin{tcolorbox}[
    title=E2: GraphRAG Index Example,
    %label={prompt:GraphRAG-index-example},
    halign title=center,
  colback=white,        % background
  colframe=blue,       % frame color
  breakable,
  boxrule=0.8pt,        % frame thickness
  arc=4mm,              % corner radius
  left=4pt,             % inner left padding
  right=4pt,            % inner right padding
  top=4pt,              % inner top padding
  bottom=4pt,            % inner bottom padding
  fontupper=\small
]
\refstepcounter{table}\label{prompt:GraphRAG-index-example}

\colorbox{Yellow}{Input:}\\
\begin{mdframed}[backgroundcolor=Yellow!20,skipabove=1em,skipbelow=1em,]
\texttt{\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#}\\
Entity\_types: Organism, Anatomical Structure, Manufactured Object, Substance, Conceptual Entity\\
Text:\\
CMR imaging enables comprehensive detection of myocardial remodeling in patients undergoing TAVR. Regression of LV matrix volume as a surrogate for reversible diffuse myocardial fibrosis is accompanied by increase of myocardial function and recovery from heart failure. Further data are required to define the value of these parameters as therapeutic targets for optimized management of TAVR patients.\\
\texttt{\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#}\\
Output:
\end{mdframed}

Output:\\
\texttt{(''entity''\textless\textbar\textgreater cmr imaging\textless\textbar\textgreater Conceptual Entity\textless\textbar\textgreater A medical imaging technology that allows for the comprehensive detection of myocardial remodeling.)}

\texttt{\#\#}

\texttt{(''entity''\textless\textbar\textgreater myocardial remodeling\textless\textbar\textgreater Conceptual Entity\textless\textbar\textgreater A pathological process involving changes in the heart muscle, observed in patients undergoing TAVR.)}

\texttt{\#\#}

\texttt{(''entity''\textless\textbar\textgreater tavr\textless\textbar\textgreater Conceptual Entity\textless\textbar\textgreater Transcatheter Aortic Valve Replacement, a medical procedure undergone by patients with myocardial remodeling.)}

\texttt{\#\#}

\texttt{(''entity''\textless\textbar\textgreater regression of LV matrix volume\textless\textbar\textgreater Conceptual Entity\textless\textbar\textgreater The reduction in the volume of the left ventricular matrix.)}

\texttt{\#\#}

\texttt{(''entity''\textless\textbar\textgreater reversible diffuse myocardial fibrosis\textless\textbar\textgreater Conceptual Entity\textless\textbar\textgreater A condition of the heart muscle for which the regression of LV matrix volume acts as a surrogate.)}

\texttt{\#\#}

\texttt{(''entity''\textless\textbar\textgreater myocardial function\textless\textbar\textgreater Conceptual Entity\textless\textbar\textgreater The physiological performance of the heart muscle.)}

\texttt{\#\#}

\texttt{(''entity''\textless\textbar\textgreater heart failure\textless\textbar\textgreater Conceptual Entity\textless\textbar\textgreater A clinical condition from which recovery is observed along with the regression of LV matrix volume.)}

\texttt{\#\#}

\vspace{1em}

\texttt{(''relationship''\textless\textbar\textgreater myocardial remodeling\textless\textbar\textgreater cmr imaging\textless\textbar\textgreater has\_method\textless\textbar\textgreater9)}

\texttt{\#\#}

\texttt{(''relationship''\textless\textbar\textgreater regression of LV matrix volume\textless\textbar\textgreater reversible diffuse myocardial fibrosis\textless\textbar\textgreater is\_interpreted\_by\textless\textbar\textgreater9)}

\texttt{\#\#}

\texttt{(''relationship''\textless\textbar\textgreater regression of LV matrix volume\textless\textbar\textgreater myocardial function\textless\textbar\textgreater associated\_with
\textless\textbar\textgreater10)}

\texttt{\#\#}

\texttt{(''relationship''\textless\textbar\textgreater regression of LV matrix volume\textless\textbar\textgreater heart failure\textless\textbar\textgreater associated\_with\textless\textbar\textgreater10)}

\texttt{\#\#}

\texttt{(''relationship''\textless\textbar\textgreater tavr\textless\textbar\textgreater myocardial remodeling\textless\textbar\textgreater occurs\_before\textless\textbar\textgreater8)}

\texttt{\textless\textbar COMPLETE\textbar \textgreater}

\end{tcolorbox}


\begin{tcolorbox}[
    title=E3: GraphRAG Query Prompt,
    halign title=center,
  colback=white,        % background
  colframe=blue,       % frame color
  breakable,
  boxrule=0.8pt,        % frame thickness
  arc=4mm,              % corner radius
  left=4pt,             % inner left padding
  right=4pt,            % inner right padding
  top=4pt,              % inner top padding
  bottom=4pt,            % inner bottom padding
  fontupper=\small
]
\refstepcounter{table}\label{prompt:GraphRAG-query}

\textbf{---Role---}

You are a helpful assistant responding to questions about data in the tables provided.

\bigskip
\textbf{---Data tables---}

\texttt{\{context\_data\}}

\bigskip
\textbf{---Goal---}

Generate a response in the specified target format that answers the user’s question using only the information provided in the tables. Do not use any external knowledge. If any prior knowledge conflicts with the tables, always rely on the tables as the sole source of truth.

If you cannot get the answer based on the tables, just say so. Do not make anything up.

Points supported by data should list their data references as follows:
\begin{quote}
``This is an example sentence supported by multiple data references \texttt{[Data: \textless dataset name\textgreater~(record ids); \textless dataset name\textgreater~(record ids)]}.''
\end{quote}

Do not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add ``+more'' to indicate that there are more.\\

For example:
\begin{quote}
``Person X is the owner of Company Y and subject to many allegations of wrongdoing \texttt{[Data: Entities (5, 7); Relationships (2, 7, 34, 46, 64, +more)]}.''
\end{quote}
where 5, 7, 2, 34, 46, and 64 represent the id (not the index) of the relevant data record.

Do not include information where the supporting evidence for it is not provided.

\bigskip
\textbf{---Target response format---}

Response type: multiple paragraphs.

Provide a concise answer using \verb|\boxed{}|, select only the correct letter from A, B, C, D. (e.g., \verb|\boxed{C}|)\\

Reference data points that support your answer using the given format (e.g., \texttt{[Data: Relationships (2, 3, 4); Entities (35, 36, 37, 39, 55, +more)]}). If no relevant information from the table supports your answer, leave the reference empty (e.g., \texttt{[]}).

\end{tcolorbox}


\begin{tcolorbox}[
    title=E4: GraphRAG Context Example,
    halign title=center,
  colback=white,        % background
  colframe=blue,       % frame color
  breakable,
  boxrule=0.8pt,        % frame thickness
  arc=4mm,              % corner radius
  left=4pt,             % inner left padding
  right=4pt,            % inner right padding
  top=4pt,              % inner top padding
  bottom=4pt,            % inner bottom padding
  fontupper=\small
]
\refstepcounter{table}\label{prompt:GraphRAG-context}
\textbf{-----Relationships-----}

\texttt{source -- (relation) --> target}

\texttt{chemotherapy -- (cause\_of) --> peripheral neuropathy due to and following chemotherapy}

\texttt{chemotherapy - induced peripheral neuropathy -- (due\_to) --> administration of antineoplastic agent}

\texttt{antineoplastic drugs -- (associated\_with) --> resistance to antineoplastic drug}

\texttt{chemotherapy - induced peripheral neuropathies -- (due\_to) --> administration of antineoplastic agent}

\texttt{hematologic malignancies -- (isa) --> neoplastic disease}

\texttt{lymphoid leukaemia -- (associated\_finding\_of) --> history of lymphoid leukemia}

\texttt{leukemia -- (associated\_finding\_of) --> history of leukemia}

\texttt{cancer -- (focus\_of) --> oral chemotherapy for malignant neoplasm}

\texttt{cancer -- (associated\_with) --> restrictive cardiomyopathy secondary to malignancy}

\texttt{cancer -- (associated\_with) --> cancer - related fatigue}

\texttt{imatinib -- (plays\_role) --> antineoplastic therapeutic role}

\texttt{imatinib -- (isa) --> antineoplastic agent}

\texttt{vorinostat -- (plays\_role) --> antineoplastic therapeutic role}

\texttt{nivolumab -- (plays\_role) --> antineoplastic therapeutic role}

\texttt{rucaparib -- (plays\_role) --> antineoplastic therapeutic role}

\texttt{nivolumab -- (isa) --> antineoplastic agent}

\texttt{rucaparib -- (isa) --> antineoplastic agent}

\texttt{antineoplastic agents -- (inverse\_isa) --> vorinostat}

\texttt{......}
\end{tcolorbox}

\end{appendices}

\end{document}